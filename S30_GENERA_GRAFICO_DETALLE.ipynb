{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S30 - RUTINA GENERACIÃ“N DE GRÃFICOS.\n",
    "\n",
    "Parte de los forecast executiÃ³n que estÃ¡n en estado 30 (Ya Ejecutado el Forecast), Genera en los archivos locales de detalle los grÃ¡ficos a nivel lÃ­nea,\n",
    "\n",
    "Los deja en archivos locales algoritmo_Pronostico_Extendido y Actualiza esl estado de 30 a 40  Graficos OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUTINA GENERADORA DE GRÃFICOS\n",
    "\n",
    "1) Leer archivo Solicitudes_Compra\n",
    "2) Leer datos adicionales y id relacionados\n",
    "3) Leer datos adicionales de la T710_Estadis_ReposiciÃ³n\n",
    "4) Generar GRAFICOS\n",
    "5) Actulizar Estado en connexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA GENERADORA GLOBAL DE GRÃ„RICOS\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# LIBRERIAS NECESARIAS \n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "import psycopg2 as pg2    # Conectores para Postgres\n",
    "import getpass  # Para obtener el usuario del sistema operativo\n",
    "import uuid  # Importar la librerÃ­a uuid\n",
    "# Mostrar el DataFrame resultante\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Evitar Mensajes Molestos\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)\n",
    "\n",
    "secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUTINAS LOCALES de PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de conexiÃ³n a la base de datos\n",
    "def Open_Conn_Postgres():\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE4']} user={secrets['USUARIO4']} password={secrets['CONTRASENA4']} host={secrets['SERVIDOR4']} port={secrets['PUERTO4']}\"\n",
    "    try:    \n",
    "        conn = pg2.connect(conn_str)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexiÃ³n: {e}')\n",
    "        return None\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "# Helper para generar identificadores Ãºnicos\n",
    "def id_aleatorio():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def get_execution(execution_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, description, name, \"timestamp\", supply_forecast_model_id, \n",
    "                ext_supplier_code, supplier_id, supply_forecast_execution_status_id\n",
    "            FROM public.spl_supply_forecast_execution\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (execution_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"description\": row[1],\n",
    "                \"name\": row[2],\n",
    "                \"timestamp\": row[3],\n",
    "                \"supply_forecast_model_id\": row[4],\n",
    "                \"ext_supplier_code\": row[5],\n",
    "                \"supplier_id\": row[6],\n",
    "                \"supply_forecast_execution_status_id\": row[7]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution(execution_id, **kwargs):\n",
    "    if not kwargs:\n",
    "        print(\"No hay valores para actualizar\")\n",
    "        return None\n",
    "\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(execution_id)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution(execution_id)  # Retorna la ejecuciÃ³n actualizada\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_by_status(status):\n",
    "    if not status:\n",
    "        print(\"No hay estados para filtrar\")\n",
    "        return None\n",
    "    \n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT id, description, name, \"timestamp\", supply_forecast_model_id, ext_supplier_code, graphic, \n",
    "                monthly_net_margin_in_millions, monthly_purchases_in_millions, monthly_sales_in_millions, sotck_days, sotck_days_colors, \n",
    "                supplier_id, supply_forecast_execution_status_id\n",
    "                FROM public.spl_supply_forecast_execution\n",
    "                WHERE supply_forecast_execution_status_id = {status};\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        fexsts = pd.read_sql(query, conn)\n",
    "        return fexsts\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_status: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn) \n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperar Maestro de ArtÃ­culos\n",
    "    articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtrÃ³ algÃºn missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de grÃ¡ficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    df_forecast[\"GRAFICO\"] = df_forecast.apply(\n",
    "        lambda row: generar_grafico_base64(df_ventas, row[\"Codigo_Articulo\"], row[\"Sucursal\"], row[\"Forecast\"], row[\"Average\"], row[\"ventas_last\"], row[\"ventas_previous\"], row[\"ventas_same_year\"]) if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Leer Dataframe de los PRODUCTOS\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT ext_code, description, id FROM public.fnd_product\n",
    "    ORDER BY ext_code;\n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    products = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "    products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "    #Unir los dataframes por Codigo_Articulo = ext_code\n",
    "    df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "    df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "    df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "    df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['code','name'], inplace=True)\n",
    "    \n",
    "    # SUBIR INFORMACIÃ“N DE ARTICULOS y ESTIDISTICA REPOSICIÃ“N\n",
    "    # Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "    columnas_seleccionadas = [\n",
    "        'C_ARTICULO', 'C_SUCU_EMPR', 'F_ULTIMA_VTA', 'Q_VTA_ACUM', 'Q_UNID_PESO_VTA_MES_ACTUAL', 'Q_VTA_ULTIMOS_15DIAS',\n",
    "        'Q_VTA_ULTIMOS_30DIAS', 'Q_TRANSF_PEND', 'Q_TRANSF_EN_PREP','Q_VENTA_30_DIAS',\n",
    "        'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_VENTA_ESPECIAL_30_DIAS','Q_VENTA_ESPECIAL_15_DIAS', 'Q_DIAS_CON_STOCK', 'Q_REPONER',\n",
    "        'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL','Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "    ]\n",
    "\n",
    "    # Filtrar el dataframe con las columnas seleccionadas\n",
    "    df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "    df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "    \n",
    "    # Realizar la fusiÃ³n de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "    df_merged = df_merged.merge(\n",
    "        df_nuevo, \n",
    "        left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "        right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "        how='left'\n",
    "    )\n",
    "    df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def generar_grafico_base64(dfv, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "    df_filtrado = dfv[(dfv[\"Codigo_Articulo\"] == articulo) & (dfv[\"Sucursal\"] == sucursal)]\n",
    "    df_filtrado = df_filtrado[df_filtrado[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=50))]\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 6), nrows= 2, ncols= 2\n",
    "    )\n",
    "    fig.suptitle(f\"Demanda Articulo {articulo} - Sucursal {sucursal}\")\n",
    "    current_ax = 0\n",
    "    #Bucle para Llenar los grÃ¡ficos\n",
    "    colors =[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "    # Ventas Diarias\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Ventas Diarias\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Unidades\"], marker=\"o\", linestyle=\"-\", label=\"Ventas\", color=colors[0])\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Media_Movil\"], linestyle=\"--\", label=\"Media MÃ³vil (7 dÃ­as)\", color=\"black\")\n",
    "    ax[0, 0].set_title(\"Ventas Diarias\")\n",
    "    ax[0, 0].legend()\n",
    "    ax[0, 0].set_xlabel(\"Fecha\")\n",
    "    ax[0, 0].set_ylabel(\"Unidades\")\n",
    "    ax[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Ventas Semanales\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\")[\"Unidades\"].sum().reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_filtrado.groupby(\"Semana\")[\"Fecha\"].min().reset_index()[\"Fecha\"].dt.isocalendar().week.astype(int)\n",
    "    df_semanal[\"Media_Movil\"] = df_semanal[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Histograma de ventas semanales\n",
    "    ax[0, 1].bar(df_semanal[\"Semana_Num\"], df_semanal[\"Unidades\"], color=[colors[1],colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[0, 1].set_xlabel(\"Semana del AÃ±o\")\n",
    "    ax[0, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[0, 1].set_title(\"Histograma de Ventas Semanales\")\n",
    "    ax[0, 1].tick_params(axis='x', rotation=60)\n",
    "    ax[0, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Graficar el Forecast vs Ventas Reales en la tercera celda\n",
    "    labels = [\"Forecast\",\"Actual\", \"Anterior\", \"AÃ±o Ant\"]\n",
    "    values = [Forecast, ventas_last, ventas_previous, ventas_same_year]\n",
    "\n",
    "    ax[1, 0].bar(labels, values, color=[colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[1, 0].set_title(\"Forecast vs Ventas Anteriores\")\n",
    "    ax[1, 0].set_ylabel(\"Unidades\")\n",
    "    ax[1, 0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Definir fechas de referencia\n",
    "    fecha_maxima = df_filtrado[\"Fecha\"].max()\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    # Calcular ventas de los Ãºltimos 30 dÃ­as\n",
    "    ventas_ultimos_30 = df_filtrado[(df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30)][\"Unidades\"].sum()\n",
    "\n",
    "    # Calcular ventas de los 30 dÃ­as previos a los Ãºltimos 30 dÃ­as\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # SimulaciÃ³n de datos para las ventas del aÃ±o anterior\n",
    "    df_filtrado_anio_anterior = df_filtrado.copy()\n",
    "    df_filtrado_anio_anterior[\"Fecha\"] = df_filtrado_anio_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_mismo_periodo_anio_anterior = df_filtrado_anio_anterior[\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) &\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # Datos para el histograma\n",
    "    labels = [\"Ãšltimos 30\", \"Anteriores 30\", \"AÃ±o anterior\", \"Average\"]\n",
    "    values = [ventas_ultimos_30, ventas_previos_30, ventas_mismo_periodo_anio_anterior, Average]\n",
    "\n",
    "    # Graficar el histograma en la celda [1,1]\n",
    "    ax[1, 1].bar(labels, values, color=[colors[0], colors[1], colors[2]], alpha=0.7)\n",
    "    ax[1, 1].set_title(\"ComparaciÃ³n de Ventas en 3 PerÃ­odos\")\n",
    "    ax[1, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[1, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Mostrar el grÃ¡fico\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para no solapar con el tÃ­tulo\n",
    "\n",
    "    # Guardar grÃ¡fico en base64\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    graficar_desde_datos_json,\n",
    "    generar_grafico_json,\n",
    "    insertar_graficos_json,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64,\n",
    "    generar_grafico_json\n",
    ")\n",
    "# Leer Dataframe de FORECAST EXECUTION \n",
    "        \n",
    "fes = get_execution_execute_by_status(50)\n",
    "\n",
    "# Mostrar la tabla con los grÃ¡ficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL RECORRE FORECAST EXCEC con STATUS 30 y pasa a 40\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 10  #FORECAST OK\n",
    "#for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 30].iterrows():\n",
    "\n",
    "for index, row in fes[fes[\"ext_supplier_code\"] == '20'].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"forecast_execution_id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        # Llamar a la funciÃ³n que genera los grÃ¡ficos y datos extendidos\n",
    "        df_merged = insertar_graficos_json(algoritmo, name, id_proveedor)\n",
    "\n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_merged.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        #fes.at[index, \"supply_forecast_execution_status_id\"] = 40\n",
    "        # âœ… Actualizar directamente en la base de datos el estado a 40\n",
    "       # update_execution(execution_id, supply_forecast_execution_status_id=40)\n",
    "        print(f\"Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ARREGLAR PROBLEMAS\n",
    "# df_sin_duplicados = df_forecast_ext.drop_duplicates(subset=['Codigo_Articulo', 'Sucursal'], keep='first')\n",
    "# file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv\"\n",
    "# df_sin_duplicados.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafico = {'articulo': 166, 'sucursal': 1,\n",
    "            'fechas': ['2025-02-13', '2025-02-15', '2025-02-17', '2025-02-20', '2025-02-23', '2025-02-24', \n",
    "                       '2025-02-25', '2025-02-28', '2025-03-01', '2025-03-02', '2025-03-04', '2025-03-05', '2025-03-06', \n",
    "                       '2025-03-07', '2025-03-08', '2025-03-11', '2025-03-12', '2025-03-13', '2025-03-14', '2025-03-15', \n",
    "                       '2025-03-18', '2025-03-21', '2025-03-22', '2025-03-23', '2025-03-24', '2025-03-26', '2025-03-28', '2025-03-29', \n",
    "                       '2025-03-30', '2025-03-31', '2025-03-31', '2025-04-01', '2025-04-02', '2025-04-03', '2025-04-04', '2025-04-04'],\n",
    "            'unidades': [4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 3.0, 1.0, 7.0, 3.0, 2.0, 1.0, 1.0, 1.0, 2.0, 4.0, 1.0, 1.0, 1.0,\n",
    "                          2.0, 2.0, 4.0, 1.0, 5.0, 4.0, 1.0, 1.0, 4.0, 2.0, 8.0, 1.0, 3.0], \n",
    "            'media_movil': [0, 0, 0, 0, 0, 0, 1.5714285714285714, 1.2857142857142858, 1.2857142857142858, 1.2857142857142858, 1.5714285714285714, \n",
    "                            1.5714285714285714, 2.4285714285714284, 2.7142857142857144, 2.7142857142857144, 2.5714285714285716, 2.5714285714285716, \n",
    "                            2.2857142857142856, 2.4285714285714284, 2.0, 1.7142857142857142, 1.5714285714285714, 1.5714285714285714, 1.7142857142857142, \n",
    "                            1.8571428571428572, 2.142857142857143, 1.7142857142857142, 2.2857142857142856, 2.7142857142857144, 2.7142857142857144, 2.5714285714285716, \n",
    "                            2.857142857142857, 2.5714285714285716, 3.5714285714285716, 3.0, 2.857142857142857], \n",
    "            'semana_num': [7, 8, 9, 10, 11, 12, 13, 14], \n",
    "            'ventas_semanales': [6.0, 3.0, 7.0, 16.0, 9.0, 5.0, 16.0, 20.0], \n",
    "            'forecast': 15.0, 'ventas_last': 14.0, \n",
    "            'ventas_previous': 7.0, 'ventas_same_year': 11.0, \n",
    "            'average': 1.484, 'ventas_ultimos_30': np.float64(62.0), \n",
    "            'ventas_previos_30': np.float64(20.0), \n",
    "            'ventas_anio_anterior': np.float64(0.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_desde_datos_json(grafico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAR PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nombre del mÃ³dulo: S30_GENERA_Grafico_Detalle.py\n",
    "\n",
    "DescripciÃ³n:\n",
    "Partiendo de los datos extendidos con estado 30, se generan los grÃ¡ficos de detalle para cada artÃ­culo y sucursal.\n",
    "Se guarda el archivo CSV con los datos extendidos y los grÃ¡ficos en formato base64.\n",
    "Se actualiza el estado a 40 en la base de datos.\n",
    "\n",
    "Autor: EWE - Zeetrex\n",
    "Fecha de creaciÃ³n: [2025-03-22]\n",
    "\"\"\"\n",
    "\n",
    "# Solo importar lo necesario desde el mÃ³dulo de funciones\n",
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64,\n",
    "    generar_grafico_json\n",
    ")\n",
    "import os\n",
    "import time\n",
    "import pandas as pd  # uso localmente la lectura de archivos.\n",
    "from datetime import datetime \n",
    "import traceback\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    print(\"ðŸ“Š Insertando GrÃ¡ficos Forecast:   \" + name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    path_ventas = f'{folder}/{name}_Ventas.csv'\n",
    "    path_forecast = f'{folder}/{algoritmo}_Pronostico_Extendido.csv'\n",
    "    path_backup = f'{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv'\n",
    "    path_log = f'{folder}/log_graficos_{name}.txt'\n",
    "\n",
    "    # Cargar historial de ventas\n",
    "    df_ventas = pd.read_csv(path_ventas)\n",
    "    df_ventas['Codigo_Articulo'] = df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal'] = df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Cargar forecast extendido\n",
    "    df_forecast = pd.read_csv(path_forecast)\n",
    "    df_forecast.fillna(0, inplace=True)\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "    # Verificar si ya existe archivo con avances\n",
    "    if os.path.exists(path_backup):\n",
    "        df_backup = pd.read_csv(path_backup)\n",
    "        procesados = set(zip(df_backup['Codigo_Articulo'], df_backup['Sucursal']))\n",
    "        print(f\"ðŸ” Recuperando avance previo: {len(procesados)} registros ya procesados\")\n",
    "    else:\n",
    "        df_backup = pd.DataFrame(columns=list(df_forecast.columns) + ['GRAFICO'])\n",
    "        procesados = set()\n",
    "\n",
    "    nuevos = 0\n",
    "    total = len(df_forecast)\n",
    "\n",
    "    for i, row in df_forecast.iterrows():\n",
    "        clave = (row['Codigo_Articulo'], row['Sucursal'])\n",
    "        if clave in procesados:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            grafico = generar_grafico_base64(\n",
    "                df_ventas,\n",
    "                row['Codigo_Articulo'],\n",
    "                row['Sucursal'],\n",
    "                row['Forecast'],\n",
    "                row['Average'],\n",
    "                row['ventas_last'],\n",
    "                row['ventas_previous'],\n",
    "                row['ventas_same_year']\n",
    "            )\n",
    "            row_data = row.to_dict()\n",
    "            row_data['GRAFICO'] = grafico\n",
    "            df_backup = pd.concat([df_backup, pd.DataFrame([row_data])], ignore_index=True)\n",
    "            nuevos += 1\n",
    "\n",
    "            if nuevos % 50 == 0 or i == total - 1:\n",
    "                df_backup.to_csv(path_backup, index=False)\n",
    "                elapsed = round(time.time() - start_time, 2)\n",
    "                print(f\"ðŸ–¼ï¸ Procesados {nuevos} nuevos registros ({i+1}/{total}) - Tiempo: {elapsed} seg\")\n",
    "                with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                    log.write(f\"[{datetime.now()}] {nuevos} registros procesados ({i+1}/{total}) - Tiempo: {elapsed} seg\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error procesando grÃ¡fico para Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\")\n",
    "            with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"[{datetime.now()}] ERROR Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "    # Guardar completo al final\n",
    "    df_backup.to_csv(path_backup, index=False)\n",
    "    elapsed = round(time.time() - start_time, 2)\n",
    "    print(f\"âœ… Finalizado: {name} - Total nuevos: {nuevos} - Tiempo total: {elapsed} segundos\")\n",
    "    with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"[{datetime.now()}] FINALIZADO: {nuevos} registros nuevos - Tiempo total: {elapsed} seg\\n\")\n",
    "\n",
    "    return df_backup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "    fes = get_execution_execute_by_status(30)\n",
    "\n",
    "    # Filtrar registros con supply_forecast_execution_status_id = 30  #FORECAST con DFATOSK\n",
    "    for index, row in fes[fes[\"fee_status_id\"].isin([30])].iterrows():\n",
    "        algoritmo = row[\"name\"] \n",
    "        name = algoritmo.split('_ALGO')[0]\n",
    "        execution_id = row[\"forecast_execution_id\"]\n",
    "        id_proveedor = row[\"ext_supplier_code\"]\n",
    "        forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "\n",
    "        print(f\"Algoritmo: {algoritmo}  - Name: {name}  exce_id: {execution_id}  Proveedor: {id_proveedor}\")\n",
    "\n",
    "        try:\n",
    "            # Estado intermedio: 35 (procesando grÃ¡ficos)\n",
    "            print(f\"ðŸ›  Marcando como 'Procesando GrÃ¡ficos' para {execution_id}\")\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=35)\n",
    "            print(f\"ðŸ›  Iniciando graficaciÃ³n para {execution_id}...\")\n",
    "\n",
    "            # GeneraciÃ³n del dataframe extendido con grÃ¡ficos\n",
    "            df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "            # Guardar el CSV con datos extendidos y grÃ¡ficos\n",
    "            file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "            df_merged.to_csv(file_path, index=False)\n",
    "            print(f\"ðŸ“ Archivo guardado correctamente: {file_path}\")\n",
    "\n",
    "            # âœ… Solo si todo fue exitoso, actualizamos el estado a 40\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=40)\n",
    "            print(f\"âœ… Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"âŒ Error procesando {name}: {e}\")\n",
    "            \n",
    "            log_path = os.path.join(folder, \"errores_s30.log\")\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{name}] ID: {execution_id} - ERROR: {str(e)}\\n\")\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUEVO ENFOQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PLOTLY\n",
    "* PARALELIZADO\n",
    "* PRECALCULO de los DATAFRAMES\n",
    "\n",
    "*Mejora\tImplementada*\n",
    "* Pre-filtrado de datos\tâœ…\n",
    "* Cache por combinaciÃ³n\tâœ…\n",
    "* Multiprocesamiento\tâœ…\n",
    "* MigraciÃ³n a Plotly\tâœ…\n",
    "* ExportaciÃ³n Base64\tâœ…\n",
    "\n",
    "Ya he migrado el mÃ³dulo S30_GENERA_Grafico_Detalle.py al nuevo enfoque que incluye:\n",
    "\n",
    "âœ… PrecÃ¡lculo de los dataframes por producto y sucursal.\n",
    "\n",
    "âœ… ParalelizaciÃ³n con multiprocessing para mejorar la performance.\n",
    "\n",
    "âœ… Uso de la nueva funciÃ³n generar_grafico_base64_plotly con Plotly para generaciÃ³n de grÃ¡ficos.\n",
    "\n",
    "âœ… Soporte para recuperaciÃ³n de avance previo y persistencia incremental.\n",
    "\n",
    "âœ… Registro en archivo de log y manejo robusto de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: 1074_COCA_ALGO_01  - Name: 1074_COCA  exce_id: 92b1a2e3-31f0-4f5a-93cd-f0ac1028091d  Proveedor: 1074\n",
      "ðŸ›  Marcando como 'Procesando GrÃ¡ficos' para 92b1a2e3-31f0-4f5a-93cd-f0ac1028091d\n",
      "ðŸ›  Iniciando graficaciÃ³n para 92b1a2e3-31f0-4f5a-93cd-f0ac1028091d...\n",
      "-> Datos Recuperados del CACHE: 1074, Label: 1074_COCA\n",
      "ðŸ“ Archivo guardado correctamente: data/1074_COCA_ALGO_01_Pronostico_Extendido_FINAL.csv\n",
      "âœ… Estado actualizado a 40 para 92b1a2e3-31f0-4f5a-93cd-f0ac1028091d\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_json,   # Se asume que esta es la nueva funciÃ³n con Plotly\n",
    "    generar_reporte_json,\n",
    "    insertar_graficos_json,\n",
    "    graficar_desde_datos_json\n",
    "    \n",
    ")\n",
    "\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "def procesar_datos_para_graficos_json(algoritmo, name, id_proveedor):\n",
    "        \n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtrÃ³ algÃºn missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de grÃ¡ficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    \n",
    "    for index, row in df_forecast.iterrows():\n",
    "        if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]):\n",
    "            preparar_y_guardar_datos_para_grafico(\n",
    "                df_ventas, \n",
    "                row[\"Codigo_Articulo\"], \n",
    "                row[\"Sucursal\"], \n",
    "                row[\"Forecast\"], \n",
    "                row[\"Average\"], \n",
    "                row[\"ventas_last\"], \n",
    "                row[\"ventas_previous\"], \n",
    "                row[\"ventas_same_year\"]\n",
    "            )\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def preparar_y_guardar_datos_para_grafico(dfv, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    path_salida = f'{folder}/{name}_{articulo}_{sucursal}.json'\n",
    "    \n",
    "    \n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "    df_filtrado = dfv[(dfv[\"Codigo_Articulo\"] == articulo) & (dfv[\"Sucursal\"] == sucursal)]\n",
    "    df_filtrado = df_filtrado[df_filtrado[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=50))]\n",
    "\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Media_Movil\"].fillna(0)\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\")[\"Unidades\"].sum().reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_filtrado.groupby(\"Semana\")[\"Fecha\"].min().reset_index()[\"Fecha\"].dt.isocalendar().week.astype(int)\n",
    "    df_semanal[\"Media_Movil\"] = df_semanal[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Fechas de comparaciÃ³n\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    ventas_ultimos_30 = df_filtrado[(df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30)][\"Unidades\"].sum()\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    df_filtrado_anio_anterior = df_filtrado.copy()\n",
    "    df_filtrado_anio_anterior[\"Fecha\"] = df_filtrado_anio_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_mismo_periodo_anio_anterior = df_filtrado_anio_anterior[\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) &\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    datos = {\n",
    "        \"articulo\": articulo,\n",
    "        \"sucursal\": sucursal,\n",
    "        \"fechas\": df_filtrado[\"Fecha\"].dt.strftime(\"%Y-%m-%d\").tolist(),\n",
    "        \"unidades\": df_filtrado[\"Unidades\"].tolist(),\n",
    "        \"media_movil\": df_filtrado[\"Media_Movil\"].tolist(),\n",
    "        \"semana_num\": df_semanal[\"Semana_Num\"].tolist(),\n",
    "        \"ventas_semanales\": df_semanal[\"Unidades\"].tolist(),\n",
    "        \"forecast\": Forecast,  # completar al momento de la ejecuciÃ³n si no se conoce antes\n",
    "        \"ventas_last\": ventas_last,\n",
    "        \"ventas_previous\": ventas_previous,\n",
    "        \"ventas_same_year\": ventas_same_year,\n",
    "        \"average\": Average,\n",
    "        \"ventas_ultimos_30\": ventas_ultimos_30,\n",
    "        \"ventas_previos_30\": ventas_previos_30,\n",
    "        \"ventas_anio_anterior\": ventas_mismo_periodo_anio_anterior\n",
    "    }\n",
    "\n",
    "    with open(path_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(datos, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fes = get_execution_execute_by_status(50)\n",
    "\n",
    "    # for index, row in fes[fes[\"fee_status_id\"].isin([30])].iterrows():\n",
    "    for index, row in fes[fes[\"name\"].isin(['1074_COCA_ALGO_01'])].iterrows():\n",
    "        algoritmo = row[\"name\"] \n",
    "        name = algoritmo.split('_ALGO')[0]\n",
    "        execution_id = row[\"forecast_execution_id\"]\n",
    "        id_proveedor = row[\"ext_supplier_code\"]\n",
    "        forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "\n",
    "        print(f\"Algoritmo: {algoritmo}  - Name: {name}  exce_id: {execution_id}  Proveedor: {id_proveedor}\")\n",
    "\n",
    "        try:\n",
    "            print(f\"ðŸ›  Marcando como 'Procesando GrÃ¡ficos' para {execution_id}\")\n",
    "            #update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=35)\n",
    "            print(f\"ðŸ›  Iniciando graficaciÃ³n para {execution_id}...\")\n",
    "            \n",
    "            # procesar_datos_para_graficos_json(algoritmo, name, id_proveedor)\n",
    "\n",
    "            df_merged = insertar_graficos_json(algoritmo, name, id_proveedor)\n",
    "\n",
    "            file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido_FINAL.csv\"\n",
    "            \n",
    "            df_merged.to_csv(file_path, index=False)\n",
    "            print(f\"ðŸ“ Archivo guardado correctamente: {file_path}\")\n",
    "\n",
    "            #update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=40)\n",
    "            print(f\"âœ… Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"âŒ Error procesando {name}: {e}\")\n",
    "\n",
    "            log_path = os.path.join(folder, \"errores_s30.log\")\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{name}] ID: {execution_id} - ERROR: {str(e)}\\n\")\n",
    "\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forecast.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64\n",
    ")\n",
    "import os\n",
    "import time\n",
    "import pandas as pd  # uso localmente la lectura de archivos.\n",
    "from datetime import datetime \n",
    "import traceback\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "\n",
    "fes = get_execution_execute_by_status(40)\n",
    "\n",
    "# Mostrar la tabla con los grÃ¡ficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requiere: pandas, plotly, kaleido (para exportar imÃ¡genes)\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Suponiendo que dfv es el DataFrame original con columnas: \n",
    "# \"Codigo_Articulo\", \"Sucursal\", \"Fecha\", \"Unidades\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. FUNCIÃ“N PARA GENERAR GRÃFICO\n",
    "# -------------------------------\n",
    "\n",
    "def generar_grafico_base64_plotly(df_filtrado, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Ventas Diarias\",\n",
    "            \"Histograma de Ventas Semanales\",\n",
    "            \"Forecast vs Ventas Anteriores\",\n",
    "            \"ComparaciÃ³n de Ventas en 3 PerÃ­odos\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=f\"Demanda Articulo {articulo} - Sucursal {sucursal}\", height=800, width=1000)\n",
    "\n",
    "    # Media mÃ³vil\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # GrÃ¡fico 1\n",
    "    fig.add_trace(go.Scatter(x=df_filtrado[\"Fecha\"], y=df_filtrado[\"Unidades\"], mode='lines+markers', name='Ventas'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_filtrado[\"Fecha\"], y=df_filtrado[\"Media_Movil\"], mode='lines', name='Media MÃ³vil', line=dict(dash='dash')), row=1, col=1)\n",
    "\n",
    "    # GrÃ¡fico 2\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\").agg({\"Unidades\": \"sum\", \"Fecha\": \"min\"}).reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_semanal[\"Fecha\"].dt.isocalendar().week\n",
    "    fig.add_trace(go.Bar(x=df_semanal[\"Semana_Num\"], y=df_semanal[\"Unidades\"], name=\"Semanas\"), row=1, col=2)\n",
    "\n",
    "    # GrÃ¡fico 3\n",
    "    fig.add_trace(go.Bar(x=[\"Forecast\", \"Actual\", \"Anterior\", \"AÃ±o Ant\"],\n",
    "                        y=[Forecast, ventas_last, ventas_previous, ventas_same_year],\n",
    "                        name=\"ComparaciÃ³n\"), row=2, col=1)\n",
    "\n",
    "    # GrÃ¡fico 4: cÃ¡lculo perÃ­odos\n",
    "    fecha_maxima = df_filtrado[\"Fecha\"].max()\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    ventas_ultimos_30 = df_filtrado[df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30][\"Unidades\"].sum()\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "    df_anterior = df_filtrado.copy()\n",
    "    df_anterior[\"Fecha\"] = df_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_anio_anterior = df_anterior[\n",
    "        (df_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) & (df_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    fig.add_trace(go.Bar(x=[\"Ãšltimos 30\", \"Anteriores 30\", \"AÃ±o anterior\", \"Average\"],\n",
    "                        y=[ventas_ultimos_30, ventas_previos_30, ventas_anio_anterior, Average],\n",
    "                        name=\"ComparaciÃ³n temporal\"), row=2, col=2)\n",
    "\n",
    "    # Exportar a base64\n",
    "    buffer = BytesIO()\n",
    "    fig.write_image(buffer, format=\"png\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. FUNCIÃ“N DE PROCESO INDIVIDUAL\n",
    "# -------------------------------\n",
    "\n",
    "def procesar_item(args):\n",
    "    articulo, sucursal, df_filtrado, Forecast, Average, ventas_last, ventas_previous, ventas_same_year = args\n",
    "    try:\n",
    "        return (articulo, sucursal, generar_grafico_base64_plotly(\n",
    "            df_filtrado, articulo, sucursal,\n",
    "            Forecast, Average, ventas_last, ventas_previous, ventas_same_year\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {articulo}-{sucursal}: {e}\")\n",
    "        return (articulo, sucursal, None)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. PRECALCULAR DATAFRAMES FILTRADOS\n",
    "# -------------------------------\n",
    "\n",
    "def preparar_y_procesar(dfv, resumen_ventas):\n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "\n",
    "    # PrecÃ¡lculo por producto-sucursal\n",
    "    pares = dfv[[\"Codigo_Articulo\", \"Sucursal\"]].drop_duplicates()\n",
    "    dict_df = {}\n",
    "    for row in pares.itertuples(index=False):\n",
    "        df_filt = dfv[\n",
    "            (dfv[\"Codigo_Articulo\"] == row.Codigo_Articulo) &\n",
    "            (dfv[\"Sucursal\"] == row.Sucursal) &\n",
    "            (dfv[\"Fecha\"] >= fecha_maxima - pd.Timedelta(days=50))\n",
    "        ].copy()\n",
    "        dict_df[(row.Codigo_Articulo, row.Sucursal)] = df_filt\n",
    "\n",
    "    # Empaquetar datos de entrada\n",
    "    lista_entradas = []\n",
    "    for (articulo, sucursal), df_filtrado in dict_df.items():\n",
    "        # Obtener mÃ©tricas simuladas para el ejemplo\n",
    "        Forecast = resumen_ventas.get((articulo, sucursal), {}).get(\"Forecast\", 0)\n",
    "        Average = resumen_ventas.get((articulo, sucursal), {}).get(\"Average\", 0)\n",
    "        ventas_last = resumen_ventas.get((articulo, sucursal), {}).get(\"Last\", 0)\n",
    "        ventas_previous = resumen_ventas.get((articulo, sucursal), {}).get(\"Previous\", 0)\n",
    "        ventas_same_year = resumen_ventas.get((articulo, sucursal), {}).get(\"SameYear\", 0)\n",
    "\n",
    "        lista_entradas.append((\n",
    "            articulo, sucursal, df_filtrado,\n",
    "            Forecast, Average, ventas_last, ventas_previous, ventas_same_year\n",
    "        ))\n",
    "\n",
    "    # Multiprocesamiento\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        resultados = pool.map(procesar_item, lista_entradas)\n",
    "\n",
    "    # Retorno en dict\n",
    "    return {(art, suc): b64img for art, suc, b64img in resultados if b64img is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in fes[fes[\"fee_status_id\"] == 40].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"forecast_execution_id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "\n",
    "    print(\"ðŸ“Š Insertando GrÃ¡ficos Forecast:   \" + name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    path_ventas = f'{folder}/{name}_Ventas.csv'\n",
    "    path_forecast = f'{folder}/{algoritmo}_Pronostico_Extendido.csv'\n",
    "    path_backup = f'{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv'\n",
    "    path_log = f'{folder}/log_graficos_{name}.txt'\n",
    "\n",
    "    # Cargar historial de ventas\n",
    "    df_ventas = pd.read_csv(path_ventas)\n",
    "    df_ventas['Codigo_Articulo'] = df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal'] = df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Cargar forecast extendido\n",
    "    df_forecast = pd.read_csv(path_forecast)\n",
    "    df_forecast.fillna(0, inplace=True)\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevos = 0\n",
    "total = len(df_forecast)\n",
    "    \n",
    "for i, row in df_forecast.iterrows():\n",
    "        # clave = (row['Codigo_Articulo'], row['Sucursal'])\n",
    "        # if clave in procesados:\n",
    "        #     continue\n",
    "\n",
    "        try:\n",
    "            grafico = generar_grafico_base64(\n",
    "                df_ventas,\n",
    "                row['Codigo_Articulo'],\n",
    "                row['Sucursal'],\n",
    "                row['Forecast'],\n",
    "                row['Average'],\n",
    "                row['ventas_last'],\n",
    "                row['ventas_previous'],\n",
    "                row['ventas_same_year']\n",
    "            )\n",
    "            row_data = row.to_dict()\n",
    "            row_data['GRAFICO'] = grafico\n",
    "            df_backup = pd.concat([df_backup, pd.DataFrame([row_data])], ignore_index=True)\n",
    "            nuevos += 1\n",
    "\n",
    "            if nuevos % 50 == 0 or i == total - 1:\n",
    "                df_backup.to_csv(path_backup, index=False)\n",
    "                elapsed = round(time.time() - start_time, 2)\n",
    "                print(f\"ðŸ–¼ï¸ Procesados {nuevos} nuevos registros ({i+1}/{total}) - Tiempo: {elapsed} seg\")\n",
    "                with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                    log.write(f\"[{datetime.now()}] {nuevos} registros procesados ({i+1}/{total}) - Tiempo: {elapsed} seg\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error procesando grÃ¡fico para Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\")\n",
    "            with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"[{datetime.now()}] ERROR Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\\n\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
