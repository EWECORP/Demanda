{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S30 - RUTINA GENERACIÓN DE GRÁFICOS.\n",
    "\n",
    "Parte de los forecast executión que están en estado 30 (Ya Ejecutado el Forecast), Genera en los archivos locales de detalle los gráficos a nivel línea,\n",
    "\n",
    "Los deja en archivos locales algoritmo_Pronostico_Extendido y Actualiza esl estado de 30 a 40  Graficos OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUTINA GENERADORA DE GRÁFICOS\n",
    "\n",
    "1) Leer archivo Solicitudes_Compra\n",
    "2) Leer datos adicionales y id relacionados\n",
    "3) Leer datos adicionales de la T710_Estadis_Reposición\n",
    "4) Generar GRAFICOS\n",
    "5) Actulizar Estado en connexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA GENERADORA GLOBAL DE GRÄRICOS\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# LIBRERIAS NECESARIAS \n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "import psycopg2 as pg2    # Conectores para Postgres\n",
    "import getpass  # Para obtener el usuario del sistema operativo\n",
    "import uuid  # Importar la librería uuid\n",
    "# Mostrar el DataFrame resultante\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Evitar Mensajes Molestos\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)\n",
    "\n",
    "secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "\n",
    "def generar_grafico_base64(dfv, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "    df_filtrado = dfv[(dfv[\"Codigo_Articulo\"] == articulo) & (dfv[\"Sucursal\"] == sucursal)]\n",
    "    df_filtrado = df_filtrado[df_filtrado[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=50))]\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 6), nrows= 2, ncols= 2\n",
    "    )\n",
    "    fig.suptitle(f\"Demanda Articulo {articulo} - Sucursal {sucursal}\")\n",
    "    current_ax = 0\n",
    "    #Bucle para Llenar los gráficos\n",
    "    colors =[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "    # Ventas Diarias\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Ventas Diarias\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Unidades\"], marker=\"o\", linestyle=\"-\", label=\"Ventas\", color=colors[0])\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Media_Movil\"], linestyle=\"--\", label=\"Media Móvil (7 días)\", color=\"black\")\n",
    "    ax[0, 0].set_title(\"Ventas Diarias\")\n",
    "    ax[0, 0].legend()\n",
    "    ax[0, 0].set_xlabel(\"Fecha\")\n",
    "    ax[0, 0].set_ylabel(\"Unidades\")\n",
    "    ax[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Ventas Semanales\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\")[\"Unidades\"].sum().reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_filtrado.groupby(\"Semana\")[\"Fecha\"].min().reset_index()[\"Fecha\"].dt.isocalendar().week.astype(int)\n",
    "    df_semanal[\"Media_Movil\"] = df_semanal[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Histograma de ventas semanales\n",
    "    ax[0, 1].bar(df_semanal[\"Semana_Num\"], df_semanal[\"Unidades\"], color=[colors[1],colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[0, 1].set_xlabel(\"Semana del Año\")\n",
    "    ax[0, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[0, 1].set_title(\"Histograma de Ventas Semanales\")\n",
    "    ax[0, 1].tick_params(axis='x', rotation=60)\n",
    "    ax[0, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Graficar el Forecast vs Ventas Reales en la tercera celda\n",
    "    labels = [\"Forecast\",\"Actual\", \"Anterior\", \"Año Ant\"]\n",
    "    values = [Forecast, ventas_last, ventas_previous, ventas_same_year]\n",
    "\n",
    "    ax[1, 0].bar(labels, values, color=[colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[1, 0].set_title(\"Forecast vs Ventas Anteriores\")\n",
    "    ax[1, 0].set_ylabel(\"Unidades\")\n",
    "    ax[1, 0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Definir fechas de referencia\n",
    "    fecha_maxima = df_filtrado[\"Fecha\"].max()\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    # Calcular ventas de los últimos 30 días\n",
    "    ventas_ultimos_30 = df_filtrado[(df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30)][\"Unidades\"].sum()\n",
    "\n",
    "    # Calcular ventas de los 30 días previos a los últimos 30 días\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # Simulación de datos para las ventas del año anterior\n",
    "    df_filtrado_anio_anterior = df_filtrado.copy()\n",
    "    df_filtrado_anio_anterior[\"Fecha\"] = df_filtrado_anio_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_mismo_periodo_anio_anterior = df_filtrado_anio_anterior[\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) &\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # Datos para el histograma\n",
    "    labels = [\"Últimos 30\", \"Anteriores 30\", \"Año anterior\", \"Average\"]\n",
    "    values = [ventas_ultimos_30, ventas_previos_30, ventas_mismo_periodo_anio_anterior, Average]\n",
    "\n",
    "    # Graficar el histograma en la celda [1,1]\n",
    "    ax[1, 1].bar(labels, values, color=[colors[0], colors[1], colors[2]], alpha=0.7)\n",
    "    ax[1, 1].set_title(\"Comparación de Ventas en 3 Períodos\")\n",
    "    ax[1, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[1, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Mostrar el gráfico\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para no solapar con el título\n",
    "\n",
    "    # Guardar gráfico en base64\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de conexión a la base de datos\n",
    "def Open_Conn_Postgres():\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE4']} user={secrets['USUARIO4']} password={secrets['CONTRASENA4']} host={secrets['SERVIDOR4']} port={secrets['PUERTO4']}\"\n",
    "    try:    \n",
    "        conn = pg2.connect(conn_str)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        return None\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "# Helper para generar identificadores únicos\n",
    "def id_aleatorio():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def get_execution(execution_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, description, name, \"timestamp\", supply_forecast_model_id, \n",
    "                ext_supplier_code, supplier_id, supply_forecast_execution_status_id\n",
    "            FROM public.spl_supply_forecast_execution\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (execution_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"description\": row[1],\n",
    "                \"name\": row[2],\n",
    "                \"timestamp\": row[3],\n",
    "                \"supply_forecast_model_id\": row[4],\n",
    "                \"ext_supplier_code\": row[5],\n",
    "                \"supplier_id\": row[6],\n",
    "                \"supply_forecast_execution_status_id\": row[7]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution(execution_id, **kwargs):\n",
    "    if not kwargs:\n",
    "        print(\"No hay valores para actualizar\")\n",
    "        return None\n",
    "\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(execution_id)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution(execution_id)  # Retorna la ejecución actualizada\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_by_status(status):\n",
    "    if not status:\n",
    "        print(\"No hay estados para filtrar\")\n",
    "        return None\n",
    "    \n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT id, description, name, \"timestamp\", supply_forecast_model_id, ext_supplier_code, graphic, \n",
    "                monthly_net_margin_in_millions, monthly_purchases_in_millions, monthly_sales_in_millions, sotck_days, sotck_days_colors, \n",
    "                supplier_id, supply_forecast_execution_status_id\n",
    "                FROM public.spl_supply_forecast_execution\n",
    "                WHERE supply_forecast_execution_status_id = {status};\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        fexsts = pd.read_sql(query, conn)\n",
    "        return fexsts\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_status: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn) \n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperar Maestro de Artículos\n",
    "    articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de gráficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    df_forecast[\"GRAFICO\"] = df_forecast.apply(\n",
    "        lambda row: generar_grafico_base64(df_ventas, row[\"Codigo_Articulo\"], row[\"Sucursal\"], row[\"Forecast\"], row[\"Average\"], row[\"ventas_last\"], row[\"ventas_previous\"], row[\"ventas_same_year\"]) if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Leer Dataframe de los PRODUCTOS\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT ext_code, description, id FROM public.fnd_product\n",
    "    ORDER BY ext_code;\n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    products = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "    products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "    #Unir los dataframes por Codigo_Articulo = ext_code\n",
    "    df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "    df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "    df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "    df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['code','name'], inplace=True)\n",
    "    \n",
    "    # SUBIR INFORMACIÓN DE ARTICULOS y ESTIDISTICA REPOSICIÓN\n",
    "    # Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "    columnas_seleccionadas = [\n",
    "        'C_ARTICULO', 'C_SUCU_EMPR', 'F_ULTIMA_VTA', 'Q_VTA_ACUM', 'Q_UNID_PESO_VTA_MES_ACTUAL', 'Q_VTA_ULTIMOS_15DIAS',\n",
    "        'Q_VTA_ULTIMOS_30DIAS', 'Q_TRANSF_PEND', 'Q_TRANSF_EN_PREP','Q_VENTA_30_DIAS',\n",
    "        'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_VENTA_ESPECIAL_30_DIAS','Q_VENTA_ESPECIAL_15_DIAS', 'Q_DIAS_CON_STOCK', 'Q_REPONER',\n",
    "        'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL','Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "    ]\n",
    "\n",
    "    # Filtrar el dataframe con las columnas seleccionadas\n",
    "    df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "    df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "    \n",
    "    # Realizar la fusión de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "    df_merged = df_merged.merge(\n",
    "        df_nuevo, \n",
    "        left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "        right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "        how='left'\n",
    "    )\n",
    "    df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer Dataframe de FORECAST EXECUTION \n",
    "        \n",
    "fes = get_execution_by_status(30)\n",
    "\n",
    "# Mostrar la tabla con los gráficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL RECORRE FORECAST EXCEC con STATUS 30 y pasa a 40\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 10  #FORECAST OK\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 30].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        # Llamar a la función que genera los gráficos y datos extendidos\n",
    "        df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_merged.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        fe.at[index, \"supply_forecast_execution_status_id\"] = 40\n",
    "        # ✅ Actualizar directamente en la base de datos el estado a 40\n",
    "        update_execution(execution_id, supply_forecast_execution_status_id=40)\n",
    "        print(f\"Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAR PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nombre del módulo: S30_GENERA_Grafico_Detalle.py\n",
    "\n",
    "Descripción:\n",
    "Partiendo de los datos extendidos con estado 30, se generan los gráficos de detalle para cada artículo y sucursal.\n",
    "Se guarda el archivo CSV con los datos extendidos y los gráficos en formato base64.\n",
    "Se actualiza el estado a 40 en la base de datos.\n",
    "\n",
    "Autor: EWE - Zeetrex\n",
    "Fecha de creación: [2025-03-22]\n",
    "\"\"\"\n",
    "\n",
    "# Solo importar lo necesario desde el módulo de funciones\n",
    "from funciones_forecast import (\n",
    "    Open_Conn_Postgres,\n",
    "    Close_Connection,\n",
    "    get_execution_by_status,\n",
    "    update_execution,\n",
    "    generar_grafico_base64\n",
    ")\n",
    "\n",
    "import pandas as pd # uso localmente la lectura de archivos.\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "# También podés importar funciones adicionales si tu módulo las necesita\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    \n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de gráficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    df_forecast[\"GRAFICO\"] = df_forecast.apply(\n",
    "        lambda row: generar_grafico_base64(df_ventas, row[\"Codigo_Articulo\"], row[\"Sucursal\"], row[\"Forecast\"], row[\"Average\"], row[\"ventas_last\"], row[\"ventas_previous\"], row[\"ventas_same_year\"]) if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "    fes = get_execution_by_status(30)\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 30  #FORECAST con DFATOSK\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 30].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        # Actualizar el status_id a 35 Procesando Graficos....\n",
    "\n",
    "        update_execution(execution_id, supply_forecast_execution_status_id=35)\n",
    "        print(f\"Iniciando Graficación Local..... {execution_id}\")\n",
    "        \n",
    "        # Llamar a la función que genera los gráficos y datos extendidos\n",
    "        df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_merged.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "        \n",
    "\n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        fes.at[index, \"supply_forecast_execution_status_id\"] = 40\n",
    "        # ✅ Actualizar directamente en la base de datos el estado a 40\n",
    "        update_execution(execution_id, supply_forecast_execution_status_id=40)\n",
    "        print(f\"Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
