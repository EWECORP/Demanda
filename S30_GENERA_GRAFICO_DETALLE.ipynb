{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S30 - RUTINA GENERACI√ìN DE GR√ÅFICOS.\n",
    "\n",
    "Parte de los forecast executi√≥n que est√°n en estado 30 (Ya Ejecutado el Forecast), Genera en los archivos locales de detalle los gr√°ficos a nivel l√≠nea,\n",
    "\n",
    "Los deja en archivos locales algoritmo_Pronostico_Extendido y Actualiza esl estado de 30 a 40  Graficos OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUTINA GENERADORA DE GR√ÅFICOS\n",
    "\n",
    "1) Leer archivo Solicitudes_Compra\n",
    "2) Leer datos adicionales y id relacionados\n",
    "3) Leer datos adicionales de la T710_Estadis_Reposici√≥n\n",
    "4) Generar GRAFICOS\n",
    "5) Actulizar Estado en connexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA GENERADORA GLOBAL DE GR√ÑRICOS\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# LIBRERIAS NECESARIAS \n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "import psycopg2 as pg2    # Conectores para Postgres\n",
    "import getpass  # Para obtener el usuario del sistema operativo\n",
    "import uuid  # Importar la librer√≠a uuid\n",
    "# Mostrar el DataFrame resultante\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Evitar Mensajes Molestos\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)\n",
    "\n",
    "secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "\n",
    "def generar_grafico_base64(dfv, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "    df_filtrado = dfv[(dfv[\"Codigo_Articulo\"] == articulo) & (dfv[\"Sucursal\"] == sucursal)]\n",
    "    df_filtrado = df_filtrado[df_filtrado[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=50))]\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 6), nrows= 2, ncols= 2\n",
    "    )\n",
    "    fig.suptitle(f\"Demanda Articulo {articulo} - Sucursal {sucursal}\")\n",
    "    current_ax = 0\n",
    "    #Bucle para Llenar los gr√°ficos\n",
    "    colors =[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "    # Ventas Diarias\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Ventas Diarias\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Unidades\"], marker=\"o\", linestyle=\"-\", label=\"Ventas\", color=colors[0])\n",
    "    ax[0, 0].plot(df_filtrado[\"Fecha\"], df_filtrado[\"Media_Movil\"], linestyle=\"--\", label=\"Media M√≥vil (7 d√≠as)\", color=\"black\")\n",
    "    ax[0, 0].set_title(\"Ventas Diarias\")\n",
    "    ax[0, 0].legend()\n",
    "    ax[0, 0].set_xlabel(\"Fecha\")\n",
    "    ax[0, 0].set_ylabel(\"Unidades\")\n",
    "    ax[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Ventas Semanales\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\")[\"Unidades\"].sum().reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_filtrado.groupby(\"Semana\")[\"Fecha\"].min().reset_index()[\"Fecha\"].dt.isocalendar().week.astype(int)\n",
    "    df_semanal[\"Media_Movil\"] = df_semanal[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Histograma de ventas semanales\n",
    "    ax[0, 1].bar(df_semanal[\"Semana_Num\"], df_semanal[\"Unidades\"], color=[colors[1],colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[0, 1].set_xlabel(\"Semana del A√±o\")\n",
    "    ax[0, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[0, 1].set_title(\"Histograma de Ventas Semanales\")\n",
    "    ax[0, 1].tick_params(axis='x', rotation=60)\n",
    "    ax[0, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Graficar el Forecast vs Ventas Reales en la tercera celda\n",
    "    labels = [\"Forecast\",\"Actual\", \"Anterior\", \"A√±o Ant\"]\n",
    "    values = [Forecast, ventas_last, ventas_previous, ventas_same_year]\n",
    "\n",
    "    ax[1, 0].bar(labels, values, color=[colors[2], colors[3], colors[4], colors[5]], alpha=0.7)\n",
    "    ax[1, 0].set_title(\"Forecast vs Ventas Anteriores\")\n",
    "    ax[1, 0].set_ylabel(\"Unidades\")\n",
    "    ax[1, 0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Definir fechas de referencia\n",
    "    fecha_maxima = df_filtrado[\"Fecha\"].max()\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    # Calcular ventas de los √∫ltimos 30 d√≠as\n",
    "    ventas_ultimos_30 = df_filtrado[(df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30)][\"Unidades\"].sum()\n",
    "\n",
    "    # Calcular ventas de los 30 d√≠as previos a los √∫ltimos 30 d√≠as\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # Simulaci√≥n de datos para las ventas del a√±o anterior\n",
    "    df_filtrado_anio_anterior = df_filtrado.copy()\n",
    "    df_filtrado_anio_anterior[\"Fecha\"] = df_filtrado_anio_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_mismo_periodo_anio_anterior = df_filtrado_anio_anterior[\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) &\n",
    "        (df_filtrado_anio_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    # Datos para el histograma\n",
    "    labels = [\"√öltimos 30\", \"Anteriores 30\", \"A√±o anterior\", \"Average\"]\n",
    "    values = [ventas_ultimos_30, ventas_previos_30, ventas_mismo_periodo_anio_anterior, Average]\n",
    "\n",
    "    # Graficar el histograma en la celda [1,1]\n",
    "    ax[1, 1].bar(labels, values, color=[colors[0], colors[1], colors[2]], alpha=0.7)\n",
    "    ax[1, 1].set_title(\"Comparaci√≥n de Ventas en 3 Per√≠odos\")\n",
    "    ax[1, 1].set_ylabel(\"Unidades Vendidas\")\n",
    "    ax[1, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Mostrar el gr√°fico\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para no solapar con el t√≠tulo\n",
    "\n",
    "    # Guardar gr√°fico en base64\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de conexi√≥n a la base de datos\n",
    "def Open_Conn_Postgres():\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE4']} user={secrets['USUARIO4']} password={secrets['CONTRASENA4']} host={secrets['SERVIDOR4']} port={secrets['PUERTO4']}\"\n",
    "    try:    \n",
    "        conn = pg2.connect(conn_str)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexi√≥n: {e}')\n",
    "        return None\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "# Helper para generar identificadores √∫nicos\n",
    "def id_aleatorio():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def get_execution(execution_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, description, name, \"timestamp\", supply_forecast_model_id, \n",
    "                ext_supplier_code, supplier_id, supply_forecast_execution_status_id\n",
    "            FROM public.spl_supply_forecast_execution\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (execution_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"description\": row[1],\n",
    "                \"name\": row[2],\n",
    "                \"timestamp\": row[3],\n",
    "                \"supply_forecast_model_id\": row[4],\n",
    "                \"ext_supplier_code\": row[5],\n",
    "                \"supplier_id\": row[6],\n",
    "                \"supply_forecast_execution_status_id\": row[7]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution(execution_id, **kwargs):\n",
    "    if not kwargs:\n",
    "        print(\"No hay valores para actualizar\")\n",
    "        return None\n",
    "\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(execution_id)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution(execution_id)  # Retorna la ejecuci√≥n actualizada\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_by_status(status):\n",
    "    if not status:\n",
    "        print(\"No hay estados para filtrar\")\n",
    "        return None\n",
    "    \n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT id, description, name, \"timestamp\", supply_forecast_model_id, ext_supplier_code, graphic, \n",
    "                monthly_net_margin_in_millions, monthly_purchases_in_millions, monthly_sales_in_millions, sotck_days, sotck_days_colors, \n",
    "                supplier_id, supply_forecast_execution_status_id\n",
    "                FROM public.spl_supply_forecast_execution\n",
    "                WHERE supply_forecast_execution_status_id = {status};\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        fexsts = pd.read_sql(query, conn)\n",
    "        return fexsts\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_status: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn) \n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperar Maestro de Art√≠culos\n",
    "    articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtr√≥ alg√∫n missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de gr√°ficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    df_forecast[\"GRAFICO\"] = df_forecast.apply(\n",
    "        lambda row: generar_grafico_base64(df_ventas, row[\"Codigo_Articulo\"], row[\"Sucursal\"], row[\"Forecast\"], row[\"Average\"], row[\"ventas_last\"], row[\"ventas_previous\"], row[\"ventas_same_year\"]) if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Leer Dataframe de los PRODUCTOS\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT ext_code, description, id FROM public.fnd_product\n",
    "    ORDER BY ext_code;\n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    products = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "    products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "    #Unir los dataframes por Codigo_Articulo = ext_code\n",
    "    df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "    df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "    df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "    df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['code','name'], inplace=True)\n",
    "    \n",
    "    # SUBIR INFORMACI√ìN DE ARTICULOS y ESTIDISTICA REPOSICI√ìN\n",
    "    # Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "    columnas_seleccionadas = [\n",
    "        'C_ARTICULO', 'C_SUCU_EMPR', 'F_ULTIMA_VTA', 'Q_VTA_ACUM', 'Q_UNID_PESO_VTA_MES_ACTUAL', 'Q_VTA_ULTIMOS_15DIAS',\n",
    "        'Q_VTA_ULTIMOS_30DIAS', 'Q_TRANSF_PEND', 'Q_TRANSF_EN_PREP','Q_VENTA_30_DIAS',\n",
    "        'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_VENTA_ESPECIAL_30_DIAS','Q_VENTA_ESPECIAL_15_DIAS', 'Q_DIAS_CON_STOCK', 'Q_REPONER',\n",
    "        'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL','Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "    ]\n",
    "\n",
    "    # Filtrar el dataframe con las columnas seleccionadas\n",
    "    df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "    df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "    \n",
    "    # Realizar la fusi√≥n de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "    df_merged = df_merged.merge(\n",
    "        df_nuevo, \n",
    "        left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "        right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "        how='left'\n",
    "    )\n",
    "    df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer Dataframe de FORECAST EXECUTION \n",
    "        \n",
    "fes = get_execution_by_status(40)\n",
    "\n",
    "# Mostrar la tabla con los gr√°ficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL RECORRE FORECAST EXCEC con STATUS 30 y pasa a 40\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 10  #FORECAST OK\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 30].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        # Llamar a la funci√≥n que genera los gr√°ficos y datos extendidos\n",
    "        df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_merged.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        fes.at[index, \"supply_forecast_execution_status_id\"] = 40\n",
    "        # ‚úÖ Actualizar directamente en la base de datos el estado a 40\n",
    "        update_execution(execution_id, supply_forecast_execution_status_id=40)\n",
    "        print(f\"Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ARREGLAR PROBLEMAS\n",
    "# df_sin_duplicados = df_forecast_ext.drop_duplicates(subset=['Codigo_Articulo', 'Sucursal'], keep='first')\n",
    "# file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv\"\n",
    "# df_sin_duplicados.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAR PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nombre del m√≥dulo: S30_GENERA_Grafico_Detalle.py\n",
    "\n",
    "Descripci√≥n:\n",
    "Partiendo de los datos extendidos con estado 30, se generan los gr√°ficos de detalle para cada art√≠culo y sucursal.\n",
    "Se guarda el archivo CSV con los datos extendidos y los gr√°ficos en formato base64.\n",
    "Se actualiza el estado a 40 en la base de datos.\n",
    "\n",
    "Autor: EWE - Zeetrex\n",
    "Fecha de creaci√≥n: [2025-03-22]\n",
    "\"\"\"\n",
    "\n",
    "# Solo importar lo necesario desde el m√≥dulo de funciones\n",
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64,\n",
    "    generar_grafico_json\n",
    ")\n",
    "import os\n",
    "import time\n",
    "import pandas as pd  # uso localmente la lectura de archivos.\n",
    "from datetime import datetime \n",
    "import traceback\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    print(\"üìä Insertando Gr√°ficos Forecast:   \" + name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    path_ventas = f'{folder}/{name}_Ventas.csv'\n",
    "    path_forecast = f'{folder}/{algoritmo}_Pronostico_Extendido.csv'\n",
    "    path_backup = f'{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv'\n",
    "    path_log = f'{folder}/log_graficos_{name}.txt'\n",
    "\n",
    "    # Cargar historial de ventas\n",
    "    df_ventas = pd.read_csv(path_ventas)\n",
    "    df_ventas['Codigo_Articulo'] = df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal'] = df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Cargar forecast extendido\n",
    "    df_forecast = pd.read_csv(path_forecast)\n",
    "    df_forecast.fillna(0, inplace=True)\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "    # Verificar si ya existe archivo con avances\n",
    "    if os.path.exists(path_backup):\n",
    "        df_backup = pd.read_csv(path_backup)\n",
    "        procesados = set(zip(df_backup['Codigo_Articulo'], df_backup['Sucursal']))\n",
    "        print(f\"üîÅ Recuperando avance previo: {len(procesados)} registros ya procesados\")\n",
    "    else:\n",
    "        df_backup = pd.DataFrame(columns=list(df_forecast.columns) + ['GRAFICO'])\n",
    "        procesados = set()\n",
    "\n",
    "    nuevos = 0\n",
    "    total = len(df_forecast)\n",
    "\n",
    "    for i, row in df_forecast.iterrows():\n",
    "        clave = (row['Codigo_Articulo'], row['Sucursal'])\n",
    "        if clave in procesados:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            grafico = generar_grafico_base64(\n",
    "                df_ventas,\n",
    "                row['Codigo_Articulo'],\n",
    "                row['Sucursal'],\n",
    "                row['Forecast'],\n",
    "                row['Average'],\n",
    "                row['ventas_last'],\n",
    "                row['ventas_previous'],\n",
    "                row['ventas_same_year']\n",
    "            )\n",
    "            row_data = row.to_dict()\n",
    "            row_data['GRAFICO'] = grafico\n",
    "            df_backup = pd.concat([df_backup, pd.DataFrame([row_data])], ignore_index=True)\n",
    "            nuevos += 1\n",
    "\n",
    "            if nuevos % 50 == 0 or i == total - 1:\n",
    "                df_backup.to_csv(path_backup, index=False)\n",
    "                elapsed = round(time.time() - start_time, 2)\n",
    "                print(f\"üñºÔ∏è Procesados {nuevos} nuevos registros ({i+1}/{total}) - Tiempo: {elapsed} seg\")\n",
    "                with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                    log.write(f\"[{datetime.now()}] {nuevos} registros procesados ({i+1}/{total}) - Tiempo: {elapsed} seg\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando gr√°fico para Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\")\n",
    "            with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"[{datetime.now()}] ERROR Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "    # Guardar completo al final\n",
    "    df_backup.to_csv(path_backup, index=False)\n",
    "    elapsed = round(time.time() - start_time, 2)\n",
    "    print(f\"‚úÖ Finalizado: {name} - Total nuevos: {nuevos} - Tiempo total: {elapsed} segundos\")\n",
    "    with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"[{datetime.now()}] FINALIZADO: {nuevos} registros nuevos - Tiempo total: {elapsed} seg\\n\")\n",
    "\n",
    "    return df_backup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "    fes = get_execution_execute_by_status(30)\n",
    "\n",
    "    # Filtrar registros con supply_forecast_execution_status_id = 30  #FORECAST con DFATOSK\n",
    "    for index, row in fes[fes[\"fee_status_id\"].isin([30])].iterrows():\n",
    "        algoritmo = row[\"name\"] \n",
    "        name = algoritmo.split('_ALGO')[0]\n",
    "        execution_id = row[\"forecast_execution_id\"]\n",
    "        id_proveedor = row[\"ext_supplier_code\"]\n",
    "        forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "\n",
    "        print(f\"Algoritmo: {algoritmo}  - Name: {name}  exce_id: {execution_id}  Proveedor: {id_proveedor}\")\n",
    "\n",
    "        try:\n",
    "            # Estado intermedio: 35 (procesando gr√°ficos)\n",
    "            print(f\"üõ† Marcando como 'Procesando Gr√°ficos' para {execution_id}\")\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=35)\n",
    "            print(f\"üõ† Iniciando graficaci√≥n para {execution_id}...\")\n",
    "\n",
    "            # Generaci√≥n del dataframe extendido con gr√°ficos\n",
    "            df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "            # Guardar el CSV con datos extendidos y gr√°ficos\n",
    "            file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "            df_merged.to_csv(file_path, index=False)\n",
    "            print(f\"üìÅ Archivo guardado correctamente: {file_path}\")\n",
    "\n",
    "            # ‚úÖ Solo si todo fue exitoso, actualizamos el estado a 40\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=40)\n",
    "            print(f\"‚úÖ Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"‚ùå Error procesando {name}: {e}\")\n",
    "            \n",
    "            log_path = os.path.join(folder, \"errores_s30.log\")\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{name}] ID: {execution_id} - ERROR: {str(e)}\\n\")\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUEVO ENFOQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PLOTLY\n",
    "* PARALELIZADO\n",
    "* PRECALCULO de los DATAFRAMES\n",
    "\n",
    "*Mejora\tImplementada*\n",
    "* Pre-filtrado de datos\t‚úÖ\n",
    "* Cache por combinaci√≥n\t‚úÖ\n",
    "* Multiprocesamiento\t‚úÖ\n",
    "* Migraci√≥n a Plotly\t‚úÖ\n",
    "* Exportaci√≥n Base64\t‚úÖ\n",
    "\n",
    "Ya he migrado el m√≥dulo S30_GENERA_Grafico_Detalle.py al nuevo enfoque que incluye:\n",
    "\n",
    "‚úÖ Prec√°lculo de los dataframes por producto y sucursal.\n",
    "\n",
    "‚úÖ Paralelizaci√≥n con multiprocessing para mejorar la performance.\n",
    "\n",
    "‚úÖ Uso de la nueva funci√≥n generar_grafico_base64_plotly con Plotly para generaci√≥n de gr√°ficos.\n",
    "\n",
    "‚úÖ Soporte para recuperaci√≥n de avance previo y persistencia incremental.\n",
    "\n",
    "‚úÖ Registro en archivo de log y manejo robusto de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: 596_PROCTER_ALGO_01  - Name: 596_PROCTER  exce_id: 9737c740-ba85-462f-b446-d61905e3d048  Proveedor: 596\n",
      "üõ† Marcando como 'Procesando Gr√°ficos' para 9737c740-ba85-462f-b446-d61905e3d048\n",
      "üõ† Iniciando graficaci√≥n para 9737c740-ba85-462f-b446-d61905e3d048...\n",
      "üìä Insertando Gr√°ficos Forecast:   596_PROCTER\n",
      "-> Datos Recuperados del CACHE: 596, Label: 596_PROCTER\n",
      "üîÅ Recuperando avance previo: 0 registros ya procesados\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64_plotly,   # Se asume que esta es la nueva funci√≥n con Plotly\n",
    "    generar_grafico_json\n",
    "    \n",
    ")\n",
    "\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "def procesar_item(args):\n",
    "    articulo, sucursal, df_filtrado, Forecast, Average, ventas_last, ventas_previous, ventas_same_year, index = args\n",
    "    try:\n",
    "        grafico = generar_grafico_base64_plotly(\n",
    "            df_filtrado, articulo, sucursal,\n",
    "            Forecast, Average, ventas_last, ventas_previous, ventas_same_year\n",
    "        )\n",
    "        return index, {\n",
    "            'Codigo_Articulo': articulo,\n",
    "            'Sucursal': sucursal,\n",
    "            'Forecast': Forecast,\n",
    "            'Average': Average,\n",
    "            'ventas_last': ventas_last,\n",
    "            'ventas_previous': ventas_previous,\n",
    "            'ventas_same_year': ventas_same_year,\n",
    "            'GRAFICO': grafico\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {articulo}-{sucursal}: {e}\")\n",
    "        return index, None\n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "    print(\"üìä Insertando Gr√°ficos Forecast:   \" + name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    path_ventas = f'{folder}/{name}_Ventas.csv'\n",
    "    path_forecast = f'{folder}/{algoritmo}_Pronostico_Extendido.csv'\n",
    "    path_backup = f'{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv'\n",
    "    path_log = f'{folder}/log_graficos_{name}.txt'\n",
    "\n",
    "    df_ventas = pd.read_csv(path_ventas)\n",
    "    df_ventas['Codigo_Articulo'] = df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal'] = df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    df_forecast = pd.read_csv(path_forecast)\n",
    "    df_forecast.fillna(0, inplace=True)\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "    if os.path.exists(path_backup):\n",
    "        df_backup = pd.read_csv(path_backup)\n",
    "        procesados = set(zip(df_backup['Codigo_Articulo'], df_backup['Sucursal']))\n",
    "        print(f\"üîÅ Recuperando avance previo: {len(procesados)} registros ya procesados\")\n",
    "    else:\n",
    "        df_backup = pd.DataFrame(columns=list(df_forecast.columns) + ['GRAFICO'])\n",
    "        procesados = set()\n",
    "\n",
    "    fecha_max = df_ventas['Fecha'].max()\n",
    "    lista_args = []\n",
    "    for i, row in df_forecast.iterrows():\n",
    "        clave = (row['Codigo_Articulo'], row['Sucursal'])\n",
    "        if clave in procesados:\n",
    "            continue\n",
    "\n",
    "        df_filt = df_ventas[\n",
    "            (df_ventas['Codigo_Articulo'] == row['Codigo_Articulo']) &\n",
    "            (df_ventas['Sucursal'] == row['Sucursal']) &\n",
    "            (df_ventas['Fecha'] >= fecha_max - pd.Timedelta(days=50))\n",
    "        ].copy()\n",
    "\n",
    "        lista_args.append((\n",
    "            row['Codigo_Articulo'], row['Sucursal'], df_filt,\n",
    "            row['Forecast'], row['Average'],\n",
    "            row['ventas_last'], row['ventas_previous'], row['ventas_same_year'], i\n",
    "        ))\n",
    "\n",
    "    nuevos = 0\n",
    "    total = len(lista_args)\n",
    "    buffer_registros = []\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        for index, result in pool.imap_unordered(procesar_item, lista_args):\n",
    "            if result:\n",
    "                buffer_registros.append(result)\n",
    "                nuevos += 1\n",
    "\n",
    "            if nuevos % 50 == 0 or nuevos == total:\n",
    "                if buffer_registros:\n",
    "                    df_buffer = pd.DataFrame(buffer_registros)\n",
    "                    df_backup = pd.concat([df_backup, df_buffer], ignore_index=True)\n",
    "                    df_backup.to_csv(path_backup, index=False)\n",
    "                    buffer_registros = []\n",
    "\n",
    "                    elapsed = round(time.time() - start_time, 2)\n",
    "                    print(f\"üñºÔ∏è Procesados {nuevos} nuevos registros ({nuevos}/{total}) - Tiempo: {elapsed} seg\")\n",
    "                    with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                        log.write(f\"[{datetime.now()}] {nuevos} registros procesados ({nuevos}/{total}) - Tiempo: {elapsed} seg\\n\")\n",
    "\n",
    "    df_backup.to_csv(path_backup, index=False)\n",
    "    elapsed = round(time.time() - start_time, 2)\n",
    "    print(f\"‚úÖ Finalizado: {name} - Total nuevos: {nuevos} - Tiempo total: {elapsed} segundos\")\n",
    "    with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"[{datetime.now()}] FINALIZADO: {nuevos} registros nuevos - Tiempo total: {elapsed} seg\\n\")\n",
    "\n",
    "    return df_backup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fes = get_execution_execute_by_status(30)\n",
    "\n",
    "    for index, row in fes[fes[\"fee_status_id\"].isin([30])].iterrows():\n",
    "        algoritmo = row[\"name\"] \n",
    "        name = algoritmo.split('_ALGO')[0]\n",
    "        execution_id = row[\"forecast_execution_id\"]\n",
    "        id_proveedor = row[\"ext_supplier_code\"]\n",
    "        forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "\n",
    "        print(f\"Algoritmo: {algoritmo}  - Name: {name}  exce_id: {execution_id}  Proveedor: {id_proveedor}\")\n",
    "\n",
    "        try:\n",
    "            print(f\"üõ† Marcando como 'Procesando Gr√°ficos' para {execution_id}\")\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=35)\n",
    "            print(f\"üõ† Iniciando graficaci√≥n para {execution_id}...\")\n",
    "\n",
    "            df_merged = insertar_graficos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "            file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido_FINAL.csv\"\n",
    "            df_merged.to_csv(file_path, index=False)\n",
    "            print(f\"üìÅ Archivo guardado correctamente: {file_path}\")\n",
    "\n",
    "            update_execution_execute(forecast_execution_execute_id, supply_forecast_execution_status_id=40)\n",
    "            print(f\"‚úÖ Estado actualizado a 40 para {execution_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"‚ùå Error procesando {name}: {e}\")\n",
    "\n",
    "            log_path = os.path.join(folder, \"errores_s30.log\")\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{name}] ID: {execution_id} - ERROR: {str(e)}\\n\")\n",
    "\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forecast.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution_execute,\n",
    "    generar_grafico_base64\n",
    ")\n",
    "import os\n",
    "import time\n",
    "import pandas as pd  # uso localmente la lectura de archivos.\n",
    "from datetime import datetime \n",
    "import traceback\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "\n",
    "fes = get_execution_execute_by_status(40)\n",
    "\n",
    "# Mostrar la tabla con los gr√°ficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requiere: pandas, plotly, kaleido (para exportar im√°genes)\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Suponiendo que dfv es el DataFrame original con columnas: \n",
    "# \"Codigo_Articulo\", \"Sucursal\", \"Fecha\", \"Unidades\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. FUNCI√ìN PARA GENERAR GR√ÅFICO\n",
    "# -------------------------------\n",
    "\n",
    "def generar_grafico_base64_plotly(df_filtrado, articulo, sucursal, Forecast, Average, ventas_last, ventas_previous, ventas_same_year):\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Ventas Diarias\",\n",
    "            \"Histograma de Ventas Semanales\",\n",
    "            \"Forecast vs Ventas Anteriores\",\n",
    "            \"Comparaci√≥n de Ventas en 3 Per√≠odos\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=f\"Demanda Articulo {articulo} - Sucursal {sucursal}\", height=800, width=1000)\n",
    "\n",
    "    # Media m√≥vil\n",
    "    df_filtrado[\"Media_Movil\"] = df_filtrado[\"Unidades\"].rolling(window=7).mean()\n",
    "\n",
    "    # Gr√°fico 1\n",
    "    fig.add_trace(go.Scatter(x=df_filtrado[\"Fecha\"], y=df_filtrado[\"Unidades\"], mode='lines+markers', name='Ventas'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_filtrado[\"Fecha\"], y=df_filtrado[\"Media_Movil\"], mode='lines', name='Media M√≥vil', line=dict(dash='dash')), row=1, col=1)\n",
    "\n",
    "    # Gr√°fico 2\n",
    "    df_filtrado[\"Semana\"] = df_filtrado[\"Fecha\"].dt.to_period(\"W\").astype(str)\n",
    "    df_semanal = df_filtrado.groupby(\"Semana\").agg({\"Unidades\": \"sum\", \"Fecha\": \"min\"}).reset_index()\n",
    "    df_semanal[\"Semana_Num\"] = df_semanal[\"Fecha\"].dt.isocalendar().week\n",
    "    fig.add_trace(go.Bar(x=df_semanal[\"Semana_Num\"], y=df_semanal[\"Unidades\"], name=\"Semanas\"), row=1, col=2)\n",
    "\n",
    "    # Gr√°fico 3\n",
    "    fig.add_trace(go.Bar(x=[\"Forecast\", \"Actual\", \"Anterior\", \"A√±o Ant\"],\n",
    "                        y=[Forecast, ventas_last, ventas_previous, ventas_same_year],\n",
    "                        name=\"Comparaci√≥n\"), row=2, col=1)\n",
    "\n",
    "    # Gr√°fico 4: c√°lculo per√≠odos\n",
    "    fecha_maxima = df_filtrado[\"Fecha\"].max()\n",
    "    fecha_inicio_ultimos30 = fecha_maxima - pd.Timedelta(days=30)\n",
    "    fecha_inicio_previos30 = fecha_inicio_ultimos30 - pd.Timedelta(days=30)\n",
    "    fecha_inicio_anio_anterior = fecha_inicio_ultimos30 - pd.DateOffset(years=1)\n",
    "    fecha_fin_anio_anterior = fecha_inicio_previos30 - pd.DateOffset(years=1)\n",
    "\n",
    "    ventas_ultimos_30 = df_filtrado[df_filtrado[\"Fecha\"] > fecha_inicio_ultimos30][\"Unidades\"].sum()\n",
    "    ventas_previos_30 = df_filtrado[\n",
    "        (df_filtrado[\"Fecha\"] > fecha_inicio_previos30) & (df_filtrado[\"Fecha\"] <= fecha_inicio_ultimos30)\n",
    "    ][\"Unidades\"].sum()\n",
    "    df_anterior = df_filtrado.copy()\n",
    "    df_anterior[\"Fecha\"] = df_anterior[\"Fecha\"] - pd.DateOffset(years=1)\n",
    "    ventas_anio_anterior = df_anterior[\n",
    "        (df_anterior[\"Fecha\"] > fecha_inicio_anio_anterior) & (df_anterior[\"Fecha\"] <= fecha_fin_anio_anterior)\n",
    "    ][\"Unidades\"].sum()\n",
    "\n",
    "    fig.add_trace(go.Bar(x=[\"√öltimos 30\", \"Anteriores 30\", \"A√±o anterior\", \"Average\"],\n",
    "                        y=[ventas_ultimos_30, ventas_previos_30, ventas_anio_anterior, Average],\n",
    "                        name=\"Comparaci√≥n temporal\"), row=2, col=2)\n",
    "\n",
    "    # Exportar a base64\n",
    "    buffer = BytesIO()\n",
    "    fig.write_image(buffer, format=\"png\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. FUNCI√ìN DE PROCESO INDIVIDUAL\n",
    "# -------------------------------\n",
    "\n",
    "def procesar_item(args):\n",
    "    articulo, sucursal, df_filtrado, Forecast, Average, ventas_last, ventas_previous, ventas_same_year = args\n",
    "    try:\n",
    "        return (articulo, sucursal, generar_grafico_base64_plotly(\n",
    "            df_filtrado, articulo, sucursal,\n",
    "            Forecast, Average, ventas_last, ventas_previous, ventas_same_year\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {articulo}-{sucursal}: {e}\")\n",
    "        return (articulo, sucursal, None)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. PRECALCULAR DATAFRAMES FILTRADOS\n",
    "# -------------------------------\n",
    "\n",
    "def preparar_y_procesar(dfv, resumen_ventas):\n",
    "    fecha_maxima = dfv[\"Fecha\"].max()\n",
    "\n",
    "    # Prec√°lculo por producto-sucursal\n",
    "    pares = dfv[[\"Codigo_Articulo\", \"Sucursal\"]].drop_duplicates()\n",
    "    dict_df = {}\n",
    "    for row in pares.itertuples(index=False):\n",
    "        df_filt = dfv[\n",
    "            (dfv[\"Codigo_Articulo\"] == row.Codigo_Articulo) &\n",
    "            (dfv[\"Sucursal\"] == row.Sucursal) &\n",
    "            (dfv[\"Fecha\"] >= fecha_maxima - pd.Timedelta(days=50))\n",
    "        ].copy()\n",
    "        dict_df[(row.Codigo_Articulo, row.Sucursal)] = df_filt\n",
    "\n",
    "    # Empaquetar datos de entrada\n",
    "    lista_entradas = []\n",
    "    for (articulo, sucursal), df_filtrado in dict_df.items():\n",
    "        # Obtener m√©tricas simuladas para el ejemplo\n",
    "        Forecast = resumen_ventas.get((articulo, sucursal), {}).get(\"Forecast\", 0)\n",
    "        Average = resumen_ventas.get((articulo, sucursal), {}).get(\"Average\", 0)\n",
    "        ventas_last = resumen_ventas.get((articulo, sucursal), {}).get(\"Last\", 0)\n",
    "        ventas_previous = resumen_ventas.get((articulo, sucursal), {}).get(\"Previous\", 0)\n",
    "        ventas_same_year = resumen_ventas.get((articulo, sucursal), {}).get(\"SameYear\", 0)\n",
    "\n",
    "        lista_entradas.append((\n",
    "            articulo, sucursal, df_filtrado,\n",
    "            Forecast, Average, ventas_last, ventas_previous, ventas_same_year\n",
    "        ))\n",
    "\n",
    "    # Multiprocesamiento\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        resultados = pool.map(procesar_item, lista_entradas)\n",
    "\n",
    "    # Retorno en dict\n",
    "    return {(art, suc): b64img for art, suc, b64img in resultados if b64img is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in fes[fes[\"fee_status_id\"] == 40].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"forecast_execution_id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "\n",
    "    print(\"üìä Insertando Gr√°ficos Forecast:   \" + name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    path_ventas = f'{folder}/{name}_Ventas.csv'\n",
    "    path_forecast = f'{folder}/{algoritmo}_Pronostico_Extendido.csv'\n",
    "    path_backup = f'{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv'\n",
    "    path_log = f'{folder}/log_graficos_{name}.txt'\n",
    "\n",
    "    # Cargar historial de ventas\n",
    "    df_ventas = pd.read_csv(path_ventas)\n",
    "    df_ventas['Codigo_Articulo'] = df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal'] = df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Cargar forecast extendido\n",
    "    df_forecast = pd.read_csv(path_forecast)\n",
    "    df_forecast.fillna(0, inplace=True)\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevos = 0\n",
    "total = len(df_forecast)\n",
    "    \n",
    "for i, row in df_forecast.iterrows():\n",
    "        # clave = (row['Codigo_Articulo'], row['Sucursal'])\n",
    "        # if clave in procesados:\n",
    "        #     continue\n",
    "\n",
    "        try:\n",
    "            grafico = generar_grafico_base64(\n",
    "                df_ventas,\n",
    "                row['Codigo_Articulo'],\n",
    "                row['Sucursal'],\n",
    "                row['Forecast'],\n",
    "                row['Average'],\n",
    "                row['ventas_last'],\n",
    "                row['ventas_previous'],\n",
    "                row['ventas_same_year']\n",
    "            )\n",
    "            row_data = row.to_dict()\n",
    "            row_data['GRAFICO'] = grafico\n",
    "            df_backup = pd.concat([df_backup, pd.DataFrame([row_data])], ignore_index=True)\n",
    "            nuevos += 1\n",
    "\n",
    "            if nuevos % 50 == 0 or i == total - 1:\n",
    "                df_backup.to_csv(path_backup, index=False)\n",
    "                elapsed = round(time.time() - start_time, 2)\n",
    "                print(f\"üñºÔ∏è Procesados {nuevos} nuevos registros ({i+1}/{total}) - Tiempo: {elapsed} seg\")\n",
    "                with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                    log.write(f\"[{datetime.now()}] {nuevos} registros procesados ({i+1}/{total}) - Tiempo: {elapsed} seg\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando gr√°fico para Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\")\n",
    "            with open(path_log, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"[{datetime.now()}] ERROR Art {row['Codigo_Articulo']} - Suc {row['Sucursal']}: {e}\\n\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
