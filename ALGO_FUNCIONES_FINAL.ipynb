{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITMOS BASADOS EN SERIES TEMPORALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Recopilación de Funciones\n",
    "\n",
    "Se compilan los Algoritmos Probados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRERIAS NECESARIAS \n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "import psycopg2 as pg2    # Conectores para Postgres\n",
    "import getpass  # Para obtener el usuario del sistema operativo\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Evitar Mensajes Molestos\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)\n",
    "\n",
    "secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES\n",
    "\n",
    "###----------------------------------------------------------------\n",
    "#     DATOS\n",
    "###----------------------------------------------------------------\n",
    "def Open_Connection():\n",
    "    secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "    conn_str = f'DRIVER={secrets[\"DRIVER2\"]};SERVER={secrets[\"SERVIDOR2\"]};PORT={secrets[\"PUERTO2\"]};DATABASE={secrets[\"BASE2\"]};UID={secrets[\"USUARIO2\"]};PWD={secrets[\"CONTRASENA2\"]}'\n",
    "    # print (conn_str) \n",
    "    try:    \n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        return conn\n",
    "    except:\n",
    "        print('Error en la Conexión')\n",
    "        return None\n",
    "\n",
    "def Open_Conn_Postgres():\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE3']} user={secrets['USUARIO3']} password={secrets['CONTRASENA3']} host={secrets['SERVIDOR3']} port={secrets['PUERTO3']}\"\n",
    "    #print (conn_str)\n",
    "    try:    \n",
    "        conn = pg2.connect(conn_str)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        return None\n",
    "    \n",
    "def Close_Connection(conn): \n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "def generar_datos(id_proveedor, etiqueta):\n",
    "    secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "    folder = secrets[\"FOLDER_DATOS\"]\n",
    "    \n",
    "    #  Intento recuperar datos cacheados\n",
    "    try:\n",
    "        data = pd.read_csv(f'{folder}/{etiqueta}.csv')\n",
    "        data['Codigo_Articulo']= data['Codigo_Articulo'].astype(int)\n",
    "        data['Sucursal']= data['Sucursal'].astype(int)\n",
    "        data['Fecha']= pd.to_datetime(data['Fecha'])\n",
    "\n",
    "        articulos = pd.read_csv(f'{folder}/{etiqueta}_articulos.csv')\n",
    "        #articulos.head()\n",
    "        print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {etiqueta}\")\n",
    "        return data, articulos\n",
    "    except:     \n",
    "        print(f\"-> Generando datos para ID: {id_proveedor}, Label: {etiqueta}\")\n",
    "        # Configuración de conexión\n",
    "        conn = Open_Connection()\n",
    "        \n",
    "        # FILTRA solo PRODUCTOS HABILITADOS y Traer datos de STOCK y PENDIENTES desde PRODUCCIÓN\n",
    "        query = f\"\"\"\n",
    "        SELECT  A.[C_PROVEEDOR_PRIMARIO]\n",
    "            ,S.[C_ARTICULO]\n",
    "            ,S.[C_SUCU_EMPR]\n",
    "            ,S.[Q_FACTOR_VENTA_ESP]\n",
    "            ,S.[Q_FACTOR_VTA_SUCU]\n",
    "            ,S.[M_OFERTA_SUCU]\n",
    "            ,S.[M_HABILITADO_SUCU]\n",
    "            ,A.M_BAJA\n",
    "            ,S.[Q_VTA_DIA_ANT]\n",
    "            ,S.[Q_VTA_ACUM]\n",
    "            ,S.[Q_ULT_ING_STOCK]\n",
    "            ,S.[Q_STOCK_A_ULT_ING]\n",
    "            ,S.[Q_15DIASVTA_A_ULT_ING_STOCK]\n",
    "            ,S.[Q_30DIASVTA_A_ULT_ING_STOCK]\n",
    "            ,S.[Q_BULTOS_PENDIENTE_OC]\n",
    "            ,S.[Q_PESO_PENDIENTE_OC]\n",
    "            ,S.[Q_UNID_PESO_PEND_RECEP_TRANSF]\n",
    "            ,S.[Q_UNID_PESO_VTA_MES_ACTUAL]\n",
    "            ,S.[F_ULTIMA_VTA]\n",
    "            ,S.[Q_VTA_ULTIMOS_15DIAS]\n",
    "            ,S.[Q_VTA_ULTIMOS_30DIAS]\n",
    "            ,S.[Q_TRANSF_PEND]\n",
    "            ,S.[Q_TRANSF_EN_PREP]\n",
    "            ,S.[M_FOLDER]\n",
    "            ,S.[M_ALTA_RENTABILIDAD]\n",
    "            ,S.[Lugar_Abastecimiento]\n",
    "            ,S.[M_COSTO_LOGISTICO]\n",
    "        \n",
    "        FROM [DIARCOP001].[DiarcoP].[dbo].[T051_ARTICULOS_SUCURSAL] S\n",
    "        LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T050_ARTICULOS] A\n",
    "            ON A.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "\n",
    "        WHERE S.[M_HABILITADO_SUCU] = 'S' -- Permitido Reponer\n",
    "            AND A.M_BAJA = 'N'  -- Activo en Maestro Artículos\n",
    "            AND A.[C_PROVEEDOR_PRIMARIO] = {id_proveedor} -- Solo del Proveedor\n",
    "        ;\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        articulos = pd.read_sql(query, conn)\n",
    "        \n",
    "        # Consulta SQL para obtener las ventas de un proveedor específico   \n",
    "        # Reemplazar {proveedor} en la consulta con el ID de la tienda actual\n",
    "        query = f\"\"\"\n",
    "        SELECT V.[F_VENTA] as Fecha\n",
    "            ,V.[C_ARTICULO] as Codigo_Articulo\n",
    "            ,V.[C_SUCU_EMPR] as Sucursal\n",
    "            ,V.[I_PRECIO_VENTA] as Precio\n",
    "            ,V.[I_PRECIO_COSTO] as Costo\n",
    "            ,V.[Q_UNIDADES_VENDIDAS] as Unidades\n",
    "            ,V.[C_FAMILIA] as Familia\n",
    "            ,A.[C_RUBRO] as Rubro\n",
    "            ,A.[C_SUBRUBRO_1] as SubRubro\n",
    "            ,LTRIM(RTRIM(REPLACE(REPLACE(REPLACE(A.N_ARTICULO, CHAR(9), ''), CHAR(13), ''), CHAR(10), ''))) as Nombre_Articulo\n",
    "            ,A.[C_CLASIFICACION_COMPRA] as Clasificacion\n",
    "        FROM [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "        LEFT JOIN [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "            ON V.C_ARTICULO = A.C_ARTICULO\n",
    "        WHERE A.[C_PROVEEDOR_PRIMARIO] = {id_proveedor} AND V.F_VENTA >= '20210101' AND A.M_BAJA ='N'\n",
    "        ORDER BY V.F_VENTA ;\n",
    "        \"\"\"\n",
    "\n",
    "        # Ejecutar la consulta SQL\n",
    "        demanda = pd.read_sql(query, conn)\n",
    "        \n",
    "        # UNIR Y FILTRAR solo la demanda de los Hartículos VALIDOS.\n",
    "        # Realizar la unión (merge) de los DataFrames por las claves especificadas\n",
    "        data = pd.merge(\n",
    "            articulos,  # DataFrame de artículos\n",
    "            demanda,    # DataFrame de demanda\n",
    "            left_on=['C_ARTICULO', 'C_SUCU_EMPR'],  # Claves en 'articulos'\n",
    "            right_on=['Codigo_Articulo', 'Sucursal'],  # Claves en 'demanda'\n",
    "            how='inner'  # Solo traer los productos que están en 'articulos'\n",
    "        )\n",
    "            \n",
    "        # Guardar los resultados en un archivo CSV con el nombre del Proveedor\n",
    "        file_path = f'{folder}/{etiqueta}_FULL.csv'\n",
    "        data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        print(f\"---> Datos de FULL guardados: {file_path}\")\n",
    "        \n",
    "        file_path = f'{folder}/{etiqueta}_articulos.csv'\n",
    "        articulos.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"---> Datos de Artículos guardados: {file_path}\")\n",
    "        \n",
    "        # Eliminar Columnas Innecesarias\n",
    "        data = data[['Fecha', 'Codigo_Articulo', 'Sucursal', 'Unidades']]\n",
    "        data['Codigo_Articulo']= data['Codigo_Articulo'].astype(int)\n",
    "        data['Sucursal']= data['Sucursal'].astype(int)\n",
    "        \n",
    "        # Guardar los resultados en un archivo CSV con el nombre del Proveedor\n",
    "        file_path = f'{folder}/{etiqueta}.csv'\n",
    "        data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Cerrar la conexión después de la iteración\n",
    "        Close_Connection(conn)\n",
    "        return data, articulos\n",
    "###----------------------------------------------------------------\n",
    "#     ALGORITMOS\n",
    "###----------------------------------------------------------------\n",
    "# ALGO_01 Promedio de Ventas Ponderado  \n",
    "def Calcular_Demanda_ALGO_01(df, id_proveedor, etiqueta, period_length, current_date, factor_last, factor_previous, factor_year):\n",
    "    print('Dentro del Calcular_Demanda_ALGO_01')\n",
    "    print(f'FORECATS control: {id_proveedor} - {etiqueta} - ventana: {period_length} - factores: {factor_last} - {factor_previous} - {factor_year}')\n",
    "    # Definir rangos de fechas para cada período\n",
    "    last_period_start = current_date - pd.Timedelta(days=period_length - 1)\n",
    "    last_period_end = current_date\n",
    "\n",
    "    previous_period_start = current_date - pd.Timedelta(days=2 * period_length - 1)\n",
    "    previous_period_end = current_date - pd.Timedelta(days=period_length)\n",
    "\n",
    "    same_period_last_year_start = current_date - pd.DateOffset(years=1) - pd.Timedelta(days=period_length - 1)\n",
    "    same_period_last_year_end = current_date - pd.DateOffset(years=1)\n",
    "\n",
    "    # Filtrar los datos para cada uno de los períodos\n",
    "    df_last = df[(df['Fecha'] >= last_period_start) & (df['Fecha'] <= last_period_end)]\n",
    "    df_previous = df[(df['Fecha'] >= previous_period_start) & (df['Fecha'] <= previous_period_end)]\n",
    "    df_same_year = df[(df['Fecha'] >= same_period_last_year_start) & (df['Fecha'] <= same_period_last_year_end)]\n",
    "\n",
    "    # Agregar las ventas (unidades) por artículo y sucursal para cada período\n",
    "    sales_last = df_last.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                        .sum().reset_index().rename(columns={'Unidades': 'ventas_last'})\n",
    "    sales_previous = df_previous.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                                .sum().reset_index().rename(columns={'Unidades': 'ventas_previous'})\n",
    "    sales_same_year = df_same_year.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                                .sum().reset_index().rename(columns={'Unidades': 'ventas_same_year'})\n",
    "\n",
    "    # Unir la información de los tres períodos\n",
    "    forecast_df = pd.merge(sales_last, sales_previous, on=['Codigo_Articulo', 'Sucursal'], how='outer')\n",
    "    forecast_df = pd.merge(forecast_df, sales_same_year, on=['Codigo_Articulo', 'Sucursal'], how='outer')\n",
    "    forecast_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Calcular la demanda estimada como el promedio de las ventas de los tres períodos\n",
    "    forecast_df['Forecast'] = (forecast_df['ventas_last'] * factor_last +\n",
    "                               forecast_df['ventas_previous'] * factor_previous +\n",
    "                               forecast_df['ventas_same_year'] * factor_year) / (factor_year + factor_last + factor_previous)\n",
    "\n",
    "    # Redondear la predicción al entero más cercano\n",
    "    forecast_df['Forecast'] = forecast_df['Forecast'].round().astype(int)\n",
    "    forecast_df['Average'] = round(forecast_df['Forecast'] /period_length ,3)\n",
    "    # Borrar Columnas Innecesarias\n",
    "    forecast_df.drop(columns=['ventas_last', 'ventas_previous', 'ventas_same_year'], inplace=True)    \n",
    "\n",
    "    return forecast_df\n",
    "\n",
    "def Calcular_Demanda_Extendida_ALGO_01(df, id_proveedor, etiqueta, period_length, current_date, factor_last, factor_previous, factor_year):\n",
    "    # Definir rangos de fechas para cada período\n",
    "    last_period_start = current_date - pd.Timedelta(days=period_length - 1)\n",
    "    last_period_end = current_date\n",
    "\n",
    "    previous_period_start = current_date - pd.Timedelta(days=2 * period_length - 1)\n",
    "    previous_period_end = current_date - pd.Timedelta(days=period_length)\n",
    "\n",
    "    same_period_last_year_start = current_date - pd.DateOffset(years=1) - pd.Timedelta(days=period_length - 1)\n",
    "    same_period_last_year_end = current_date - pd.DateOffset(years=1)\n",
    "\n",
    "    # Filtrar los datos para cada uno de los períodos\n",
    "    df_last = df[(df['Fecha'] >= last_period_start) & (df['Fecha'] <= last_period_end)]\n",
    "    df_previous = df[(df['Fecha'] >= previous_period_start) & (df['Fecha'] <= previous_period_end)]\n",
    "    df_same_year = df[(df['Fecha'] >= same_period_last_year_start) & (df['Fecha'] <= same_period_last_year_end)]\n",
    "\n",
    "    # Agregar las ventas (unidades) por artículo y sucursal para cada período mas todos los datos anteriores.\n",
    "    sales_last = df_last.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                        .sum().reset_index().rename(columns={'Unidades': 'ventas_last'})\n",
    "    sales_previous = df_previous.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                                .sum().reset_index().rename(columns={'Unidades': 'ventas_previous'})\n",
    "    sales_same_year = df_same_year.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'] \\\n",
    "                                .sum().reset_index().rename(columns={'Unidades': 'ventas_same_year'})\n",
    "\n",
    "    # Unir la información de los tres períodos\n",
    "    validacion_df = pd.merge(sales_last, sales_previous, on=['Codigo_Articulo', 'Sucursal'], how='outer')\n",
    "    validacion_df = pd.merge(validacion_df, sales_same_year, on=['Codigo_Articulo', 'Sucursal'], how='outer')\n",
    "    validacion_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Calcular la demanda estimada como el promedio de las ventas de los tres períodos\n",
    "    validacion_df['Forecast'] = (validacion_df['ventas_last'] * factor_last +\n",
    "                               validacion_df['ventas_previous'] * factor_previous +\n",
    "                               validacion_df['ventas_same_year'] * factor_year) / (factor_year + factor_last + factor_previous)\n",
    "    validacion_df['Average'] = round(validacion_df['Forecast'] /period_length ,3)\n",
    "    \n",
    "    # Redondear la predicción al entero más cercano\n",
    "    validacion_df['Forecast'] = validacion_df['Forecast'].round(2)\n",
    "\n",
    "    return validacion_df\n",
    "\n",
    "def Calcular_Demanda_ALGO_05(df_prv,forecast_window,id_proveedor, etiqueta):\n",
    "    # Lista para almacenar los resultados del pronóstico\n",
    "    resultados = []\n",
    "\n",
    "    # Agrupar los datos por 'Codigo_Articulo' y 'Sucursal'\n",
    "    for (codigo, sucursal), grupo in df_prv.groupby(['Codigo_Articulo', 'Sucursal']):\n",
    "        # Establecer 'Fecha' como índice y ordenar los datos\n",
    "        grupo = grupo.set_index('Fecha').sort_index()\n",
    "        \n",
    "        # Resamplear a diario sumando las ventas\n",
    "        ventas_diarias = grupo['Unidades'].resample('D').sum().fillna(0)\n",
    "        \n",
    "        # Seleccionar un periodo reciente para calcular la media; por ejemplo, los últimos 30 días\n",
    "        # Si hay menos de 30 días de datos, se utiliza el periodo disponible\n",
    "        ventas_recientes = ventas_diarias[-30:]\n",
    "        media_diaria = ventas_recientes.mean()\n",
    "        \n",
    "        # Pronosticar la demanda para el periodo de reposición\n",
    "        pronostico = media_diaria * forecast_window\n",
    "        \n",
    "        resultados.append({\n",
    "            'Codigo_Articulo': codigo,\n",
    "            'Sucursal': sucursal,\n",
    "            'Forecast': round(pronostico, 2),\n",
    "            'Average': round(media_diaria, 3)\n",
    "        })\n",
    "\n",
    "    # Crear el DataFrame de pronósticos\n",
    "    df_pronostico = pd.DataFrame(resultados)\n",
    "\n",
    "    return df_pronostico\n",
    "\n",
    "\n",
    "def Calcular_Demanda_Extendida_ALGO_05(df_prv,forecast_window,id_proveedor, etiqueta):\n",
    "    max_date = df_prv['Fecha'].max()\n",
    "    cutoff_date = max_date - timedelta(days=forecast_window)\n",
    "    resultados_validacion = []\n",
    "\n",
    "    # Agrupar los datos por 'Codigo_Articulo' y 'Sucursal'\n",
    "    for (codigo, sucursal), grupo in df_prv.groupby(['Codigo_Articulo', 'Sucursal']):\n",
    "        # Establecer 'Fecha' como índice y ordenar cronológicamente\n",
    "        grupo = grupo.set_index('Fecha').sort_index()\n",
    "        \n",
    "        # Datos de entrenamiento: hasta la fecha de corte\n",
    "        datos_entrenamiento = grupo.loc[:cutoff_date]\n",
    "        \n",
    "        # Verificar que existan suficientes datos para calcular el promedio (por ejemplo, al menos 30 días)\n",
    "        if len(datos_entrenamiento) < 30:\n",
    "            continue  # O vemos como manejarlo de otra forma\n",
    "        \n",
    "        # Resamplear a ventas diarias en el conjunto de entrenamiento\n",
    "        ventas_diarias_entrenamiento = datos_entrenamiento['Unidades'].resample('D').sum().fillna(0)\n",
    "        \n",
    "        # Calcular la media diaria de los últimos 30 días del periodo de entrenamiento\n",
    "        ventas_recientes = ventas_diarias_entrenamiento[-30:]\n",
    "        media_diaria = ventas_recientes.mean()\n",
    "        \n",
    "        # Calcular el pronóstico para la ventana definida\n",
    "        forecast = media_diaria * forecast_window\n",
    "        \n",
    "        # Definir el periodo de validación: desde el día siguiente a la fecha de corte hasta completar la ventana\n",
    "        inicio_validacion = cutoff_date + timedelta(days=1)\n",
    "        fin_validacion = cutoff_date + timedelta(days=forecast_window)\n",
    "        \n",
    "        # Extraer y resumir las ventas reales en el periodo de validación\n",
    "        ventas_validacion = grupo.loc[inicio_validacion:fin_validacion]['Unidades'].resample('D').sum().fillna(0)\n",
    "        ventas_reales = ventas_validacion.sum()\n",
    "        \n",
    "        # Calcular medidas de error\n",
    "        error_absoluto = abs(forecast - ventas_reales)\n",
    "        error_porcentual = (error_absoluto / ventas_reales * 100) if ventas_reales != 0 else None\n",
    "        \n",
    "        resultados_validacion.append({\n",
    "            'Codigo_Articulo': codigo,\n",
    "            'Sucursal': sucursal,\n",
    "            'Forecast': round(forecast, 2),\n",
    "            'Ventas_Reales': round(ventas_reales, 2),\n",
    "            'Error_Absoluto': round(error_absoluto, 2),\n",
    "            'Error_Porcentual': round(error_porcentual, 2) if error_porcentual is not None else None\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame con los resultados de validación\n",
    "    df_validacion = pd.DataFrame(resultados_validacion)\n",
    "\n",
    "    return df_validacion\n",
    "\n",
    "\n",
    "def Exportar_Pronostico(df_forecast, proveedor, etiqueta, algoritmo):\n",
    "    df_forecast['Codigo_Articulo']= df_forecast['Codigo_Articulo'].astype(int)\n",
    "    df_forecast['Sucursal']= df_forecast['Sucursal'].astype(int)\n",
    "    \n",
    "    # tools.display_dataframe_to_user(name=\"SET de Datos del Proveedor\", dataframe=df_forecast)\n",
    "    # df_forecast.info()\n",
    "    print(f'-> ** Pronostico Guardado en: {folder}/{etiqueta}_Pronostico_{algoritmo}.csv **')\n",
    "    df_forecast.to_csv(f'{folder}/{etiqueta}_Pronostico_{algoritmo}.csv', index=False)\n",
    "    \n",
    "    ## GUARDAR TABLA EN POSTGRES\n",
    "    usuario = getpass.getuser()  # Obtiene el usuario del sistema operativo\n",
    "    fecha_actual = datetime.today().strftime('%Y-%m-%d')  # Obtiene la fecha de hoy en formato 'YYYY-MM-DD'\n",
    "\n",
    "    conn = Open_Conn_Postgres()\n",
    "    \n",
    "    # Query de inserción\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO public.f_oc_precarga_connexa (\n",
    "        c_proveedor, c_articulo, c_sucu_empr, q_forecast_unidades, f_alta_forecast, c_usuario_forecast, create_date\n",
    "    ) VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir el DataFrame a una lista de tuplas para la inserción en bloque\n",
    "    data_to_insert = [\n",
    "        (proveedor, row['Codigo_Articulo'], row['Sucursal'], row['Forecast'], fecha_actual, usuario, fecha_actual)\n",
    "        for _, row in df_forecast.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.executemany(insert_query, data_to_insert)\n",
    "        conn.commit()\n",
    "        print(f\"✅ Inserción completada: {len(data_to_insert)} registros insertados.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"❌ Error en la inserción: {e}\")\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "        print(\"✅ Conexión cerrada.\")\n",
    "    \n",
    "    \n",
    "def Procesar_ALGO_05(data, proveedor, etiqueta, ventana, fecha):\n",
    "    df_forecast = Calcular_Demanda_ALGO_05(data, ventana, proveedor, etiqueta)    # Exportar el resultado a un CSV para su posterior procesamiento\n",
    "    df_forecast['Codigo_Articulo']= df_forecast['Codigo_Articulo'].astype(int)\n",
    "    df_forecast['Sucursal']= df_forecast['Sucursal'].astype(int)\n",
    "    df_forecast.to_csv(f'{folder}/{etiqueta}_ALGO_05_Solicitudes_Compra.csv', index=False)\n",
    "\n",
    "    df_validacion = Calcular_Demanda_Extendida_ALGO_05(data, ventana, proveedor, etiqueta)\n",
    "    df_validacion['Codigo_Articulo']= df_validacion['Codigo_Articulo'].astype(int)\n",
    "    df_validacion['Sucursal']= df_validacion['Sucursal'].astype(int)\n",
    "    df_validacion.to_csv(f'{folder}/{etiqueta}_ALGO_05_Datos_Validacion.csv', index=False)\n",
    "    print(f'-> ** Validación Exportada: {etiqueta}_ALGO_05_Datos_Validacion.csv *** : ventana: {ventana}  - {fecha}')\n",
    "    \n",
    "    Exportar_Pronostico(df_forecast, proveedor, etiqueta, 'ALGO_05')  # Impactar Datos en la Interface   \n",
    "    return\n",
    "\n",
    "def Procesar_ALGO_01(data, proveedor, etiqueta, ventana, fecha, factor_last=None, factor_previous=None, factor_year=None):    \n",
    "    # Asignar valores por defecto si los factores no están definidos\n",
    "    factor_last = 77 if factor_last is None else int(factor_last)\n",
    "    factor_previous = 22 if factor_previous is None else int(factor_previous)\n",
    "    factor_year = 11 if factor_year is None else int(factor_year)\n",
    "\n",
    "    print(f'--> ALGO_01 ventana {ventana} - Peso de los Factores Utilizados: último: {factor_last} previo: {factor_previous} año anterior: {factor_year}')\n",
    "        \n",
    "    df_forecast = Calcular_Demanda_ALGO_01(data, proveedor, etiqueta, ventana, fecha, factor_last, factor_previous, factor_year)\n",
    "    df_forecast['Codigo_Articulo']= df_forecast['Codigo_Articulo'].astype(int)\n",
    "    df_forecast['Sucursal']= df_forecast['Sucursal'].astype(int)\n",
    "    df_forecast.to_csv(f'{folder}/{etiqueta}_ALGO_01_Solicitudes_Compra.csv', index=False)   # Exportar el resultado a un CSV para su posterior procesamiento\n",
    "    \n",
    "    df_validacion = Calcular_Demanda_Extendida_ALGO_01(data, proveedor, etiqueta, ventana, fecha, factor_last, factor_previous, factor_year)\n",
    "    df_validacion['Codigo_Articulo']= df_validacion['Codigo_Articulo'].astype(int)\n",
    "    df_validacion['Sucursal']= df_validacion['Sucursal'].astype(int)\n",
    "    df_validacion.to_csv(f'{folder}/{etiqueta}_ALGO_01_Datos_Validacion.csv', index=False)\n",
    "    print(f'-> ** Validación Exportada: {etiqueta}_ALGO_01_Datos_Validacion.csv *** : ventana: {ventana}  - {fecha}')\n",
    "    \n",
    "    Exportar_Pronostico(df_forecast, proveedor, etiqueta, 'ALGO_01')  # Impactar Datos en la Interface        \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL para obtener el pronóstico\n",
    "def get_forecast( id_proveedor, lbl_proveedor, period_lengh=30, algorithm='basic', f1=None, f2=None, f3=None, current_date=None ):\n",
    "    \"\"\"\n",
    "    Genera la predicción de demanda según el algoritmo seleccionado.\n",
    "\n",
    "    Parámetros:\n",
    "    - id_proveedor: ID del proveedor.\n",
    "    - lbl_proveedor: Etiqueta del proveedor.\n",
    "    - period_lengh: Número de días del período a analizar (por defecto 30).\n",
    "    - algorithm: Algoritmo a utilizar.\n",
    "    - current_date: Fecha de referencia; si es None, se toma la fecha máxima de los datos.\n",
    "    - factores de ponderación: F1, F2, F3  (No importa en que unidades estén, luego los hace relativos al total del peso)\n",
    "\n",
    "    Retorna:\n",
    "    - Un DataFrame con las predicciones.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Dentro del get_forecast')\n",
    "    print(f'FORECAST control: {id_proveedor} - {lbl_proveedor} - ventana: {period_lengh} - {algorithm} factores: {f1} - {f2} - {f3}')\n",
    "    # Generar los datos de entrada\n",
    "    data, articulos = generar_datos(id_proveedor, lbl_proveedor)\n",
    "\n",
    "        # Determinar la fecha base\n",
    "    if current_date is None:\n",
    "        current_date = data['Fecha'].max()  # Se toma la última fecha en los datos\n",
    "    else:\n",
    "        current_date = pd.to_datetime(current_date)  # Se asegura que sea un objeto datetime\n",
    "\n",
    "    print(f'Fecha actual {current_date}')\n",
    "    \n",
    "\n",
    "    # Selección del algoritmo de predicción\n",
    "    match algorithm:\n",
    "        case 'ALGO_01':\n",
    "            return Procesar_ALGO_01(data, id_proveedor, lbl_proveedor, period_lengh, current_date, f1, f2, f3)\n",
    "        case 'ALGO_05':\n",
    "            return Procesar_ALGO_05(data, id_proveedor, lbl_proveedor, period_lengh, current_date)\n",
    "        case _:\n",
    "            raise ValueError(f\"Error: El algoritmo '{algorithm}' no está implementado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 189 - 189_BODEGAS_LOPEZ - ventana: 30 - ALGO_05 factores: None - None - None\n",
      "Dentro del get_forecast\n",
      "FORECAST control: 189 - 189_BODEGAS_LOPEZ - ventana: 30 - ALGO_05 factores: None - None - None\n",
      "-> Datos Recuperados del CACHE: 189, Label: 189_BODEGAS_LOPEZ\n",
      "Fecha actual 2025-02-23 00:00:00\n",
      "-> ** Validación Exportada: 189_BODEGAS_LOPEZ_ALGO_05_Datos_Validacion.csv *** : ventana: 30  - 2025-02-23 00:00:00\n",
      "-> ** Pronostico Guardado en: data/189_BODEGAS_LOPEZ_Pronostico_ALGO_05.csv **\n",
      "dbname=diarco_data user=postgres password=aladelta10$ host=140.99.164.229 port=5432\n",
      "✅ Inserción completada: 307 registros insertados.\n",
      "✅ Conexión cerrada.\n",
      "------------------------------------------------------------------\n",
      "Procesando: 189 - 189_BODEGAS_LOPEZ - ventana: 30 - ALGO_01 factores: 100 - 10 - 10\n",
      "Dentro del get_forecast\n",
      "FORECAST control: 189 - 189_BODEGAS_LOPEZ - ventana: 30 - ALGO_01 factores: 100 - 10 - 10\n",
      "-> Datos Recuperados del CACHE: 189, Label: 189_BODEGAS_LOPEZ\n",
      "Fecha actual 2025-02-23 00:00:00\n",
      "--> ALGO_01 ventana 30 - Peso de los Factores Utilizados: último: 100 previo: 10 año anterior: 10\n",
      "Dentro del Calcular_Demanda_ALGO_01\n",
      "FORECATS control: 189 - 189_BODEGAS_LOPEZ - ventana: 30 - factores: 100 - 10 - 10\n",
      "-> ** Validación Exportada: 189_BODEGAS_LOPEZ_ALGO_01_Datos_Validacion.csv *** : ventana: 30  - 2025-02-23 00:00:00\n",
      "-> ** Pronostico Guardado en: data/189_BODEGAS_LOPEZ_Pronostico_ALGO_01.csv **\n",
      "dbname=diarco_data user=postgres password=aladelta10$ host=140.99.164.229 port=5432\n",
      "✅ Inserción completada: 237 registros insertados.\n",
      "✅ Conexión cerrada.\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#EJECUCIÓN MASIVA x LISTA\n",
    "\n",
    "proveedores = [\n",
    "    # {\"id\": 20, \"nombre\": \"MOLINOS RIO DE LA PLATA\", \"label\": \"20_MOLINOS\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    # {\"id\": 20, \"nombre\": \"MOLINOS RIO DE LA PLATA\", \"label\": \"20_MOLINOS\", \"ventana\": 30, \"algoritmo\" : \"ALGO_01\", \"f1\": 90, \"f2\": 10, \"f3\": 20},\n",
    "    # {\"id\": 25, \"nombre\": \"CAFES LA VIRGINIA S.A.\", \"label\": \"25_LA_VIRGINIA\", \"ventana\": 30, \"algoritmo\"  :  \"ALGO_05\"},\n",
    "    # {\"id\": 25, \"nombre\": \"CAFES LA VIRGINIA S.A.\", \"label\": \"25_LA_VIRGINIA\", \"ventana\": 30, \"algoritmo\"  :   \"ALGO_01\", \"f1\": 80, \"f2\": 10, \"f3\": 20},\n",
    "    # {\"id\": 62, \"nombre\": \"ARCOR\",\"label\":\"62_ARCOR\", \"label\": \"62_ARCOR\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    # {\"id\": 98, \"nombre\": \"FRATELLI BRANCA DESTILERIAS S.A.\", \"label\": \"98_FRATELLI_BRANCA\", \"ventana\": 30, \"algoritmo\" : \"ALGO_01\", \"f1\": 70, \"f2\": 10, \"f3\": 20},\n",
    "    # {\"id\": 98, \"nombre\": \"FRATELLI BRANCA DESTILERIAS S.A.\", \"label\": \"98_FRATELLI_BRANCA\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    # {\"id\": 140, \"nombre\": \"UNILEVER DE ARGENTINA S.A.\", \"label\": \"140_UNILEVER\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    {\"id\": 189, \"nombre\": \"BODEGAS Y VIÑEDOS LOPEZ S.A.I.C.\", \"label\": \"189_BODEGAS_LOPEZ\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    {\"id\": 189, \"nombre\": \"BODEGAS Y VIÑEDOS LOPEZ S.A.I.C.\", \"label\": \"189_BODEGAS_LOPEZ\", \"ventana\": 30, \"algoritmo\" : \"ALGO_01\", \"f1\": 100, \"f2\": 10, \"f3\": 10},\n",
    "    # {\"id\": 1465, \"nombre\": \"QUICKFOOD S.A.\", \"label\":\"1465_QUICKFOOD\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"},\n",
    "    # {\"id\": 327, \"nombre\": \"PALADINI S.A.\", \"label\":\"327_PALADINI\", \"ventana\": 30, \"algoritmo\" : \"ALGO_05\"}\n",
    "]\n",
    "\n",
    "for proveedor in proveedores:\n",
    "    f1 = proveedor.get(\"f1\", None)  # Si no está en el diccionario, devuelve None\n",
    "    f2 = proveedor.get(\"f2\", None)\n",
    "    f3 = proveedor.get(\"f3\", None)\n",
    "    \n",
    "    print(f'Procesando: {proveedor[\"id\"]} - {proveedor[\"label\"]} - ventana: {proveedor[\"ventana\"]} - {proveedor[\"algoritmo\"]} factores: {f1} - {f2} - {f3}')\n",
    "    \n",
    "    #get_forecast( proveedor[\"id\"], proveedor[\"label\"], proveedor[\"ventana\"], proveedor[\"algoritmo\"], proveedor[\"f1\"], proveedor[\"f2\"], proveedor[\"f3\"])\n",
    "    get_forecast(proveedor[\"id\"], proveedor[\"label\"], proveedor[\"ventana\"], proveedor[\"algoritmo\"], f1, f2, f3)\n",
    "    \n",
    "    print('------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiqueta ='189_BODEGAS_LOPEZ'\n",
    "data = pd.read_csv(f'{folder}/{etiqueta}.csv')\n",
    "data['Codigo_Articulo']= data['Codigo_Articulo'].astype(int)\n",
    "data['Sucursal']= data['Sucursal'].astype(int)\n",
    "data['Fecha']= pd.to_datetime(data['Fecha'])\n",
    "\n",
    "# import ace_tools_open as tools\n",
    "tools.display_dataframe_to_user(name=\"SET de Datos del Proveedor\", dataframe=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la consulta SQL\n",
    "sucursales = pd.read_sql(query, hola)\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"DataFrame Básico\", dataframe=sucursales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
