{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITMOS BASADOS EN SERIES TEMPORALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Modelos ARIMA con Componentes Estacionales (SARIMA)\n",
    "El modelo SARIMA (Seasonal Autoregressive Integrated Moving Average) es una extensión del modelo ARIMA que incorpora patrones estacionales y se basa en valores anteriores de la serie temporal.\n",
    "\n",
    "### Estructura del modelo SARIMA(p, d, q)(P, D, Q, s):\n",
    "* (p, d, q) → Componentes autoregresivos, diferenciales y de media móvil.\n",
    "* (P, D, Q, s) → Componentes estacionales con periodicidad 𝑠 (en este caso, s=12 meses).\n",
    "\n",
    "Este modelo aprende la relación entre:\n",
    "* Ventas recientes (p).\n",
    "* Tendencias de largo plazo (d).\n",
    "* Variaciones estacionales mensuales (P, D, Q, s).\n",
    "\n",
    "### 📌 SARIMA es adecuado cuando la demanda tiene estacionalidad anual y fluctuaciones a corto plazo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#from pmdarima.arima import auto_arima\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpmdarima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_arima\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyodbc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\__init__.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[0;32m     54\u001b[0m     tsdisplay\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\arima\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapprox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\arima\\approx.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\utils\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\utils\\array.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas_series\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_iterable\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_series\u001b[39m(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mc:\\PY\\.venv\\Lib\\site-packages\\pmdarima\\utils\\_array.pyx:1\u001b[0m, in \u001b[0;36minit pmdarima.utils._array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "#from pmdarima.arima import auto_arima\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.2.3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pmdarima' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmdarima version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpmdarima\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pmdarima' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pmdarima version:\", pmdarima.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEGIR el PROVEEDOR\n",
    "\n",
    "proveedor = 327\n",
    "label = '327-Paladini'\n",
    "\n",
    "# Cargar Datos\n",
    "data = pd.read_csv(f'data/{label}.csv')\n",
    "data.head()\n",
    "\n",
    "# Adecuar Tipos de Datos\n",
    "data['Sucursal']= data['Sucursal'].astype(int)\n",
    "data['Familia']= data['Familia'].astype(int)\n",
    "data['Rubro']= data['Rubro'].astype(int)\n",
    "data['SubRubro']= data['SubRubro'].astype(int)\n",
    "data['Clasificacion']= data['Clasificacion'].astype(int)\n",
    "data['Codigo_Articulo']= data['Codigo_Articulo'].astype(int)\n",
    "data['Fecha'] = pd.to_datetime(data['Fecha'])  # Convertir a formato datetime si aún no lo está\n",
    "data.sort_values(by='Fecha', ascending=True)  # Ordenar por fecha de menor a mayor\n",
    "\n",
    "# Crear una nueva columna con el mes y el año para análisis temporal\n",
    "data['Año-Mes'] = data['Fecha'].dt.to_period('M')\n",
    "\n",
    "# Crear una nueva columna con el formato Año-Semana (AAAA-WW) a partir de la columna Fecha\n",
    "data['Año-Semana'] = data['Fecha'].dt.strftime('%Y-%W')\n",
    "\n",
    "# Confirmar que la columna sigue siendo un campo datetime\n",
    "#print(data.dtypes)\n",
    "\n",
    "#data = data.sort_values(by='Fecha', ascending=True)  # Ordenar en orden ascendente (del más antiguo al más reciente)\n",
    "#data = data.reset_index()\n",
    "\n",
    "# Recortar Cantidad de Datos ULTIMO AÑO COMPLETO\n",
    "df = data[data['Fecha']>='2021-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔹 Explicación del Código\n",
    "#### 📊 Preprocesamiento de datos\n",
    "* Se agruparon las ventas mensuales por artículo y sucursal.\n",
    "* Se seleccionó un producto y sucursal específicos para entrenar el modelo.\n",
    "* Se convirtió el DataFrame en una serie temporal con frecuencia mensual (asfreq('M')).\n",
    "\n",
    "#### 🔎 Selección de Parámetros Automáticos con auto_arima\n",
    "* auto_arima(train, seasonal=True, m=12, trace=True) encuentra automáticamente los mejores parámetros (p, d, q) y (P, D, Q, s) considerando estacionalidad mensual (m=12).\n",
    "#### 📈 Entrenamiento del Modelo SARIMA\n",
    "* Se usa SARIMAX de statsmodels con los parámetros encontrados.\n",
    "* Se entrena el modelo en los datos históricos de ventas.\n",
    "#### 📊 Predicción y Evaluación\n",
    "* Se generan predicciones para el conjunto de prueba.\n",
    "* Se calculan métricas de error:\n",
    "* MAE (Error Absoluto Medio)\n",
    "* RMSE (Raíz del Error Cuadrático Medio)\n",
    "#### 📉 Visualización\n",
    "Se grafica la serie original junto con las predicciones para ver la precisión del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisar:\n",
    "* ✅ SARIMA es ideal cuando hay patrones estacionales fuertes en la demanda.\n",
    "* ✅ Este modelo mejora la precisión al incorporar información del año pasado y la tendencia de corto plazo.\n",
    "* ✅ Se puede entrenar un SARIMA diferente para cada producto y sucursal, o usar técnicas más avanzadas como modelos de series temporales múltiples (VAR, Prophet, LSTM) si tienes muchos productos.\n",
    "\n",
    "¡Puedes probarlo en tus datos y ver qué tan bien predice la demanda en cada tienda 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que 'Año-Mes' está en formato datetime\n",
    "df['Año-Mes'] = pd.to_datetime(df['Año-Mes'].astype(str))\n",
    "\n",
    "# Agrupar las ventas mensuales por artículo y sucursal\n",
    "df_ventas_mensuales = df.groupby(['Año-Mes', 'Codigo_Articulo', 'Sucursal'])['Unidades'].sum().reset_index()\n",
    "\n",
    "# Seleccionar un producto y una sucursal para el análisis SARIMA\n",
    "codigo_articulo = df_ventas_mensuales['Codigo_Articulo'].unique()[0]  # Primer artículo\n",
    "sucursal = df_ventas_mensuales['Sucursal'].unique()[0]  # Primera sucursal\n",
    "\n",
    "df_filtrado = df_ventas_mensuales[(df_ventas_mensuales['Codigo_Articulo'] == codigo_articulo) &\n",
    "                                  (df_ventas_mensuales['Sucursal'] == sucursal)]\n",
    "\n",
    "# Convertir en serie temporal con 'Año-Mes' como índice\n",
    "df_filtrado.set_index('Año-Mes', inplace=True)\n",
    "df_filtrado = df_filtrado.asfreq('M')  # Asegurar frecuencia mensual\n",
    "\n",
    "# Dividir en datos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "train_size = int(len(df_filtrado) * 0.8)\n",
    "train, test = df_filtrado.iloc[:train_size], df_filtrado.iloc[train_size:]\n",
    "\n",
    "# Identificar automáticamente los mejores parámetros SARIMA (p, d, q) y (P, D, Q, s)\n",
    "auto_model = auto_arima(train, seasonal=True, m=12, trace=True, stepwise=True, suppress_warnings=True)\n",
    "\n",
    "# Entrenar el modelo SARIMA con los parámetros encontrados\n",
    "order = auto_model.order\n",
    "seasonal_order = auto_model.seasonal_order\n",
    "sarima_model = sm.tsa.SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "sarima_fit = sarima_model.fit()\n",
    "\n",
    "# Generar predicciones en el conjunto de prueba\n",
    "pred_start = test.index[0]\n",
    "pred_end = test.index[-1]\n",
    "predictions = sarima_fit.predict(start=pred_start, end=pred_end)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mae = mean_absolute_error(test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "\n",
    "print(f\"Error Absoluto Medio (MAE): {mae:.2f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Graficar resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['Unidades'], label=\"Entrenamiento\", color='blue')\n",
    "plt.plot(test.index, test['Unidades'], label=\"Prueba\", color='green')\n",
    "plt.plot(test.index, predictions, label=\"Predicción SARIMA\", color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Unidades Vendidas\")\n",
    "plt.legend()\n",
    "plt.title(f\"Forecast SARIMA para Código_Articulo {codigo_articulo} en Sucursal {sucursal}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ver Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Asegurar que la columna 'Año-Mes' está en formato datetime\n",
    "df['Año-Mes'] = pd.to_datetime(df['Año-Mes'].astype(str))\n",
    "\n",
    "# Agrupar las ventas mensuales por artículo y sucursal\n",
    "df_ventas_mensuales = df.groupby(['Año-Mes', 'Codigo_Articulo', 'Sucursal'])['Unidades'].sum().reset_index()\n",
    "\n",
    "# Crear variables de lag (ventas del mes anterior y mismo mes del año anterior)\n",
    "df_ventas_mensuales['lag_1'] = df_ventas_mensuales.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'].shift(1)\n",
    "df_ventas_mensuales['lag_12'] = df_ventas_mensuales.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'].shift(12)\n",
    "\n",
    "# Eliminar filas con valores nulos generados por los lags\n",
    "df_ventas_mensuales.dropna(inplace=True)\n",
    "\n",
    "# Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df_ventas_mensuales[['lag_1', 'lag_12']]\n",
    "y = df_ventas_mensuales['Unidades']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión lineal\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Error Absoluto Medio (MAE): {mae:.2f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las predicciones\n",
    "df_forecast = df_ventas_mensuales[['Año-Mes', 'Codigo_Articulo', 'Sucursal']].copy()\n",
    "df_forecast['Forecast_Unidades'] = modelo.predict(X)  # Predicción con el modelo\n",
    "\n",
    "# Merge con el DataFrame original de ventas mensuales\n",
    "df_resultado = df_ventas_mensuales.merge(df_forecast, on=['Año-Mes', 'Codigo_Articulo', 'Sucursal'], how='left')\n",
    "\n",
    "# Ordenar por fecha y mostrar las primeras filas\n",
    "df_resultado = df_resultado.sort_values(by=['Codigo_Articulo', 'Sucursal', 'Año-Mes'])\n",
    "\n",
    "# Mostrar resultado\n",
    "import ace_tools_open  as tools\n",
    "tools.display_dataframe_to_user(name=\"Forecast de Ventas\", dataframe=df_resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) MEJORAS\n",
    "\n",
    "### ✅ Agregar más variables:\n",
    "\n",
    "* **Precio del producto:** Puede afectar la demanda.\n",
    "* **Promociones/descuentos:** Puede influir en las ventas.\n",
    "* **Tendencias globales:** Se puede agregar una tendencia para mejorar la predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Asegurar que 'Año-Mes' está en formato datetime\n",
    "df['Año-Mes'] = pd.to_datetime(df['Año-Mes'].astype(str))\n",
    "\n",
    "# Agrupar las ventas mensuales por artículo y sucursal, sumando unidades vendidas y promediando el precio\n",
    "df_ventas_mensuales = df.groupby(['Año-Mes', 'Codigo_Articulo', 'Sucursal']).agg(\n",
    "    Unidades=('Unidades', 'sum'),\n",
    "    Precio=('Precio', 'mean')  # Precio promedio del mes\n",
    ").reset_index()\n",
    "\n",
    "# Crear variables de lag (ventas del mes anterior y del mismo mes del año anterior)\n",
    "df_ventas_mensuales['lag_1'] = df_ventas_mensuales.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'].shift(1)\n",
    "df_ventas_mensuales['lag_12'] = df_ventas_mensuales.groupby(['Codigo_Articulo', 'Sucursal'])['Unidades'].shift(12)\n",
    "\n",
    "# Crear variable de lag del precio del mes anterior\n",
    "df_ventas_mensuales['lag_1_precio'] = df_ventas_mensuales.groupby(['Codigo_Articulo', 'Sucursal'])['Precio'].shift(1)\n",
    "\n",
    "# Crear variable de descuento: Si el precio bajó más de un 5% respecto al mes anterior\n",
    "df_ventas_mensuales['Descuento'] = (df_ventas_mensuales['lag_1_precio'] > df_ventas_mensuales['Precio'] * 1.05).astype(int)\n",
    "\n",
    "# Agregar una tendencia global (número de mes desde el inicio de la serie)\n",
    "df_ventas_mensuales['Tendencia'] = (df_ventas_mensuales['Año-Mes'] - df_ventas_mensuales['Año-Mes'].min()).dt.days // 30\n",
    "\n",
    "# Eliminar filas con valores nulos generados por los lags\n",
    "df_ventas_mensuales.dropna(inplace=True)\n",
    "\n",
    "# Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df_ventas_mensuales[['lag_1', 'lag_12', 'lag_1_precio', 'Descuento', 'Tendencia']]\n",
    "y = df_ventas_mensuales['Unidades']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión lineal\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Error Absoluto Medio (MAE): {mae:.2f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Mostrar los coeficientes del modelo\n",
    "coeficientes = pd.DataFrame(modelo.coef_, X.columns, columns=['Coeficiente'])\n",
    "print(coeficientes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las predicciones\n",
    "df_forecast = df_ventas_mensuales[['Año-Mes', 'Codigo_Articulo', 'Sucursal']].copy()\n",
    "df_forecast['Forecast_Unidades'] = modelo.predict(X)  # Predicción con el modelo\n",
    "\n",
    "# Merge con el DataFrame original de ventas mensuales\n",
    "df_resultado = df_ventas_mensuales.merge(df_forecast, on=['Año-Mes', 'Codigo_Articulo', 'Sucursal'], how='left')\n",
    "\n",
    "# Ordenar por fecha y mostrar las primeras filas\n",
    "df_resultado = df_resultado.sort_values(by=['Codigo_Articulo', 'Sucursal', 'Año-Mes'])\n",
    "\n",
    "# Mostrar resultado\n",
    "import ace_tools_open  as tools\n",
    "tools.display_dataframe_to_user(name=\"Forecast de Ventas\", dataframe=df_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Otra Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MODELO ML REGRESION LINEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "def create_training_data(df, period_length=30, min_date=None, max_date=None):\n",
    "    \"\"\"\n",
    "    Genera un conjunto de entrenamiento mediante ventanas móviles.\n",
    "    \n",
    "    Para cada combinación de 'Codigo_Articulo' y 'Sucursal', se calculan:\n",
    "      - 'ventas_last': Ventas acumuladas en los últimos 'period_length' días (ventana actual).\n",
    "      - 'ventas_previous': Ventas acumuladas en el período anterior inmediato (previo a la ventana actual).\n",
    "      - 'ventas_same_year': Ventas acumuladas en el mismo período del año anterior.\n",
    "      - 'target': Ventas acumuladas en el período inmediatamente posterior a la ventana actual.\n",
    "    \n",
    "    Se requiere que existan datos suficientes para cubrir cada uno de los períodos.\n",
    "    \n",
    "    Parámetros:\n",
    "      - df: DataFrame con los datos históricos.\n",
    "      - period_length: Número de días que conforman cada ventana (por defecto 30).\n",
    "      - min_date: Fecha mínima para considerar la fecha de corte (forecast_date) en el entrenamiento.\n",
    "      - max_date: Fecha máxima para la fecha de corte; se recomienda que sea menor o igual a (Fecha máxima del df - period_length).\n",
    "    \n",
    "    Retorna:\n",
    "      - training_df: DataFrame con las features y el target para cada ejemplo de entrenamiento.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que la columna 'Fecha' sea de tipo datetime\n",
    "    df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "    \n",
    "    # Definir un rango viable para la fecha de corte (forecast_date)\n",
    "    overall_min_date = df['Fecha'].min() + pd.Timedelta(days=2*period_length) + pd.DateOffset(years=1)\n",
    "    overall_max_date = df['Fecha'].max() - pd.Timedelta(days=period_length)\n",
    "    \n",
    "    if min_date is None:\n",
    "        min_date = overall_min_date\n",
    "    if max_date is None:\n",
    "        max_date = overall_max_date\n",
    "    \n",
    "    training_rows = []\n",
    "    \n",
    "    # Se agrupa la información por artículo y sucursal\n",
    "    groups = df.groupby(['Codigo_Articulo', 'Sucursal'])\n",
    "    \n",
    "    for (codigo, sucursal), group in groups:\n",
    "        group = group.sort_values('Fecha')\n",
    "        # Se obtienen las fechas únicas disponibles en el grupo\n",
    "        fechas_unicas = group['Fecha'].unique()\n",
    "        # Se filtran las fechas que se encuentran en el rango viable para forecast_date\n",
    "        viable_dates = [d for d in fechas_unicas if (d >= min_date) and (d <= max_date)]\n",
    "        \n",
    "        for forecast_date in viable_dates:\n",
    "            # Definir ventanas:\n",
    "            # Ventana actual (últimos period_length días hasta forecast_date)\n",
    "            last_start = forecast_date - pd.Timedelta(days=period_length - 1)\n",
    "            last_end = forecast_date\n",
    "            # Ventana anterior inmediata\n",
    "            previous_start = forecast_date - pd.Timedelta(days=2*period_length)\n",
    "            previous_end = forecast_date - pd.Timedelta(days=period_length)\n",
    "            # Mismo período del año anterior\n",
    "            same_year_start = forecast_date - pd.DateOffset(years=1) - pd.Timedelta(days=period_length - 1)\n",
    "            same_year_end = forecast_date - pd.DateOffset(years=1)\n",
    "            # Período objetivo (target) a pronosticar: los period_length días siguientes a forecast_date\n",
    "            target_start = forecast_date + pd.Timedelta(days=1)\n",
    "            target_end = forecast_date + pd.Timedelta(days=period_length)\n",
    "            \n",
    "            # Filtrar el grupo según cada ventana\n",
    "            mask_last = (group['Fecha'] >= last_start) & (group['Fecha'] <= last_end)\n",
    "            mask_previous = (group['Fecha'] >= previous_start) & (group['Fecha'] <= previous_end)\n",
    "            mask_same_year = (group['Fecha'] >= same_year_start) & (group['Fecha'] <= same_year_end)\n",
    "            mask_target = (group['Fecha'] >= target_start) & (group['Fecha'] <= target_end)\n",
    "            \n",
    "            ventas_last = group.loc[mask_last, 'Unidades'].sum()\n",
    "            ventas_previous = group.loc[mask_previous, 'Unidades'].sum()\n",
    "            ventas_same_year = group.loc[mask_same_year, 'Unidades'].sum()\n",
    "            target_sales = group.loc[mask_target, 'Unidades'].sum()\n",
    "            \n",
    "            # Se pueden agregar condiciones para omitir ejemplos sin información (por ejemplo, si target es nulo)\n",
    "            # En este ejemplo se incluyen todos los casos, asumiendo que ventas nulas representan 0.\n",
    "            training_rows.append({\n",
    "                'Codigo_Articulo': codigo,\n",
    "                'Sucursal': sucursal,\n",
    "                'forecast_date': forecast_date,\n",
    "                'ventas_last': ventas_last,\n",
    "                'ventas_previous': ventas_previous,\n",
    "                'ventas_same_year': ventas_same_year,\n",
    "                'target': target_sales\n",
    "            })\n",
    "    \n",
    "    training_df = pd.DataFrame(training_rows)\n",
    "    return training_df\n",
    "\n",
    "def forecast_linear_regression(df, forecast_date=None, period_length=30, training_min_date=None, training_max_date=None):\n",
    "    \"\"\"\n",
    "    Realiza la predicción de la demanda utilizando un modelo de regresión lineal.\n",
    "    \n",
    "    Procedimiento:\n",
    "      1. Se crea un conjunto de entrenamiento a partir de los datos históricos mediante la función 'create_training_data'.\n",
    "      2. Se entrena un modelo global de regresión lineal utilizando como variables independientes:\n",
    "           - ventas_last, ventas_previous, ventas_same_year\n",
    "         y como variable dependiente el target (ventas acumuladas en el período posterior).\n",
    "      3. Para cada combinación de 'Codigo_Articulo' y 'Sucursal', se calculan las features correspondientes al período\n",
    "         de pronóstico definido por 'forecast_date' y se predice la demanda.\n",
    "    \n",
    "    Parámetros:\n",
    "      - df: DataFrame con la información histórica.\n",
    "      - forecast_date: Fecha para la cual se desea pronosticar la demanda. Si no se indica, se toma (Fecha máxima - period_length).\n",
    "      - period_length: Número de días que definen cada ventana (por defecto 30).\n",
    "      - training_min_date: Fecha mínima para incluir ejemplos en el entrenamiento.\n",
    "      - training_max_date: Fecha máxima para incluir ejemplos en el entrenamiento.\n",
    "    \n",
    "    Retorna:\n",
    "      - forecast_df: DataFrame con la predicción para cada artículo y sucursal.\n",
    "      - model: Modelo de regresión lineal entrenado.\n",
    "      - training_df: Conjunto de entrenamiento utilizado.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que la columna 'Fecha' sea de tipo datetime\n",
    "    df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "    \n",
    "    # Definir la fecha de pronóstico si no se especifica\n",
    "    if forecast_date is None:\n",
    "        forecast_date = df['Fecha'].max() - pd.Timedelta(days=period_length)\n",
    "    else:\n",
    "        forecast_date = pd.to_datetime(forecast_date)\n",
    "    \n",
    "    # Crear el conjunto de entrenamiento\n",
    "    training_df = create_training_data(df, period_length=period_length, min_date=training_min_date, max_date=training_max_date)\n",
    "    \n",
    "    if training_df.empty:\n",
    "        raise ValueError(\"No se pudo generar el conjunto de entrenamiento. Verifique el rango de fechas y la información disponible.\")\n",
    "    \n",
    "    # Definir las variables independientes (features) y la variable dependiente (target)\n",
    "    features = ['ventas_last', 'ventas_previous', 'ventas_same_year']\n",
    "    X_train = training_df[features]\n",
    "    y_train = training_df['target']\n",
    "    \n",
    "    # Entrenar el modelo de regresión lineal\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Para la fecha de pronóstico, se calculan las features para cada combinación de artículo y sucursal\n",
    "    forecast_rows = []\n",
    "    groups = df.groupby(['Codigo_Articulo', 'Sucursal'])\n",
    "    \n",
    "    for (codigo, sucursal), group in groups:\n",
    "        group = group.sort_values('Fecha')\n",
    "        # Definir las ventanas para la fecha de pronóstico\n",
    "        last_start = forecast_date - pd.Timedelta(days=period_length - 1)\n",
    "        last_end = forecast_date\n",
    "        previous_start = forecast_date - pd.Timedelta(days=2*period_length)\n",
    "        previous_end = forecast_date - pd.Timedelta(days=period_length)\n",
    "        same_year_start = forecast_date - pd.DateOffset(years=1) - pd.Timedelta(days=period_length - 1)\n",
    "        same_year_end = forecast_date - pd.DateOffset(years=1)\n",
    "        \n",
    "        mask_last = (group['Fecha'] >= last_start) & (group['Fecha'] <= last_end)\n",
    "        mask_previous = (group['Fecha'] >= previous_start) & (group['Fecha'] <= previous_end)\n",
    "        mask_same_year = (group['Fecha'] >= same_year_start) & (group['Fecha'] <= same_year_end)\n",
    "        \n",
    "        ventas_last = group.loc[mask_last, 'Unidades'].sum()\n",
    "        ventas_previous = group.loc[mask_previous, 'Unidades'].sum()\n",
    "        ventas_same_year = group.loc[mask_same_year, 'Unidades'].sum()\n",
    "        \n",
    "        # Vector de features para la predicción\n",
    "        X_pred = np.array([[ventas_last, ventas_previous, ventas_same_year]])\n",
    "        pred = model.predict(X_pred)[0]\n",
    "        \n",
    "        forecast_rows.append({\n",
    "            'Codigo_Articulo': codigo,\n",
    "            'Sucursal': sucursal,\n",
    "            'forecast': int(round(pred)),\n",
    "            'ventas_last': ventas_last,\n",
    "            'ventas_previous': ventas_previous,\n",
    "            'ventas_same_year': ventas_same_year\n",
    "        })\n",
    "    \n",
    "    forecast_df = pd.DataFrame(forecast_rows)\n",
    "    return forecast_df, model, training_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fechas = pd.date_range(start='2023-01-01', end='2025-02-10', freq='D')\n",
    "np.random.seed(42)\n",
    "\n",
    "#df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la fecha de pronóstico: por ejemplo, la última fecha disponible menos period_length\n",
    "forecast_date = df['Fecha'].max() - pd.Timedelta(days=30)\n",
    "\n",
    "# Se ejecuta el algoritmo de regresión lineal para obtener la predicción\n",
    "forecast_lr, model, training_data = forecast_linear_regression(\n",
    "    df, \n",
    "    forecast_date=forecast_date, \n",
    "    period_length=30\n",
    "    # Se pueden especificar training_min_date y training_max_date si se desea limitar el conjunto de entrenamiento\n",
    ")\n",
    "\n",
    "print(\"Demanda estimada utilizando el algoritmo de regresión lineal:\")\n",
    "print(forecast_lr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en el conjunto de entrenamiento\n",
    "X_train = training_data[['ventas_last', 'ventas_previous', 'ventas_same_year']]\n",
    "y_train = training_data['target']\n",
    "y_pred_train = model.predict(X_train)\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "print(f\"MAE en entrenamiento: {mae:.2f}\")\n",
    "print(f\"MSE en entrenamiento: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_train, y_pred_train)\n",
    "print(f\"MAPE en entrenamiento: {mape:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
