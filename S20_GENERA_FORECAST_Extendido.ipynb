{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S20 - RUTINA GENERACIÓN DE DATOS ADICIONALES en el FORECAST.\n",
    "\n",
    "Parte de los forecast executión que están en estado 20 (Ya Ejecutado el Forecast), Genera en los archivos locales de detalle los gráficos a nivel línea,\n",
    "\n",
    "Los deja en archivos locales algoritmo_Pronostico_Extendido y Actualiza esl estado de 20 a 30  Forecast FINALIZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUTINA GENERADORA DE GRÁFICOS\n",
    "\n",
    "1) Leer archivo Solicitudes_Compra\n",
    "2) Leer datos adicionales y id relacionados\n",
    "3) Leer datos adicionales de la T710_Estadis_Reposición\n",
    "4) Generar GRAFICOS\n",
    "5) Actulizar Estado en connexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo importa lo necesario desde el módulo de funciones\n",
    "from funciones_forecast import (\n",
    "    get_execution_execute_by_status,\n",
    "    get_execution,\n",
    "    get_execution_by_status,\n",
    "    Close_Connection,\n",
    "    Open_Conn_Postgres,\n",
    "    update_execution\n",
    ")\n",
    "\n",
    "import pandas as pd # uso localmente la lectura de archivos.\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funciones_forecast\n",
    "print(funciones_forecast.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extender_datos_forecast(algoritmo, name, id_proveedor):\n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperar Maestro de Artículos\n",
    "    articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "        # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Leer Dataframe de los PRODUCTOS\n",
    "    conn =Open_Conn_Postgres()\n",
    "    query = \"\"\"\n",
    "    SELECT ext_code, description, id FROM public.fnd_product\n",
    "    ORDER BY ext_code;\n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    products = pd.read_sql(query, conn)\n",
    "    # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "    products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "    products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "    #Unir los dataframes por Codigo_Articulo = ext_code\n",
    "    df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "    df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "    df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "    df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "    df_merged.drop(columns=['code','name'], inplace=True)\n",
    "    \n",
    "    # SUBIR INFORMACIÓN DE ARTICULOS y ESTIDISTICA REPOSICIÓN\n",
    "    # Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "    columnas_seleccionadas = [\n",
    "        'C_PROVEEDOR_PRIMARIO', 'C_ARTICULO', 'C_SUCU_EMPR', 'I_PRECIO_VTA',\n",
    "        'I_COSTO_ESTADISTICO', 'Q_FACTOR_VENTA_ESP', 'Q_FACTOR_VTA_SUCU',\n",
    "        'F_ULTIMA_VTA', 'Q_VENTA_30_DIAS', 'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_TRANSF_PEND',\n",
    "        'Q_DIAS_CON_STOCK', 'Q_REPONER', 'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL',\n",
    "        'Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "    ]\n",
    "\n",
    "    # Filtrar el dataframe con las columnas seleccionadas\n",
    "    df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "    df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "    \n",
    "    # Realizar la fusión de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "    df_merged = df_merged.merge(\n",
    "        df_nuevo, \n",
    "        left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "        right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "        how='left'\n",
    "    )\n",
    "    df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL RECORRE FORECAST EXCEC con STATUS 20 y pasa a 30\n",
    "\n",
    "fes = get_execution_by_status(20)\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 20  #FORECAST OK\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 20].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        # Llamar a la función que genera los gráficos y datos extendidos\n",
    "        df_extendido = extender_datos_forecast(algoritmo, name, id_proveedor)\n",
    "\n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_extendido.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        fes.at[index, \"supply_forecast_execution_status_id\"] = 30\n",
    "        # ✅ Actualizar directamente en la base de datos el estado a 30\n",
    "        update_execution(execution_id, supply_forecast_execution_status_id=30)\n",
    "        print(f\"Estado actualizado a 30 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name ='62_ARCOR'\n",
    "algoritmo = '62_ARCOR_ALGO_05'\n",
    "id_proveedor = '62'\n",
    "\n",
    "# Recuperar Maestro de Artículos\n",
    "articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "# Recuperando Forecast Calculado\n",
    "df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_proveedor = '189'\n",
    "name= '189_BODEGAS_LOPEZ'\n",
    "forecast_execution_id ='1c15b1e1-a00a-4da7-bf39-257a830a4f8a'\n",
    "forecast_execution_execute_id ='xxx'\n",
    "supplier_id = '8e6e66f7-2482-451b-a656-5a1999343186'\n",
    "algoritmo = '189_BODEGAS_LOPEZ_ALGO_04'\n",
    "\n",
    "\n",
    "df_forecast_ext = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "df_forecast_ext['Codigo_Articulo']= df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "df_forecast_ext['Sucursal']= df_forecast_ext['Sucursal'].astype(int)\n",
    "df_forecast_ext.fillna(0)   # Por si se filtró algún missing value\n",
    "\n",
    "\n",
    "# Mostrar la tabla con los gráficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast Extendido\", dataframe=df_forecast_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fes = get_execution_by_status(40)\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Pendientes Ejecución\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.display_dataframe_to_user(name=\"Pendientes Ejecución\", dataframe=df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALCULAR Datos Faltantes en Lote de Archivos.\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 40].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(execution_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    \n",
    "    try:\n",
    "        df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "        df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "        df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "        df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "        # Recuperar Maestro de Artículos\n",
    "        articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "        # Recuperando Forecast Calculado\n",
    "        # df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "        # df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "        # print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "        # Tomo el Calculado para pegar al final los datos.\n",
    "        df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "        df_forecast['Codigo_Articulo']= df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "        df_forecast['Sucursal']= df_forecast_ext['Sucursal'].astype(int)\n",
    "        df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "\n",
    "\n",
    "            # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "        conn =Open_Conn_Postgres()\n",
    "        query = \"\"\"\n",
    "        SELECT code, name, id FROM public.fnd_site\n",
    "        ORDER BY code \n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        stores = pd.read_sql(query, conn)\n",
    "        # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "        stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "        stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "        # Leer Dataframe de los PRODUCTOS\n",
    "        conn =Open_Conn_Postgres()\n",
    "        query = \"\"\"\n",
    "        SELECT ext_code, description, id FROM public.fnd_product\n",
    "        ORDER BY ext_code;\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        products = pd.read_sql(query, conn)\n",
    "        # Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "        products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "        products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "        #Unir los dataframes por Codigo_Articulo = ext_code\n",
    "        df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "        df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "        df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "        df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "        df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "        df_merged.drop(columns=['code','name'], inplace=True)\n",
    "\n",
    "        # SUBIR INFORMACIÓN DE ARTICULOS y ESTIDISTICA REPOSICIÓN\n",
    "        # Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "        columnas_seleccionadas = [\n",
    "            'C_PROVEEDOR_PRIMARIO', 'C_ARTICULO', 'C_SUCU_EMPR', 'I_PRECIO_VTA',\n",
    "            'I_COSTO_ESTADISTICO', 'Q_FACTOR_VENTA_ESP', 'Q_FACTOR_VTA_SUCU',\n",
    "            'F_ULTIMA_VTA', 'Q_VENTA_30_DIAS', 'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_TRANSF_PEND',\n",
    "            'Q_DIAS_CON_STOCK', 'Q_REPONER', 'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL',\n",
    "            'Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "        ]\n",
    "\n",
    "        # Filtrar el dataframe con las columnas seleccionadas\n",
    "        df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "        df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "\n",
    "        # Realizar la fusión de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "        df_merged = df_merged.merge(\n",
    "            df_nuevo, \n",
    "            left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "            right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "            how='left'\n",
    "        )\n",
    "        df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)\n",
    "        \n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_merged.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARREGLAR FORECAST EXTENDIDO\n",
    "\n",
    "df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "# Recuperar Maestro de Artículos\n",
    "articulos = pd.read_csv(f'{folder}/{name}_Articulos.csv')\n",
    "\n",
    "# Recuperando Forecast Calculado\n",
    "# df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "# df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "# print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "# Tomo el Calculado para pegar al final los datos.\n",
    "df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "df_forecast['Codigo_Articulo']= df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "df_forecast['Sucursal']= df_forecast_ext['Sucursal'].astype(int)\n",
    "df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "\n",
    "\n",
    "    # AGREGAR DATOS COMPLEMENTARIOS para Subir a CONNEXA\n",
    "conn =Open_Conn_Postgres()\n",
    "query = \"\"\"\n",
    "SELECT code, name, id FROM public.fnd_site\n",
    "ORDER BY code \n",
    "\"\"\"\n",
    "# Ejecutar la consulta SQL\n",
    "stores = pd.read_sql(query, conn)\n",
    "# Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "# Leer Dataframe de los PRODUCTOS\n",
    "conn =Open_Conn_Postgres()\n",
    "query = \"\"\"\n",
    "SELECT ext_code, description, id FROM public.fnd_product\n",
    "ORDER BY ext_code;\n",
    "\"\"\"\n",
    "# Ejecutar la consulta SQL\n",
    "products = pd.read_sql(query, conn)\n",
    "# Intentar convertir el campo 'code' a int, eliminando las filas que no se puedan convertir\n",
    "products = products[pd.to_numeric(products['ext_code'], errors='coerce').notna()].copy()\n",
    "products['ext_code'] = products['ext_code'].astype(int)\n",
    "\n",
    "#Unir los dataframes por Codigo_Articulo = ext_code\n",
    "df_merged = df_forecast.merge(products, left_on='Codigo_Articulo', right_on='ext_code', how='left')\n",
    "df_merged.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "df_merged.drop(columns=['ext_code','description'], inplace=True)\n",
    "\n",
    "df_merged = df_merged.merge(stores, left_on = 'Sucursal', right_on='code', how='left')\n",
    "df_merged.rename(columns={'id': 'site_id'}, inplace=True)\n",
    "df_merged.drop(columns=['code','name'], inplace=True)\n",
    "\n",
    "# SUBIR INFORMACIÓN DE ARTICULOS y ESTIDISTICA REPOSICIÓN\n",
    "# Seleccionar las columnas requeridas en un nuevo dataframe  FALTA ,I_COSTO_ESTADISTICO,I_PRECIO_VTA\n",
    "columnas_seleccionadas = [\n",
    "    'C_PROVEEDOR_PRIMARIO', 'C_ARTICULO', 'C_SUCU_EMPR', 'I_PRECIO_VTA',\n",
    "    'I_COSTO_ESTADISTICO', 'Q_FACTOR_VENTA_ESP', 'Q_FACTOR_VTA_SUCU',\n",
    "    'F_ULTIMA_VTA', 'Q_VENTA_30_DIAS', 'Q_VENTA_15_DIAS', 'Q_VENTA_DOMINGO', 'Q_TRANSF_PEND',\n",
    "    'Q_DIAS_CON_STOCK', 'Q_REPONER', 'Q_REPONER_INCLUIDO_SOBRE_STOCK', 'Q_VENTA_DIARIA_NORMAL',\n",
    "    'Q_DIAS_STOCK', 'Q_DIAS_SOBRE_STOCK', 'Q_DIAS_ENTREGA_PROVEEDOR'\n",
    "]\n",
    "\n",
    "# Filtrar el dataframe con las columnas seleccionadas\n",
    "df_nuevo = articulos[columnas_seleccionadas].copy()\n",
    "df_nuevo['C_SUCU_EMPR']= df_nuevo['C_SUCU_EMPR'].astype(int)\n",
    "\n",
    "# Realizar la fusión de los DataFrames utilizando 'Sucursal' y 'Codigo_Articulo' como claves\n",
    "df_merged = df_merged.merge(\n",
    "    df_nuevo, \n",
    "    left_on=['Sucursal', 'Codigo_Articulo'], \n",
    "    right_on=['C_SUCU_EMPR', 'C_ARTICULO'], \n",
    "    how='left'\n",
    ")\n",
    "df_merged.drop(columns=['C_SUCU_EMPR','C_ARTICULO'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el archivo CSV\n",
    "file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "df_merged.to_csv(file_path, index=False)\n",
    "print(f\"Archivo guardado: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer Dataframe de FORECAST EXECUTION \n",
    "        \n",
    "fes = get_execution_by_status(20)\n",
    "\n",
    "# Mostrar la tabla con los gráficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast execution by STATUS\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
