{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S50 - ACTUALIZAR DATOS DEL HEADER ya Existentes en CONNEXA \n",
    "\n",
    "Parte de los forecast executión que están en estado 50 (Ya publicados OK), Genera los datos acumulados del registro excec y sube los archivos a connexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías estándar\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "# Importar librerías de terceros\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Importar funciones necesarias del módulo `funciones_forecast`\n",
    "from funciones_forecast import (\n",
    "    Open_Conn_Postgres,\n",
    "    Open_Connection,\n",
    "    Close_Connection,\n",
    "    get_execution_by_status,\n",
    "    Open_Postgres_retry,\n",
    "    mover_archivos_procesados,\n",
    "    actualizar_site_ids,\n",
    "    insertar_graficos_forecast,\n",
    "    get_precios,\n",
    "    get_execution_execute_by_status,\n",
    "    update_execution,\n",
    "    update_execution_execute,\n",
    "    create_execution_execute_result,\n",
    "    generar_mini_grafico,\n",
    "    generar_grafico_base64\n",
    ")\n",
    "\n",
    "# Importar librerías adicionales necesarias\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Cargar configuraciones desde archivo `.env`\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fes = get_execution_execute_by_status(45)\n",
    "tools.display_dataframe_to_user(name=\"Contenido de Archivos Markdown\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJECUTABLE PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_datos_stock(id_proveedor, etiqueta):\n",
    "    secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "    folder = secrets[\"FOLDER_DATOS\"]\n",
    "    \n",
    "    #  Intento recuperar datos cacheados\n",
    "    try:         \n",
    "        print(f\"-> Generando datos para ID: {id_proveedor}, Label: {etiqueta}\")\n",
    "        # Configuración de conexión\n",
    "        conn = Open_Connection()\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # FILTRA solo PRODUCTOS HABILITADOS y Traer datos de STOCK y PENDIENTES desde PRODUCCIÓN\n",
    "        # ----------------------------------------------------------------\n",
    "        query = f\"\"\"              \n",
    "        SELECT A.[C_PROVEEDOR_PRIMARIO] as Codigo_Proveedor\n",
    "            ,S.[C_ARTICULO] as Codigo_Articulo\n",
    "            ,S.[C_SUCU_EMPR] as Codigo_Sucursal\n",
    "            ,S.[I_PRECIO_VTA] as Precio_Venta\n",
    "            ,S.[I_COSTO_ESTADISTICO] as Precio_Costo\n",
    "            ,S.[Q_FACTOR_VTA_SUCU] as Factor_Venta\n",
    "            ,ST.Q_UNID_ARTICULO + ST.Q_PESO_ARTICULO AS Stock_Unidades-- Stock Cierre Dia Anterior\n",
    "            ,(R.[Q_VENTA_30_DIAS] + R.[Q_VENTA_15_DIAS]) * S.[Q_FACTOR_VTA_SUCU] AS Venta_Unidades_30_Dias -- OJO convertida desde BULTOS DIARCO\n",
    "                    \n",
    "            ,(ST.Q_UNID_ARTICULO + ST.Q_PESO_ARTICULO)* S.[I_COSTO_ESTADISTICO] AS Stock_Valorizado-- Stock Cierre Dia Anterior\n",
    "            ,(R.[Q_VENTA_30_DIAS] + R.[Q_VENTA_15_DIAS]) * S.[Q_FACTOR_VTA_SUCU] * S.[I_COSTO_ESTADISTICO] AS Venta_Valorizada\n",
    "\n",
    "            ,ROUND(((ST.Q_UNID_ARTICULO + ST.Q_PESO_ARTICULO)* S.[I_COSTO_ESTADISTICO]) / \t\n",
    "                ((R.[Q_VENTA_30_DIAS] + R.[Q_VENTA_15_DIAS]+0.0001) * S.[Q_FACTOR_VTA_SUCU] * S.[I_COSTO_ESTADISTICO] ),0) * 30\n",
    "                AS Dias_Stock\n",
    "                    \n",
    "            ,S.[F_ULTIMA_VTA]\n",
    "            ,S.[Q_VTA_ULTIMOS_15DIAS] * S.[Q_FACTOR_VTA_SUCU] AS VENTA_UNIDADES_1Q -- OJO esto está en BULTOS DIARCO\n",
    "            ,S.[Q_VTA_ULTIMOS_30DIAS] * S.[Q_FACTOR_VTA_SUCU] AS VENTA_UNIDADES_2Q -- OJO esto está en BULTOS DIARCO\n",
    "                \n",
    "        FROM [DIARCOP001].[DiarcoP].[dbo].[T051_ARTICULOS_SUCURSAL] S\n",
    "        INNER JOIN [DIARCOP001].[DiarcoP].[dbo].[T050_ARTICULOS] A\n",
    "            ON A.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "        LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T060_STOCK] ST\n",
    "            ON ST.C_ARTICULO = S.[C_ARTICULO] \n",
    "            AND ST.C_SUCU_EMPR = S.[C_SUCU_EMPR]\n",
    "        LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T710_ESTADIS_REPOSICION] R\n",
    "            ON R.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "            AND R.[C_SUCU_EMPR] = S.[C_SUCU_EMPR]\n",
    "\n",
    "        WHERE S.[M_HABILITADO_SUCU] = 'S' -- Permitido Reponer\n",
    "            AND A.M_BAJA = 'N'  -- Activo en Maestro Artículos\n",
    "            AND A.[C_PROVEEDOR_PRIMARIO] = {id_proveedor} -- Solo del Proveedor\n",
    "                        \n",
    "        ORDER BY S.[C_ARTICULO],S.[C_SUCU_EMPR];\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        df_stock = pd.read_sql(query, conn)\n",
    "        file_path = f'{folder}/{etiqueta}_Stock.csv'\n",
    "        df_stock['Codigo_Proveedor']= df_stock['Codigo_Proveedor'].astype(int)\n",
    "        df_stock['Codigo_Articulo']= df_stock['Codigo_Articulo'].astype(int)\n",
    "        df_stock['Codigo_Sucursal']= df_stock['Codigo_Sucursal'].astype(int)\n",
    "        df_stock.fillna(0, inplace= True)\n",
    "        # df_stock.to_csv(file_path, index=False, encoding='utf-8')        \n",
    "        print(f\"---> Datos de STOCK guardados: {file_path}\")\n",
    "        return df_stock\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "\n",
    "def obtener_demora_oc(id_proveedor, etiqueta):\n",
    "    secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "    folder = secrets[\"FOLDER_DATOS\"]\n",
    "    \n",
    "    #  Intento recuperar datos cacheados\n",
    "    try:         \n",
    "        print(f\"-> Generando datos para ID: {id_proveedor}, Label: {etiqueta}\")\n",
    "        # Configuración de conexión\n",
    "        conn = Open_Connection()\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # FILTRA solo PRODUCTOS HABILITADOS y Traer datos de STOCK y PENDIENTES desde PRODUCCIÓN\n",
    "        # ----------------------------------------------------------------\n",
    "        query = f\"\"\"              \n",
    "        SELECT  [C_OC]\n",
    "            ,[U_PREFIJO_OC]\n",
    "            ,[U_SUFIJO_OC]      \n",
    "            ,[U_DIAS_LIMITE_ENTREGA]\n",
    "            , DATEADD(DAY, [U_DIAS_LIMITE_ENTREGA], [F_ENTREGA]) as FECHA_LIMITE\n",
    "            , DATEDIFF (DAY, DATEADD(DAY, [U_DIAS_LIMITE_ENTREGA], [F_ENTREGA]), GETDATE()) as Demora\n",
    "            ,[C_PROVEEDOR] as Codigo_Proveedor\n",
    "            ,[C_SUCU_COMPRA] as Codigo_Sucursal\n",
    "            ,[C_SUCU_DESTINO]\n",
    "            ,[C_SUCU_DESTINO_ALT]\n",
    "            ,[C_SITUAC]\n",
    "            ,[F_SITUAC]\n",
    "            ,[F_ALTA_SIST]\n",
    "            ,[F_EMISION]\n",
    "            ,[F_ENTREGA]    \n",
    "            ,[C_USUARIO_OPERADOR]    \n",
    "            \n",
    "        FROM [DIARCOP001].[DiarcoP].[dbo].[T080_OC_CABE]  \n",
    "        WHERE [C_SITUAC] = 1\n",
    "        AND C_PROVEEDOR = {id_proveedor} \n",
    "        AND DATEADD(DAY, [U_DIAS_LIMITE_ENTREGA], [F_ENTREGA]) < GETDATE();\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        df_demoras = pd.read_sql(query, conn)\n",
    "        df_demoras['Codigo_Proveedor']= df_demoras['Codigo_Proveedor'].astype(int)\n",
    "        df_demoras['Codigo_Sucursal']= df_demoras['Codigo_Sucursal'].astype(int)\n",
    "        df_demoras['Demora']= df_demoras['Demora'].astype(int)\n",
    "        df_demoras.fillna(0, inplace= True)         \n",
    "        print(f\"---> Datos de OC DEMORADAS Recuperados: {etiqueta}\")\n",
    "        return df_demoras\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Leer Dataframe de FORECAST EXECUTION  de Estado 50 y Actualizar HEADER\n",
    "fes = get_execution_execute_by_status(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 36\t- 1 - \tEstaod OC Pendiente         -                        \tPendiente \n",
    "* 36\t- 2 -\tEstado OC Cerrada           -                       \tCumplida  \n",
    "* 36\t- 3 -\tEstado OC Anulada           -                       \tAnulada   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S50 ACTUALIZAR SOLO CABECERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elegido = '596_PROCTER_ALGO_01'\n",
    "for index, row in fes[fes[\"name\"] == elegido].iterrows():\n",
    "    \n",
    "# for index, row in fes[fes[\"fee_status_id\"] == 50].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"forecast_execution_id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "    supplier_id = row[\"supplier_id\"]\n",
    "    \n",
    "    folderP = folder + '/procesado'\n",
    "\n",
    "    print(f\"Algoritmo: {algoritmo}  - Name: {name} exce_id: {forecast_execution_execute_id} id: Proveedor {id_proveedor}\")\n",
    "    print(f\"supplier-id: {supplier_id} ----------------------------------------------------\")\n",
    "\n",
    "    try:        \n",
    "        # DATOS COMPLEMENTARIOS\n",
    "        df_stock = obtener_datos_stock(id_proveedor= id_proveedor, etiqueta= algoritmo )\n",
    "        total_stock_valorizado = float(round(df_stock['Stock_Valorizado'].sum() / 1000000, 2))\n",
    "        total_venta_valorizada = float(round(df_stock['Venta_Valorizada'].sum() / 1000000, 2))\n",
    "        days= int( total_stock_valorizado / total_venta_valorizada * 30 )\n",
    "        # Condiciones Dias de STOCK\n",
    "        if days > 30:\n",
    "            semaforo= 'green'\n",
    "        elif 10 < days <= 30:\n",
    "            semaforo ='yellow'\n",
    "        elif days <= 10:\n",
    "            semaforo ='red'\n",
    "        else:\n",
    "            semaforo = 'white' # Valor predeterminado\n",
    "\n",
    "        # DEMORA de OC\n",
    "        df_demora = obtener_demora_oc(id_proveedor= id_proveedor, etiqueta= algoritmo )\n",
    "        if df_demora.empty:  # Verifica si el DataFrame está vacío\n",
    "            maximo_atraso_oc = 0\n",
    "        else:\n",
    "            maximo_atraso_oc = int(round(df_demora['Demora'].max()))\n",
    "        \n",
    "        # ARTICULOS FALTANTES\n",
    "        articulos_faltantes = df_stock[df_stock[\"Stock_Unidades\"] == 0][\"Codigo_Articulo\"].nunique()\n",
    "        if articulos_faltantes > 5:\n",
    "            quiebres= 'R'\n",
    "        elif 1 < articulos_faltantes <= 5:\n",
    "            quiebres ='Y'\n",
    "        elif articulos_faltantes <= 1:\n",
    "            quiebres ='G'\n",
    "        else:\n",
    "            quiebres = 'white' # Valor predeterminado\n",
    "                                \n",
    "        update_execution_execute(\n",
    "            forecast_execution_execute_id,\n",
    "            otif = randint(70, 100),  # Simulación de OTIF entre 70 y 100\n",
    "            sotck_days = days, # Viene de la Nueva Rutina              \n",
    "            sotck_days_colors = semaforo, # Nueva Rutina\n",
    "            maximum_backorder_days = maximo_atraso_oc, # Calcula Mäxima Demora\n",
    "            contains_breaks = quiebres  # ICONO de FALTANTES\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Stock Actualizado para {execution_id}\")\n",
    "        \n",
    "        # # ✅ Morver Archivo a carpeta de Procesado ....\n",
    "        # mover_archivos_procesados(algoritmo, folder)\n",
    "        # print(f\"✅ Archivo movido a Procesado: {algoritmo}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faltantes = df_stock[df_stock[\"Stock_Unidades\"] == 0]\n",
    "tools.display_dataframe_to_user(name=\"Articulos con FALTANTES\", dataframe=df_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARCHES y ARREGLO DE PROBLEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUALZAR STOCK execution_result\n",
    "\n",
    "conn = Open_Connection()\n",
    "        \n",
    "# ----------------------------------------------------------------\n",
    "# FILTRA solo PRODUCTOS HABILITADOS y Traer datos de STOCK y PENDIENTES desde PRODUCCIÓN\n",
    "# ----------------------------------------------------------------\n",
    "query = f\"\"\"              \n",
    "SELECT A.[C_PROVEEDOR_PRIMARIO] as Codigo_Proveedor\n",
    "    ,S.[C_ARTICULO] as Codigo_Articulo\n",
    "    ,S.[C_SUCU_EMPR] as Codigo_Sucursal\n",
    "    ,S.[I_PRECIO_VTA] as Precio_Venta\n",
    "    ,S.[I_COSTO_ESTADISTICO] as Precio_Costo\n",
    "    ,S.[Q_FACTOR_VTA_SUCU] as Factor_Venta\n",
    "    ,ST.Q_UNID_ARTICULO + ST.Q_PESO_ARTICULO AS Stock_Unidades-- Stock Cierre Dia Anterior\n",
    "    ,(R.[Q_VENTA_30_DIAS] + R.[Q_VENTA_15_DIAS]) * S.[Q_FACTOR_VTA_SUCU] AS Venta_Unidades_30_Dias -- OJO convertida desde BULTOS DIARCO\n",
    "            \n",
    "    ,(ST.Q_UNID_ARTICULO + ST.Q_PESO_ARTICULO)* S.[I_COSTO_ESTADISTICO] AS Stock_Valorizado-- Stock Cierre Dia Anterior\n",
    "    ,(R.[Q_VENTA_30_DIAS] + R.[Q_VENTA_15_DIAS]) * S.[Q_FACTOR_VTA_SUCU] * S.[I_COSTO_ESTADISTICO] AS Venta_Valorizada\n",
    "\n",
    "    ,S.[F_ULTIMA_VTA]\n",
    "    ,S.[Q_VTA_ULTIMOS_15DIAS] * S.[Q_FACTOR_VTA_SUCU] AS VENTA_UNIDADES_1Q -- OJO esto está en BULTOS DIARCO\n",
    "    ,S.[Q_VTA_ULTIMOS_30DIAS] * S.[Q_FACTOR_VTA_SUCU] AS VENTA_UNIDADES_2Q -- OJO esto está en BULTOS DIARCO\n",
    "        \n",
    "FROM [DIARCOP001].[DiarcoP].[dbo].[T051_ARTICULOS_SUCURSAL] S\n",
    "INNER JOIN [DIARCOP001].[DiarcoP].[dbo].[T050_ARTICULOS] A\n",
    "    ON A.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T060_STOCK] ST\n",
    "    ON ST.C_ARTICULO = S.[C_ARTICULO] \n",
    "    AND ST.C_SUCU_EMPR = S.[C_SUCU_EMPR]\n",
    "LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T710_ESTADIS_REPOSICION] R\n",
    "    ON R.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "    AND R.[C_SUCU_EMPR] = S.[C_SUCU_EMPR]\n",
    "\n",
    "WHERE S.[M_HABILITADO_SUCU] = 'S' -- Permitido Reponer\n",
    "    AND A.M_BAJA = 'N'  -- Activo en Maestro Artículos\n",
    "    AND A.[C_PROVEEDOR_PRIMARIO] IN ( 596)  -- Solo LISTA DE PROVEEDORES\n",
    "                \n",
    "ORDER BY S.[C_ARTICULO],S.[C_SUCU_EMPR];\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar la consulta SQL\n",
    "df_stock = pd.read_sql(query, conn)\n",
    "\n",
    "\n",
    "file_path = f'{folder}/TOTAL_Stock.csv'\n",
    "df_stock['Codigo_Proveedor']= df_stock['Codigo_Proveedor'].astype(int)\n",
    "df_stock['Codigo_Articulo']= df_stock['Codigo_Articulo'].astype(int)\n",
    "df_stock['Codigo_Sucursal']= df_stock['Codigo_Sucursal'].astype(int)\n",
    "df_stock.fillna(0, inplace= True)\n",
    "df_stock.to_csv(file_path, index=False, encoding='utf-8')        \n",
    "print(f\"---> Datos de STOCK guardados: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"DATOS de STOCK\", dataframe=df_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "\n",
    "# === CONFIGURACIÓN DE CONEXIÓN A POSTGRESQL ===\n",
    "secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "\n",
    "DB_TYPE = \"postgresql\"\n",
    "DB_USER = secrets['USUARIO4']\n",
    "DB_PASS = secrets['CONTRASENA4']  # ⚠️ Reemplazar por la contraseña real o usar variables de entorno\n",
    "DB_HOST = secrets['SERVIDOR4']\n",
    "DB_PORT = secrets['PUERTO4']\n",
    "DB_NAME = secrets['BASE4']\n",
    "\n",
    "# Crear engine de conexión\n",
    "engine = sqlalchemy.create_engine(\n",
    "    f\"{DB_TYPE}://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "# === FUNCION PARA ACTUALIZAR STOCK EN BATCHES DE UPDATE INDIVIDUAL (LENTO) ===\n",
    "def actualizar_stock(df, engine, batch_size=10000):\n",
    "    total_actualizados = 0\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "\n",
    "            # Crear lista de valores para cada fila del batch\n",
    "            values = [\n",
    "                {\n",
    "                    \"ext_product_code\": str(row[\"Codigo_Articulo\"]),\n",
    "                    \"ext_site_code\": str(row[\"Codigo_Sucursal\"]),\n",
    "                    \"ext_supplier_code\": str(row[\"Codigo_Proveedor\"]),\n",
    "                    \"quantity_stock\": float(row[\"Stock_Unidades\"])\n",
    "                }\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Consulta SQL con parámetros bindeados\n",
    "            update_sql = text(\"\"\"\n",
    "                UPDATE spl_supply_forecast_execution_execute_result\n",
    "                SET quantity_stock = :quantity_stock\n",
    "                WHERE ext_product_code = :ext_product_code\n",
    "                    AND ext_site_code = :ext_site_code\n",
    "                    AND ext_supplier_code = :ext_supplier_code\n",
    "            \"\"\")\n",
    "\n",
    "            # Ejecutar el batch de updates\n",
    "            conn.execute(update_sql, values)\n",
    "            total_actualizados += len(values)\n",
    "            print(f\"Batch {i // batch_size + 1}: {len(values)} registros actualizados.\")\n",
    "\n",
    "    print(f\"✅ Total registros actualizados: {total_actualizados}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def actualizar_stock_masivo(df, engine):\n",
    "    with engine.begin() as conn:\n",
    "        print(\"1. Creando tabla temporal...\")\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            DROP TABLE IF EXISTS tmp_stock_update;\n",
    "            CREATE TEMP TABLE tmp_stock_update (\n",
    "                ext_product_code VARCHAR,\n",
    "                ext_site_code VARCHAR,\n",
    "                ext_supplier_code VARCHAR,\n",
    "                quantity_stock NUMERIC\n",
    "            ) ON COMMIT DROP;\n",
    "        \"\"\"))\n",
    "\n",
    "        print(\"2. Cargando datos en tabla temporal...\")\n",
    "\n",
    "        # Convertir tipos para coincidir con el esquema\n",
    "        df_tmp = df[[\"Codigo_Articulo\", \"Codigo_Sucursal\", \"Codigo_Proveedor\", \"Stock_Unidades\"]].copy()\n",
    "        df_tmp.columns = [\"ext_product_code\", \"ext_site_code\", \"ext_supplier_code\", \"quantity_stock\"]\n",
    "        \n",
    "        # === CORRECCIÓN: Valores negativos a 0 ===\n",
    "        df_tmp[\"quantity_stock\"] = df_tmp[\"quantity_stock\"].clip(lower=0)\n",
    "        df_tmp[\"ext_product_code\"] = df_tmp[\"ext_product_code\"].astype(str)\n",
    "        df_tmp[\"ext_site_code\"] = df_tmp[\"ext_site_code\"].astype(str)\n",
    "        df_tmp[\"ext_supplier_code\"] = df_tmp[\"ext_supplier_code\"].astype(str)\n",
    "\n",
    "        # Cargar en bloque usando COPY (eficiente)\n",
    "        df_tmp.to_sql(\"tmp_stock_update\", con=engine, if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "        print(\"3. Ejecutando UPDATE masivo...\")\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            UPDATE spl_supply_forecast_execution_execute_result AS t\n",
    "            SET quantity_stock = u.quantity_stock\n",
    "            FROM tmp_stock_update AS u\n",
    "            WHERE t.ext_product_code = u.ext_product_code\n",
    "                AND t.ext_site_code = u.ext_site_code\n",
    "                AND t.ext_supplier_code = u.ext_supplier_code;\n",
    "        \"\"\"))\n",
    "\n",
    "        print(\"✅ Actualización finalizada con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUTAR\n",
    "\n",
    "# === CARGA DEL DATAFRAME ===\n",
    "# df = pd.read_csv(\"ruta_al_archivo.csv\")  # O si ya está cargado, usar directamente\n",
    "actualizar_stock_masivo(df_stock, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ARREGLAR PROBLEMAS\n",
    "df_sin_duplicados = df_forecast_ext.drop_duplicates(subset=['Codigo_Articulo', 'Sucursal'], keep='first')\n",
    "file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido_Con_Graficos.csv\"\n",
    "df_sin_duplicados.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Función para decodificar y mostrar una imagen en Jupyter Notebook\n",
    "    \n",
    "img_data = base64.b64decode(mini_grafico)\n",
    "buffer = BytesIO(img_data)\n",
    "\n",
    "# Mostrar la imagen en Jupyter Notebook\n",
    "display(Image(buffer.getvalue()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar en base de datos            \n",
    "update_execution_execute(\n",
    "    forecast_execution_execute_id,\n",
    "    supply_forecast_execution_status_id=50,\n",
    "    monthly_sales_in_millions=total_venta,\n",
    "    monthly_purchases_in_millions=total_costo,\n",
    "    monthly_net_margin_in_millions=total_margen,\n",
    "    graphic=mini_grafico,\n",
    "    total_products=total_productos,\n",
    "    total_units=total_unidades,\n",
    "    otif = randint(70, 100),  # Simulación de OTIF entre 70 y 100\n",
    "    sotck_days = randint(10,76), # Simulación de stock_days entre 10 y 76\n",
    "    sotck_days_colors ='green', # Simulación de semaforo\n",
    "    maximum_backorder_days = randint(0,45) # Simulación de oc_delay entre 0 y 45\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
