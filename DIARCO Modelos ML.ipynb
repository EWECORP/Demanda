{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bdca35",
   "metadata": {},
   "source": [
    "NOTA: Este notebook solo contiene codigo para explorar alternativas de estimación de la demanda.\n",
    "\n",
    "La idea es identificar el algoritmo que mejor se adapta a cada situación y hacer una evaluación de su accuracy.\n",
    "\n",
    "Haremos las evaluaciónes importando directamente los datos desde la BD y guardando un cache local en modo csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1cde4",
   "metadata": {},
   "source": [
    "# IMPORTACIONES BÁSICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaecf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872ca22",
   "metadata": {},
   "source": [
    "# CARGAR DATOS DE LA DEMANDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb64fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.54.200.90;PORT=1433;DATABASE=data-sync;UID=eettlin;PWD=lOc4l_eXt$24;\n"
     ]
    }
   ],
   "source": [
    "# OBTENER PARÁMETROS de la CONEXIÓN\n",
    "\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "secrets = dotenv_values(\".env\")\n",
    "local_secrets = dotenv_values(\".env.dev\")\n",
    "\n",
    "DRIVER = secrets[\"DRIVER2\"]\n",
    "SERVIDOR = secrets[\"SERVIDOR2\"]\n",
    "PUERTO = secrets[\"PUERTO2\"]\n",
    "BASE = secrets[\"BASE2\"]\n",
    "USUARIO = secrets[\"USUARIO2\"]\n",
    "CONTRASENA = secrets[\"CONTRASENA2\"]\n",
    "\n",
    "constr = f'DRIVER={DRIVER};SERVER={SERVIDOR};PORT={PUERTO};DATABASE={BASE};UID={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "# conn_str = (\n",
    "#     \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "#     \"SERVER=10.54.200.90;\"\n",
    "#     \"PORT=1433;\"\n",
    "#     \"DATABASE=data-sync;\"\n",
    "#     \"UID=eettlin;\"\n",
    "#     \"PWD=lOc4l_eXt$24;\"\n",
    "# )\n",
    "\n",
    "print (constr)\n",
    "#print (conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listado de Sucursales\n",
    "\n",
    "id_tienda\tsuc_nombre\tsucursal\tformato\n",
    "1\tMERCADO CENTRAL\tMercado Central\tMAYORISTA\n",
    "2\tPOSADAS\tPosadas\tMAYORISTA\n",
    "3\tTRELEW\tTrelew\tMAYORISTA\n",
    "4\tCASANOVA\tCasanova\tMAYORISTA\n",
    "5\tSAN MARTIN\tSan Martin\tMAYORISTA\n",
    "7\tBARRACAS\tBarracas\tMAYORISTA\n",
    "8\tNEUQUEN\tNeuquen\tMAYORISTA\n",
    "10\tJUNIN\tJunin\tMAYORISTA\n",
    "11\tBAHIA BLANCA\tBahía Blanca\tMAYORISTA\n",
    "12\tLA RIOJA\tLa Rioja\tMAYORISTA\n",
    "13\tPILAR\tPilar\tMAYORISTA\n",
    "15\tBERAZATEGUI\tBerazategui\tMAYORISTA\n",
    "16\tCORRIENTES\tCorrientes\tMAYORISTA\n",
    "18\tPERGAMINO\tPergamino\tMAYORISTA\n",
    "19\tESQUEL\tEsquel\tMAYORISTA\n",
    "20\tSANTA ROSA\tSanta Rosa\tMAYORISTA\n",
    "21\tCOM. RIVADAVIA\tComod. Rivadavia\tMAYORISTA\n",
    "22\t9 DE JULIO\t9 de Julio\tMAYORISTA\n",
    "23\tBARILOCHE\tBariloche\tMAYORISTA\n",
    "24\tLANUS\tLanus\tMAYORISTA\n",
    "25\tPINAMAR\tPinamar\tMAYORISTA\n",
    "26\tSALTA\tSalta\tMAYORISTA\n",
    "27\tVILLA MERCEDES\tVilla Mercedes\tMAYORISTA\n",
    "28\tZAPALA\tZapala\tMAYORISTA\n",
    "29\tMORON\tMorón\tMAYORISTA\n",
    "30\tRIO GALLEGOS\tRío Gallegos\tMAYORISTA\n",
    "31\tTANDIL\tTandil\tMAYORISTA\n",
    "32\tCORONEL SUAREZ\tCoronel Suarez\tMAYORISTA\n",
    "33\tCHASCOMUS\tChascomus\tMAYORISTA\n",
    "34\tVIEDMA\tViedma\tMAYORISTA\n",
    "35\tCONCORDIA\tConcordia\tMAYORISTA\n",
    "36\tPARANA\tParaná\tMAYORISTA\n",
    "37\tRIO GRANDE\tRío Grande\tMAYORISTA\n",
    "38\tMARTINEZ\tMartinez\tMAYORISTA\n",
    "41\tBASE 2\tBase 2\tMAYORISTA\n",
    "42\tCALETA OLIVIA\tCaleta Olivia\tMAYORISTA\n",
    "43\tR. SAENZ PENA\tR.S.Peña\tMAYORISTA\n",
    "44\tCALAFATE\tCalafate\tMAYORISTA\n",
    "45\tSANTIAGO DEL ESTERO\tSantiago del Estero\tMAYORISTA\n",
    "46\tCUTRAL CO\tCutral-Co\tMAYORISTA\n",
    "47\tSAN RAFAEL\tSan Rafael\tMAYORISTA\n",
    "48\tUSHUAIA\tUshuaia                                           \tMAYORISTA\n",
    "49\tTRENQUE LAUQUEN\tTrenque Lauquen\tPUEBLO\n",
    "60\tPEHUAJO\tPehuajó\tPUEBLO\n",
    "61\tPUERTO DESEADO\tPuerto Deseado\tPUEBLO\n",
    "62\tTRES ARROYOS\tTres Arroyos\tPUEBLO\n",
    "64\tGENERAL PICO\tGral Pico\tMAYORISTA\n",
    "65\tANELO\tAñelo\tMAYORISTA\n",
    "66\tLUJAN\tLuján\tMAYORISTA\n",
    "67\tPIEDRABUENA\tPiedra Buena\tPUEBLO\n",
    "68\tEL BOLSON\tEl Bolsón\tPUEBLO\n",
    "69\tVENADO TUERTO\tVenado Tuerto\tPUEBLO\n",
    "70\tPICO TRUNCADO\tPico Truncado\tPUEBLO\n",
    "71\tJUNIN DE LOS ANDES\tJunín de los Andes\tPUEBLO\n",
    "72\tSAN MIGUEL\tSan Miguel\tBARRIO\n",
    "73\tGUALEGUAYCHU\tGualeguaychú\tPUEBLO\n",
    "74\tCHILECITO\tChilecito\tPUEBLO\n",
    "75\tCHOS MALAL\tChos Malal\tPUEBLO\n",
    "76\tBOLIVAR\tBolívar\tPUEBLO\n",
    "77\tCONCORDIA-DP\tConcordia II\tPUEBLO\n",
    "78\tRAFAELA\tRafaela\tPUEBLO\n",
    "79\tVIEDMA II\tViedma II\tMAYORISTA\n",
    "85\tBASE III\tBASE III\tC.LOGISTICO\n",
    "86\tCONCEPCION TUCUMAN DP\tConcepción Tucumán\tPUEBLO\n",
    "87\tRIO TURBIO\tRío Turbio\tMAYORISTA\n",
    "100\tGOYA CORRIENTES-DP\tGoya Corrientes\tPUEBLO\n",
    "301\tCÓRDOBA 4325\tCórdoba 4325\tBARRIO\n",
    "302\tEVA PERON 2701 CAP\tEva Peron 2701\tBARRIO\n",
    "303\tSAENZ 1136 CAP\tSáenz 1136\tBARRIO\n",
    "304\tPATRICIOS 796\tPatricios 796\tBARRIO\n",
    "305\tCORRIENTES 3374\tCorrientes 3374\tBARRIO\n",
    "306\tTACUARI 419 CAP\tTacuarí 419\tBARRIO\n",
    "307\tPUEYRREDON 582 CAP\tPueyrredón 582\tBARRIO\n",
    "308\tCABILDO 4201\tCabildo 4201\tBARRIO\n",
    "309\tMONROE 1616\tMonroe 1616\tBARRIO\n",
    "310\tSANABRIA\tSanabria\tBARRIO\n",
    "311\tCORRALES 6902\tCorrales 6902\tBARRIO\n",
    "312\tESCALADA\tEscalada\tBARRIO\n",
    "313\tCASTANARES 4758 CAP\tCastañares\tBARRIO\n",
    "314\tCHILAVERT\tChilavert\tBARRIO\n",
    "315\tPEDERNERA\tPedernera\tBARRIO\n",
    "316\tRAMÓN FALCON\tRamón Falcon\tBARRIO\n",
    "317\tENTRE RIOS 785 CAP\tEntre Rios 785\tBARRIO\n",
    "318\tCABILDO 3327 CAP\tCabildo 3327/9\tBARRIO\n",
    "319\tALVAREZ JONTE 3629 CAP\tMiranda 3629 (ex Alvarez Jonte)\tBARRIO\n",
    "320\tAV. CAZÓN 1342\tAv. Cazón 1342\tBARRIO\n",
    "322\tALMAFUERTE 3464 SAN JUSTO\tAlmafuerte 3464\tBARRIO\n",
    "323\tALBERDI 6429 CAP\tAlberdi 6429\tBARRIO\n",
    "326\tAV. URQUIZA 4884\tAV. URQUIZA 4884\tBARRIO\n",
    "328\tCALLE 897, N° 4322\tCalle 897, N° 4322\tBARRIO\n",
    "330\tAV. FRANCISCO BEIRÓ 3261\tAv. Francisco Beiró 3261\tBARRIO\n",
    "332\tRAMÓN LISTA 5020/22\tRamón Lista 5020/22\tBARRIO\n",
    "333\tBARTOLOMÉ MITRE 994\tBartolomé Mitre 994\tBARRIO\n",
    "334\tAVELLANEDA 1990 CAP\tAv. Avellaneda 1990/1992\tBARRIO\n",
    "335\tPATRICIOS 945 CAP\tAv. Regimiento Patricios 945\tBARRIO\n",
    "336\tAV. SANTA FE 4611\tAv. Santa Fe 4611\tBARRIO\n",
    "337\tAV. BOEDO 452\tAv. Boedo 452\tBARRIO\n",
    "339\tSAN LORENZO 2301 SAN MARTIN\tSan Lorenzo 2301\tBARRIO\n",
    "340\tDIAZ VELEZ 4402 CAP\tAv. Díaz Vélez 4402\tBARRIO\n",
    "341\tLAVALLE 520\tLavalle 520\tBARRIO\n",
    "342\tMONTES DE OCA 1049 CAP\tAv. Montes de Oca 1049\tBARRIO\n",
    "344\tRIO DE JANEIRO  838/40\tRio de Janeiro  838/40\tBARRIO\n",
    "345\tMARTIN GARCIA 653 CAP\tMartín García 653\tBARRIO\n",
    "346\tINDEPENDENCIA 2490 CAP\tIndependencia 2490\tBARRIO\n",
    "348\tCORRIENTES 2970 CAP\tCorrientes 2970\tBARRIO\n",
    "351\tRIVADAVIA 8517 CAP\tRivadavia 8517\tBARRIO\n",
    "353\tRAMON CASTILLO\tRamón Castillo 1721\tBARRIO\n",
    "354\tHONORIO PUEYRREDON CAP\tHonorio Pueyrredon 1740\tBARRIO\n",
    "357\tACOYTE 260 CAP\tAcoyte 262\tBARRIO\n",
    "359\tBALBIN 3650\tBalbin 3650\tBARRIO\n",
    "360\tPUEYRREDON 1068\tPueyrredon 1068\tBARRIO\n",
    "361\tCORRIENTES 5270 \tCorrientes 5270 \tBARRIO\n",
    "362\tGASCON 685\tGascon 685\tBARRIO\n",
    "367\tAV ALTE BROWN 1057\tAv Alte Brown 1057\tBARRIO\n",
    "368\tALVEAR 2331\tAlvear 2331\tBARRIO\n",
    "369\tLOPE DE VEGA 1888\tLope De Vega 1888\tBARRIO\n",
    "370\tBAEZ 777\tBaez 777\tBARRIO\n",
    "373\tJUAN BAUTISTA ALBERDI 77\tJuan Bautista Alberdi 77\tBARRIO\n",
    "381\tCORRIENTES 5288\tCorrientes 5288\tBARRIO\n",
    "382\tRIVADAVIA 8224\tRivadavia 8224\tBARRIO\n",
    "383\tSAN MARTIN 546\tSan Martin 546\tBARRIO\n",
    "384\tAV.RIVADAVIA 11678\tAv.Rivadavia 11678\tBARRIO\n",
    "385\tCORRIENTES 1900\tCorrientes 1900\tBARRIO\n",
    "386\tRIO DE JANEIRO 793\tRio De Janeiro 793\tBARRIO\n",
    "387\tDIAZ VELEZ 41734175\tDiaz Velez 41734175\tBARRIO\n",
    "388\tAV.CORRALES 7417\tAv.Corrales 7417\tBARRIO\n",
    "390\tSAN JUAN 2692\tSan Juan 2692\tBARRIO\n",
    "391\tTRIUNVIRATO 4207\tTriunvirato 4207\tBARRIO\n",
    "393\tFRANCISCO BEIRO  4386 \tFrancisco Beiro  4386 \tBARRIO\n",
    "394\tLAVALLE 1765\tLavalle 1765\tBARRIO\n",
    "395\tNAZARRE 251214\tNazarre 251214\tBARRIO\n",
    "396\tAV.CABILDO 4662\tAv.Cabildo 4662\tBARRIO\n",
    "397\tAV.FRANCISCO BEIRO 5450\tAv.Francisco Beiro 5450\tBARRIO\n",
    "399\tCAMARONES 14121424\tCamarones 14121424\tBARRIO\n",
    "400\tAV.SARMIENTO 1641\tAv.Sarmiento 1641\tBARRIO\n",
    "402\tCABELLO 3417\tCabello 3417\tBARRIO\n",
    "403\tRIVADAVIA 2446\tRivadavia 2446\tBARRIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dc3d8",
   "metadata": {},
   "source": [
    "### GENERAR DATOS DE UNA SUCURSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148710ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_15780\\1659785625.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Configuración de conexión\n",
    "conn_str = f'DRIVER={DRIVER};SERVER={SERVIDOR};PORT={PUERTO};DATBASE={BASE};UID={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "query = \"\"\"\n",
    "-- Consulta con datos históricos y actuales\n",
    "SELECT V.[F_VENTA] as Fecha\n",
    "      ,V.[C_ARTICULO] as Codigo_Articulo\n",
    "      ,V.[C_SUCU_EMPR] as Sucursal\n",
    "      ,V.[I_PRECIO_VENTA] as Precio\n",
    "      ,V.[I_PRECIO_COSTO] as Costo\n",
    " --     ,V.[I_VENDIDO] as Total \n",
    "      ,V.[Q_UNIDADES_VENDIDAS] as Unidades\n",
    "   \n",
    "      ,V.[C_FAMILIA] as Familia\n",
    "      ,A.[C_RUBRO] as Rubro\n",
    "      ,A.[C_SUBRUBRO_1] as SubRubro\n",
    " --     ,A.[C_SUBRUBRO_2]\n",
    "      ,A.[N_ARTICULO] as Nombre_Articulo\n",
    "\t  ,A.[C_CLASIFICACION_COMPRA] as Clasificacion\n",
    " \n",
    "  FROM [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "  LEFT JOIN [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "\tON V.C_ARTICULO = A.C_ARTICULO\n",
    "WHERE V.[C_SUCU_EMPR] BETWEEN  73 AND 73  AND\n",
    "V.F_VENTA >='20210101' AND A.M_BAJA ='N';\n",
    "\"\"\"\n",
    "data = pd.read_sql(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31e4e6",
   "metadata": {},
   "source": [
    "#### Guardar Archivo en DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86daafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados exitosamente en 'datos_SanMartin.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar en CSV\n",
    "data.to_csv('data/P073_Gualeguaychú.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Datos guardados exitosamente en 'datos_SanMartin.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4792ec7",
   "metadata": {},
   "source": [
    "## GENERAR LOCALMENTE TODAS LAS SUCURSALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f400c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer Lista de Sucursales a Procesar\n",
    "import chardet\n",
    "\n",
    "with open('data/Sucursales.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result['encoding'])  # Muestra la codificación detectada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo con codificación UTF-8\n",
    "sucursales = pd.read_csv('data/Sucursales.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Visualización de las primeras filas\\\n",
    "print(sucursales.head())\n",
    "\n",
    "sucursales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8582b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERAR sobre el Liistado de Sucursales y Generar Archivos CSV\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Configuración de conexión a SQL Server\n",
    "conn_str = f'DRIVER={DRIVER};SERVER={SERVIDOR};PORT={PUERTO};DATABASE={BASE};UID={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Iterar sobre cada tienda en el DataFrame \"sucursales\"\n",
    "for index, row in sucursales.iterrows():\n",
    "    tienda = row['id_tienda']  # Asegúrate de que esta columna existe en el DataFrame\n",
    "    label = row['label']  # Asegúrate de que esta columna existe en el DataFrame\n",
    "    \n",
    "    # Reemplazar {tienda} en la consulta con el ID de la tienda actual\n",
    "    query = f\"\"\"\n",
    "    SELECT V.[F_VENTA] as Fecha\n",
    "          ,V.[C_ARTICULO] as Codigo_Articulo\n",
    "          ,V.[C_SUCU_EMPR] as Sucursal\n",
    "          ,V.[I_PRECIO_VENTA] as Precio\n",
    "          ,V.[I_PRECIO_COSTO] as Costo\n",
    "          ,V.[Q_UNIDADES_VENDIDAS] as Unidades\n",
    "          ,V.[C_FAMILIA] as Familia\n",
    "          ,A.[C_RUBRO] as Rubro\n",
    "          ,A.[C_SUBRUBRO_1] as SubRubro\n",
    "          ,A.[N_ARTICULO] as Nombre_Articulo\n",
    "          ,A.[C_CLASIFICACION_COMPRA] as Clasificacion\n",
    "    FROM [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "    LEFT JOIN [DCO-DBCORE-P02].[DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "        ON V.C_ARTICULO = A.C_ARTICULO\n",
    "    WHERE V.[C_SUCU_EMPR] = {tienda} AND V.F_VENTA >= '20240101'  AND A.M_BAJA ='N';\n",
    "    \"\"\"\n",
    "\n",
    "    # Ejecutar la consulta SQL\n",
    "    data = pd.read_sql(query, conn)\n",
    "\n",
    "    # Guardar los resultados en un archivo CSV con el nombre de la tienda\n",
    "    file_path = f'data/{label}.csv'\n",
    "    data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "# Cerrar la conexión después de la iteración\n",
    "conn.close()\n",
    "print(\"Proceso finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96738ad8",
   "metadata": {},
   "source": [
    "## CARGAR DATASET desde ARCHIVO LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efe4c6",
   "metadata": {},
   "source": [
    "### Revisar FICHERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "df = pd.read_csv('data/datos_Gualelguaychu_Pueblo.csv', parse_dates=['Fecha'])\n",
    "\n",
    "# Visualización de las primeras filas\\\n",
    "print(df.head())\n",
    "\n",
    "# Información general del dataframe\n",
    "print(df.info())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5774b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('direct_marketing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7937ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1043707 entries, 0 to 1043706\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count    Dtype         \n",
      "---  ------           --------------    -----         \n",
      " 0   Fecha            1043707 non-null  datetime64[ns]\n",
      " 1   Codigo_Articulo  1043707 non-null  float64       \n",
      " 2   Sucursal         1043707 non-null  float64       \n",
      " 3   Precio           1043707 non-null  float64       \n",
      " 4   Costo            1043707 non-null  float64       \n",
      " 5   Unidades         1043707 non-null  float64       \n",
      " 6   Familia          1043707 non-null  float64       \n",
      " 7   Rubro            1043707 non-null  float64       \n",
      " 8   SubRubro         1043707 non-null  float64       \n",
      " 9   Nombre_Articulo  1043707 non-null  object        \n",
      " 10  Clasificacion    1043707 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(9), object(1)\n",
      "memory usage: 87.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a71725",
   "metadata": {},
   "source": [
    "# DATA ANÁLISIS\n",
    "\n",
    "## Evaluación de Algoritmos para la Estimación de la Demanda\n",
    "   Este Notebook tiene como objetivo evaluar distintos enfoques para la estimación de la demanda a partir de datos históricos de ventas. Se abordan procesos de análisis exploratorio, preprocesamiento, modelado y evaluación de modelos. Los ejemplos presentados incluyen métodos estadísticos (ARIMA y Holt-Winters) y un modelo de Machine Learning (Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25fc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b71eb",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "En esta sección se analiza la distribución de las ventas, se detectan posibles valores nulos o atípicos, y se estudia la evolución temporal de la demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b51266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1043707 entries, 0 to 1043706\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count    Dtype         \n",
      "---  ------           --------------    -----         \n",
      " 0   Fecha            1043707 non-null  datetime64[ns]\n",
      " 1   Codigo_Articulo  1043707 non-null  float64       \n",
      " 2   Sucursal         1043707 non-null  float64       \n",
      " 3   Precio           1043707 non-null  float64       \n",
      " 4   Costo            1043707 non-null  float64       \n",
      " 5   Unidades         1043707 non-null  float64       \n",
      " 6   Familia          1043707 non-null  float64       \n",
      " 7   Rubro            1043707 non-null  float64       \n",
      " 8   SubRubro         1043707 non-null  float64       \n",
      " 9   Nombre_Articulo  1043707 non-null  object        \n",
      " 10  Clasificacion    1043707 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(9), object(1)\n",
      "memory usage: 87.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2319cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agregar DATOS de DIA SEMANA y SEMANA AÑO\n",
    "\n",
    "df['Dia_Semana'] = df['Fecha'].dt.weekday\n",
    "\n",
    "df['Semana_Año'] = df['Fecha'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4e2f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Codigo_Articulo</th>\n",
       "      <th>Sucursal</th>\n",
       "      <th>Precio</th>\n",
       "      <th>Costo</th>\n",
       "      <th>Unidades</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Rubro</th>\n",
       "      <th>SubRubro</th>\n",
       "      <th>Nombre_Articulo</th>\n",
       "      <th>Clasificacion</th>\n",
       "      <th>Dia_Semana</th>\n",
       "      <th>Semana_Año</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>59743.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>67.899</td>\n",
       "      <td>49.324</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>Protectores Diarios Ladysoft Clasico 20 Un    ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>59786.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>221.899</td>\n",
       "      <td>152.588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>Toallas Femeninas Ladysoft Nocturnas Soft Con ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>70807.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>965.899</td>\n",
       "      <td>495.588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>4137.0</td>\n",
       "      <td>Asadera Okey De Vidrio Budinera 26 cm         ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>70828.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>155.959</td>\n",
       "      <td>123.740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3881.0</td>\n",
       "      <td>4576.0</td>\n",
       "      <td>Cepillo Okey De Mano                          ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>70830.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>112.399</td>\n",
       "      <td>96.620</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3881.0</td>\n",
       "      <td>4576.0</td>\n",
       "      <td>Escobilla Okey de Baño                        ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fecha  Codigo_Articulo  Sucursal   Precio    Costo  Unidades  Familia  \\\n",
       "0 2022-03-16          59743.0      73.0   67.899   49.324       4.0      8.0   \n",
       "1 2022-03-16          59786.0      73.0  221.899  152.588       1.0      8.0   \n",
       "2 2022-03-16          70807.0      73.0  965.899  495.588       1.0     10.0   \n",
       "3 2022-03-16          70828.0      73.0  155.959  123.740       1.0     10.0   \n",
       "4 2022-03-16          70830.0      73.0  112.399   96.620       3.0     10.0   \n",
       "\n",
       "    Rubro  SubRubro                                    Nombre_Articulo  \\\n",
       "0  3014.0    3022.0  Protectores Diarios Ladysoft Clasico 20 Un    ...   \n",
       "1  3014.0    3022.0  Toallas Femeninas Ladysoft Nocturnas Soft Con ...   \n",
       "2  2698.0    4137.0  Asadera Okey De Vidrio Budinera 26 cm         ...   \n",
       "3  3881.0    4576.0  Cepillo Okey De Mano                          ...   \n",
       "4  3881.0    4576.0  Escobilla Okey de Baño                        ...   \n",
       "\n",
       "   Clasificacion  Dia_Semana  Semana_Año  \n",
       "0            2.0           2          11  \n",
       "1            2.0           2          11  \n",
       "2            4.0           2          11  \n",
       "3            1.0           2          11  \n",
       "4            4.0           2          11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51dd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobación de valores nulos\n",
    "print('Valores nulos por columna:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Análisis de la distribución de ventas (Unidades)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Unidades'], bins=50, kde=True)\n",
    "plt.title('Distribución de Ventas (Unidades)')\n",
    "plt.xlabel('Unidades')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Evolución de ventas a lo largo del tiempo (agrupado por fecha)\n",
    "ventas_diarias = df.groupby('Fecha')['Unidades'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(ventas_diarias['Fecha'], ventas_diarias['Unidades'], label='Ventas Diarias')\n",
    "plt.title('Evolución de Ventas Diarias')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades Vendidas')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70702ea4",
   "metadata": {},
   "source": [
    "## Preprocesamiento y Transformación de los Datos\n",
    "\n",
    "Se procede a convertir la columna `Fecha` en el índice del dataframe. Además, se selecciona un ejemplo representativo (un artículo y una tienda) para realizar la predicción. Posteriormente, se agrupan las ventas por día y se generan variables derivadas (lags y medias móviles) para enriquecer la información de entrada.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'Fecha' en índice\n",
    "df.set_index('Fecha', inplace=True)\n",
    "\n",
    "# Seleccionar un ejemplo: se elige un artículo y una tienda específicos\n",
    "codigo_articulo_ej = df['Codigo_Articulo'].unique()[0]\n",
    "sucursal_ej = df['Sucursal'].unique()[0]\n",
    "\n",
    "df_ej = df[(df['Codigo_Articulo'] == codigo_articulo_ej) & (df['Sucursal'] == sucursal_ej)].copy()\n",
    "\n",
    "# Agrupar las ventas por día\n",
    "ventas_diarias_ej = df_ej['Unidades'].resample('D').sum().fillna(0)\n",
    "\n",
    "print(ventas_diarias_ej.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718eeb4",
   "metadata": {},
   "source": [
    "### Creación de Variables (Feature Engineering)\n",
    "\n",
    "Se generan variables de retardo (lag) y medias móviles para incorporar información histórica en el modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = ventas_diarias_ej.to_frame(name='Unidades')\n",
    "\n",
    "# Crear variables de lag (por ejemplo, 1, 7 y 14 días)\n",
    "for lag in [1, 7, 14]:\n",
    "    df_model[f'lag_{lag}'] = df_model['Unidades'].shift(lag)\n",
    "\n",
    "# Crear variable de media móvil de 7 días\n",
    "df_model['rolling_mean_7'] = df_model['Unidades'].rolling(window=7).mean()\n",
    "\n",
    "# Eliminar valores nulos generados por el lag y la media móvil\n",
    "df_model.dropna(inplace=True)\n",
    "\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e66c2",
   "metadata": {},
   "source": [
    "## División de Datos en Entrenamiento y Prueba\n",
    "\n",
    "Se define un corte temporal para separar el conjunto en datos de entrenamiento y de prueba. En este ejemplo, se utiliza el último mes de datos como conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c461b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = df_model.index.max() - pd.Timedelta(days=30)\n",
    "train_data = df_model.loc[:train_end]\n",
    "test_data = df_model.loc[train_end + pd.Timedelta(days=1):]\n",
    "\n",
    "print('Periodo de Entrenamiento:', train_data.index.min(), 'a', train_data.index.max())\n",
    "print('Periodo de Prueba:', test_data.index.min(), 'a', test_data.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cee337",
   "metadata": {},
   "source": [
    "## Modelado\n",
    "\n",
    "A continuación se presentan tres enfoques de modelado:\n",
    "\n",
    "1. **ARIMA:** Modelo estadístico para series temporales.\n",
    "2. **Holt-Winters:** Suavizamiento exponencial con componentes para tendencia y estacionalidad.\n",
    "3. **Random Forest:** Modelo de Machine Learning basado en árboles de decisión, utilizando las variables derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2056846",
   "metadata": {},
   "source": [
    "### 1. Modelo ARIMA\n",
    "\n",
    "Se ajusta un modelo ARIMA sobre la serie temporal de entrenamiento y se realizan predicciones para el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00119066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de un modelo ARIMA\n",
    "# Se define el orden del modelo (p, d, q); este parámetro puede ser optimizado\n",
    "arima_order = (1, 1, 1)\n",
    "\n",
    "model_arima = ARIMA(train_data['Unidades'], order=arima_order)\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Predicción sobre el conjunto de prueba\n",
    "pred_arima = model_arima_fit.forecast(steps=len(test_data))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_data.index, train_data['Unidades'], label='Entrenamiento')\n",
    "plt.plot(test_data.index, test_data['Unidades'], label='Real')\n",
    "plt.plot(test_data.index, pred_arima, label='Predicción ARIMA')\n",
    "plt.title('Predicción con ARIMA')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43379d",
   "metadata": {},
   "source": [
    "### 2. Modelo Holt-Winters (Exponential Smoothing)\n",
    "\n",
    "Se utiliza el método de Holt-Winters para modelar la serie teniendo en cuenta la tendencia y la estacionalidad (con periodicidad semanal en este ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec74d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo Holt-Winters\n",
    "model_hw = ExponentialSmoothing(train_data['Unidades'], trend='add', seasonal='add', seasonal_periods=7)\n",
    "model_hw_fit = model_hw.fit()\n",
    "\n",
    "# Predicción sobre el conjunto de prueba\n",
    "pred_hw = model_hw_fit.forecast(steps=len(test_data))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_data.index, train_data['Unidades'], label='Entrenamiento')\n",
    "plt.plot(test_data.index, test_data['Unidades'], label='Real')\n",
    "plt.plot(test_data.index, pred_hw, label='Predicción Holt-Winters')\n",
    "plt.title('Predicción con Holt-Winters')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b57bf",
   "metadata": {},
   "source": [
    "### 3. Modelo de Machine Learning: Random Forest\n",
    "\n",
    "Se entrena un modelo de Random Forest utilizando las variables de retardo y medias móviles obtenidas en el feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de datos para el modelo de Machine Learning\n",
    "features = [col for col in df_model.columns if col != 'Unidades']\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['Unidades']\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['Unidades']\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción sobre el conjunto de prueba\n",
    "pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_data.index, train_data['Unidades'], label='Entrenamiento')\n",
    "plt.plot(test_data.index, y_test, label='Real')\n",
    "plt.plot(test_data.index, pred_rf, label='Predicción Random Forest')\n",
    "plt.title('Predicción con Random Forest')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda1eb4",
   "metadata": {},
   "source": [
    "## Evaluación de Resultados\n",
    "\n",
    "Se calculan las principales métricas de error para cada uno de los modelos: RMSE, MAE y MAPE. Esto permitirá comparar objetivamente el desempeño de los distintos enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Evaluación ARIMA\n",
    "rmse_arima = sqrt(mean_squared_error(test_data['Unidades'], pred_arima))\n",
    "mae_arima = mean_absolute_error(test_data['Unidades'], pred_arima)\n",
    "mape_arima = mean_absolute_percentage_error(test_data['Unidades'], pred_arima)\n",
    "\n",
    "print('ARIMA:')\n",
    "print(f'RMSE: {rmse_arima:.2f}')\n",
    "print(f'MAE: {mae_arima:.2f}')\n",
    "print(f'MAPE: {mape_arima:.2f}%')\n",
    "\n",
    "# Evaluación Holt-Winters\n",
    "rmse_hw = sqrt(mean_squared_error(test_data['Unidades'], pred_hw))\n",
    "mae_hw = mean_absolute_error(test_data['Unidades'], pred_hw)\n",
    "mape_hw = mean_absolute_percentage_error(test_data['Unidades'], pred_hw)\n",
    "\n",
    "print('\\\\nHolt-Winters:')\n",
    "print(f'RMSE: {rmse_hw:.2f}')\n",
    "print(f'MAE: {mae_hw:.2f}')\n",
    "print(f'MAPE: {mape_hw:.2f}%')\n",
    "\n",
    "# Evaluación Random Forest\n",
    "rmse_rf = sqrt(mean_squared_error(y_test, pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, pred_rf)\n",
    "mape_rf = mean_absolute_percentage_error(y_test, pred_rf)\n",
    "\n",
    "print('\\\\nRandom Forest:')\n",
    "print(f'RMSE: {rmse_rf:.2f}')\n",
    "print(f'MAE: {mae_rf:.2f}')\n",
    "print(f'MAPE: {mape_rf:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    # Convertir a arrays de NumPy para facilitar las operaciones vectorizadas\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Calcular el denominador de la fórmula: suma de los valores absolutos de y_true y y_pred\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    # Evitar la división por cero: donde el denominador es cero, se asigna 0 (ya que 0/0 se considera error nulo en este contexto)\n",
    "    smape_values = np.where(denominator == 0, 0, 2.0 * np.abs(y_true - y_pred) / denominator)\n",
    "    # Calcular el SMAPE medio y multiplicar por 100 para expresarlo en porcentaje\n",
    "    return np.mean(smape_values) * 100\n",
    "\n",
    "# Reemplazar la línea original de MAPE por la siguiente:\n",
    "smape_arima = smape(test_data['Unidades'], pred_arima)\n",
    "smape_hw = smape(test_data['Unidades'], pred_hw)\n",
    "smape_rf = smape(y_test, pred_rf)\n",
    "\n",
    "\n",
    "print('\\\\Metricas SMAPE:')\n",
    "print(f'Ramdon Forest: {smape_rf:.2f}')\n",
    "print(f'Hot Winters: {smape_hw:.2f}')\n",
    "print(f'ARIMA: {smape_arima:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209fd38",
   "metadata": {},
   "source": [
    "## Conclusiones y Siguientes Pasos\n",
    "\n",
    "El presente Notebook demuestra un enfoque integral para la evaluación de diferentes algoritmos de predicción de la demanda. Se recomienda continuar con la optimización de hiperparámetros, explorar modelos adicionales (incluyendo técnicas de deep learning y métodos de ensemble) y, finalmente, integrar el pipeline en el sistema de reposición automática para su validación en un entorno real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79287e4b",
   "metadata": {},
   "source": [
    "El grupo de trabajo puede beneficiarse de incluir variables que representen tanto el día de la semana como la semana del año, ya que ambas permiten capturar diferentes niveles de estacionalidad y patrones cíclicos en los datos. En particular:\n",
    "\n",
    "### Día de la semana:\n",
    "La incorporación de esta variable puede ser especialmente útil para capturar efectos a corto plazo. En muchos entornos, especialmente en el sector retail, se observa que la demanda varía significativamente entre días laborables y fines de semana. Por ejemplo, es común que se registren picos de ventas en ciertos días (por ejemplo, durante el fin de semana o en días con promociones especiales). Esta variable, al ser de naturaleza categórica, puede incluirse mediante codificación dummy o utilizando transformaciones que permitan capturar dichas variaciones.\n",
    "\n",
    "### Semana del año:\n",
    "La variable que indica la semana del año ayuda a identificar patrones de largo plazo y tendencias estacionales que se repiten anualmente, tales como las variaciones propias de temporadas de vacaciones, festividades o campañas estacionales. Este enfoque resulta relevante cuando la demanda presenta fluctuaciones significativas a lo largo del año, permitiendo al modelo ajustarse a estas variaciones y mejorar la precisión en periodos específicos.\n",
    "\n",
    "El grupo de trabajo deberá, a partir de un análisis exploratorio de los datos, determinar cuál de estas variables (o la combinación de ambas) aporta mayor valor predictivo. Es importante tener en cuenta que la inclusión simultánea de ambas puede aportar información complementaria, pero también podría generar redundancias o problemas de multicolinealidad si las dos variables están altamente correlacionadas. Por ello, se recomienda realizar pruebas empíricas y análisis de sensibilidad para validar la utilidad de cada variable en el contexto específico de la demanda a estimar.\n",
    "\n",
    "## Opinión GENERAL\n",
    "La incorporación tanto del día de la semana como de la semana del año suele resultar ventajosa, siempre que se respalde con un análisis previo que confirme la existencia de patrones estacionales a diferentes escalas temporales. Se sugiere lo siguiente:\n",
    "\n",
    "### Evaluación empírica:\n",
    "Es recomendable que el grupo de trabajo realice un análisis de descomposición de la serie temporal para identificar si existen patrones estacionales diarios o anuales. De esta forma, se podrá determinar si los cambios en la demanda se relacionan de forma consistente con días específicos de la semana o si existen fluctuaciones a nivel semanal que justifiquen la inclusión de la semana del año.\n",
    "\n",
    "### Manejo de la multicolinealidad:\n",
    "En caso de que ambas variables muestren una correlación alta, podría ser prudente aplicar técnicas de reducción de dimensionalidad o selección de variables para evitar redundancias en el modelo, lo que permitirá que la predicción sea más robusta y que se maximice la capacidad explicativa del conjunto de variables.\n",
    "\n",
    "### Flexibilidad del modelo:\n",
    "La combinación de ambas variables puede aportar un mayor grado de precisión al modelo de predicción, ya que permite captar tanto las variaciones de corto plazo (día a día) como las tendencias de largo plazo (ciclo anual). Esto es especialmente relevante en contextos donde la demanda se ve influenciada por factores operativos diarios y por eventos estacionales o promocionales.\n",
    "\n",
    "En resumen, ChatGPT opina que, de implementarse correctamente y tras una adecuada validación empírica, la inclusión de estas variables estacionales puede mejorar significativamente el rendimiento predictivo del modelo, ofreciendo una visión más completa y adaptada a los patrones inherentes a la demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17216bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Unidades.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c820a",
   "metadata": {},
   "source": [
    "## BALANCEO DE CLASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = data[data['conversion'] == 0]\n",
    "df_minority = data[data['conversion'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586215b",
   "metadata": {},
   "source": [
    "## PREDICTORAS Y TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Fecha',\n",
    " 'Codigo_Articulo',\n",
    " 'Sucursal',\n",
    " 'Precio',\n",
    " 'Costo',\n",
    " 'Familia',\n",
    " 'Rubro',\n",
    " 'SubRubro',\n",
    " 'Clasificacion']]\n",
    "      \n",
    "y = data.Unidades       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda42b8",
   "metadata": {},
   "source": [
    "## PREPARACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd606d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "object_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X[object_columns]))\n",
    "X_encoded.columns = encoder.get_feature_names_out(object_columns)\n",
    "\n",
    "X = X.drop(object_columns, axis=1).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cec795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, X_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "columns = X.columns\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bad29",
   "metadata": {},
   "source": [
    "## SEPARACIÓN DE TRAIN Y TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3b16b",
   "metadata": {},
   "source": [
    "## FUNCIÓN PARA MEDIR EL ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def mide_error(nombre_modelo, y_pred):\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'AUC de {nombre_modelo}: ', round(auc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9be67",
   "metadata": {},
   "source": [
    "# DATASET PARA SERIES TEMPORALES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0def974",
   "metadata": {},
   "source": [
    "## CARGA DEL FICHERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df_st = sm.datasets.macrodata.load_pandas().data.loc[:,['year','quarter','infl']]\n",
    "#df_st['period'] = pd.PeriodIndex(year=df_st['year'], quarter=df_st['quarter'], freq='Q')\n",
    "df_st['Fecha'] = pd.PeriodIndex.from_fields(year=df_st['year'], quarter=df_st['quarter'], freq='Q')\n",
    "\n",
    "df_st.set_index('Fecha', inplace=True)\n",
    "df_st.drop(['year', 'quarter'], axis=1, inplace=True)\n",
    "df_st.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275cd06",
   "metadata": {},
   "source": [
    "## SEPARACIÓN DE TRAIN Y TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab66999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_st.iloc[:-4]\n",
    "test_data = df_st.iloc[-4:] \n",
    "train_data = train_data['infl'].values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdec31",
   "metadata": {},
   "source": [
    "# REGRESIÓN LOGÍSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e14007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='newton-cg')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('Regresión Logística', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc78400",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15491615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('KNN', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3605f9a",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_bayes = BernoulliNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "y_pred = naive_bayes.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('Naive Bayes', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80139a74",
   "metadata": {},
   "source": [
    "# K-MEDIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "kmeans.labels_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124b1c9",
   "metadata": {},
   "source": [
    "# ÁRBOL DE DECISIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14867d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('Árbol de Decisión', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc0442",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03726c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "\n",
    "model = auto_arima(train_data, seasonal=False, suppress_warnings=True)\n",
    "model_fit = model.fit(train_data)\n",
    "predictions = model_fit.predict(n_periods=len(test_data))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_st[-12:].index.strftime('%YQ%q'), df_st[-12:].infl, label='Realidad')\n",
    "plt.plot(df_st[-4:].index.strftime('%YQ%q'), predictions, label='Predicción')\n",
    "plt.xticks(rotation=45, fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f27cf",
   "metadata": {},
   "source": [
    "# SUAVIZADO EXPONENCIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbe7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "model = ExponentialSmoothing(train_data, seasonal=None, trend='add')\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_st[-12:].index.strftime('%YQ%q'), df_st[-12:].infl, label='Realidad')\n",
    "plt.plot(df_st[-4:].index.strftime('%YQ%q'), predictions, label='Predicción')\n",
    "plt.xticks(rotation=45, fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b9a6b",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6579ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('Random Forest', y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe991a",
   "metadata": {},
   "source": [
    "# XGBBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd948ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('XGBboost', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbc57c",
   "metadata": {},
   "source": [
    "# LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier()\n",
    "hist_gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "y_pred = hist_gradient_boosting.predict_proba(X_test)[:,1]\n",
    "\n",
    "mide_error('XGBboost', y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
