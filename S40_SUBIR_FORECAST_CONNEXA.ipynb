{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S40 - ACTUALIZAR DATOS CONNEXA - SUBIR RESULTADOS FORECAST\n",
    "\n",
    "Parte de los forecast excecutión que están en estado 40 (Ya Graficados), Genera los datos acumulados del registro excec y sube los archivos a connexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESAR xxx_Ponostico_Extendido\n",
    "\n",
    "1) Leer ejecuciones con Status 40.\n",
    "2) Actualizar los Datos y el Minigráfico de la cabecera de ejecución.\n",
    "3) Cargar datos en la tabla execuciton_excecute_result.\n",
    "4) Actulizar Estado en connexa a 50 DISPONIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA TRANSFERENCIA DE ARCHIVOS.\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# LIBRERIAS NECESARIAS \n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "import psycopg2 as pg2    # Conectores para Postgres\n",
    "import pyodbc  # Conector para SQL Server\n",
    "import time  # Para medir el tiempo de ejecución\n",
    "import getpass  # Para obtener el usuario del sistema operativo\n",
    "import uuid  # Importar la librería uuid\n",
    "# Mostrar el DataFrame resultante\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Evitar Mensajes Molestos\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)\n",
    "\n",
    "secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Funciones de conexión a la base de datos\n",
    "# -----------------------------------------------------------\n",
    "def Open_Conn_Postgres():\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE4']} user={secrets['USUARIO4']} password={secrets['CONTRASENA4']} host={secrets['SERVIDOR4']} port={secrets['PUERTO4']}\"\n",
    "    try:    \n",
    "        conn = pg2.connect(conn_str)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        return None\n",
    "    \n",
    "\n",
    "def Open_Postgres_retry(max_retries=5, wait_seconds=10):\n",
    "    secrets = dotenv_values(\".env\")   # Cargar credenciales desde .env    \n",
    "    conn_str = f\"dbname={secrets['BASE4']} user={secrets['USUARIO4']} password={secrets['CONTRASENA4']} host={secrets['SERVIDOR4']} port={secrets['PUERTO4']}\"\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            conn = pg2.connect(conn_str)\n",
    "            return conn \n",
    "        except Exception as e:\n",
    "            print(f\"Error en la conexión, intento {i+1}/{max_retries}: {e}\")\n",
    "            time.sleep(wait_seconds)\n",
    "    return None  # Retorna None si todos los intentos fallan\n",
    "\n",
    "def Open_Connection():\n",
    "    secrets = dotenv_values(\".env\")   # Connection String from .env\n",
    "    conn_str = f'DRIVER={secrets[\"DRIVER2\"]};SERVER={secrets[\"SERVIDOR2\"]};PORT={secrets[\"PUERTO2\"]};DATABASE={secrets[\"BASE2\"]};UID={secrets[\"USUARIO2\"]};PWD={secrets[\"CONTRASENA2\"]}'\n",
    "    # print (conn_str) \n",
    "    try:    \n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        return conn\n",
    "    except:\n",
    "        print('Error en la Conexión')\n",
    "        return None\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "# Helper para generar identificadores únicos\n",
    "def id_aleatorio():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. Operaciones CRUD para spl_supply_forecast_execution_parameter\n",
    "# -----------------------------------------------------------\n",
    "def create_execution_parameter(supply_forecast_execution_id, supply_forecast_model_parameter_id, value):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        id_exec_param = id_aleatorio()\n",
    "        timestamp = datetime.utcnow()\n",
    "        query = \"\"\"\n",
    "            INSERT INTO public.spl_supply_forecast_execution_parameter(\n",
    "                id, \"timestamp\", supply_forecast_execution_id, supply_forecast_model_parameter_id, value\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cur.execute(query, (id_exec_param, timestamp, supply_forecast_execution_id, supply_forecast_model_parameter_id, value))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return id_exec_param\n",
    "    except Exception as e:\n",
    "        print(f\"Error en create_execution_parameter: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_parameter(exec_param_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, \"timestamp\", supply_forecast_execution_id, supply_forecast_model_parameter_id, value\n",
    "            FROM public.spl_supply_forecast_execution_parameter\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (exec_param_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"timestamp\": row[1],\n",
    "                \"supply_forecast_execution_id\": row[2],\n",
    "                \"supply_forecast_model_parameter_id\": row[3],\n",
    "                \"value\": row[4]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_parameter: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution_parameter(exec_param_id, **kwargs):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause =  \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(exec_param_id)\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution_parameter\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution_parameter(exec_param_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution_parameter: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def delete_execution_parameter(exec_param_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            DELETE FROM public.spl_supply_forecast_execution_parameter\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (exec_param_id,))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error en delete_execution_parameter: {e}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. Operaciones CRUD para spl_supply_forecast_execution_execute\n",
    "# -----------------------------------------------------------\n",
    "def create_execution_execute(end_execution, last_execution, start_execution, supply_forecast_execution_id, supply_forecast_execution_schedule_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        id_exec = id_aleatorio()\n",
    "        timestamp = datetime.utcnow()\n",
    "        query = \"\"\"\n",
    "            INSERT INTO public.spl_supply_forecast_execution_execute(\n",
    "                id, end_execution, last_execution, start_execution, \"timestamp\", supply_forecast_execution_id, supply_forecast_execution_schedule_id\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cur.execute(query, (id_exec, end_execution, last_execution, start_execution, timestamp, supply_forecast_execution_id, supply_forecast_execution_schedule_id))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return id_exec\n",
    "    except Exception as e:\n",
    "        print(f\"Error en create_execution_execute: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_execute(exec_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, end_execution, last_execution, start_execution, \"timestamp\", supply_forecast_execution_id, supply_forecast_execution_schedule_id\n",
    "            FROM public.spl_supply_forecast_execution_execute\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (exec_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"end_execution\": row[1],\n",
    "                \"last_execution\": row[2],\n",
    "                \"start_execution\": row[3],\n",
    "                \"timestamp\": row[4],\n",
    "                \"supply_forecast_execution_id\": row[5],\n",
    "                \"supply_forecast_execution_schedule_id\": row[6]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_execute: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution_execute(exec_id, **kwargs):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(exec_id)\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution_execute\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution_execute(exec_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution_execute: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def delete_execution_execute(exec_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            DELETE FROM public.spl_supply_forecast_execution_execute\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (exec_id,))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error en delete_execution_execute: {e}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. Operaciones CRUD para spl_supply_forecast_execution_execute_result\n",
    "# -----------------------------------------------------------\n",
    "def create_execution_execute_result(confidence_level, error_margin, expected_demand, average_daily_demand, lower_bound, upper_bound,\n",
    "                                    product_id, site_id, supply_forecast_execution_execute_id, algorithm, average, ext_product_code, ext_site_code, ext_supplier_code,\n",
    "                                    forcast, graphic, quantity_stock, sales_last, sales_previous, sales_same_year, supplier_id, windows, deliveries_pending):\n",
    "    conn = Open_Postgres_retry()\n",
    "    if conn is None:\n",
    "        print(\"❌ No se pudo conectar después de varios intentos\")\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        id_result = id_aleatorio()\n",
    "        timestamp = datetime.utcnow()\n",
    "        query = \"\"\"\n",
    "            INSERT INTO public.spl_supply_forecast_execution_execute_result (\n",
    "                id, confidence_level, error_margin, expected_demand, average_daily_demand, lower_bound, \"timestamp\", upper_bound, \n",
    "                product_id, site_id, supply_forecast_execution_execute_id, algorithm, average, ext_product_code, ext_site_code, ext_supplier_code, \n",
    "                forcast, graphic, quantity_stock, sales_last, sales_previous, sales_same_year, supplier_id, windows, \n",
    "                deliveries_pending\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cur.execute(query, (id_result, confidence_level, error_margin, expected_demand, average_daily_demand, lower_bound, timestamp, upper_bound, \n",
    "                            product_id, site_id, supply_forecast_execution_execute_id, algorithm, average, ext_product_code, ext_site_code, ext_supplier_code,\n",
    "                            forcast, graphic, quantity_stock, sales_last, sales_previous, sales_same_year, supplier_id, windows, deliveries_pending))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return id_result\n",
    "    except Exception as e:\n",
    "        print(f\"Error en create_execution_execute_result: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_execution_execute_result(result_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM public.spl_supply_forecast_execution_execute_result\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (result_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            return dict(zip(columns, row))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution_execute_result: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution_execute_result(result_id, **kwargs):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(result_id)\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution_execute_result\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution_execute_result(result_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution_execute_result: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def delete_execution_execute_result(result_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            DELETE FROM public.spl_supply_forecast_execution_execute_result\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (result_id,))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error en delete_execution_execute_result: {e}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Operaciones CRUD para spl_supply_forecast_execution\n",
    "# -----------------------------------------------------------\n",
    "def get_execution(execution_id):\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT id, description, name, \"timestamp\", supply_forecast_model_id, \n",
    "                ext_supplier_code, supplier_id, supply_forecast_execution_status_id\n",
    "            FROM public.spl_supply_forecast_execution\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, (execution_id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"description\": row[1],\n",
    "                \"name\": row[2],\n",
    "                \"timestamp\": row[3],\n",
    "                \"supply_forecast_model_id\": row[4],\n",
    "                \"ext_supplier_code\": row[5],\n",
    "                \"supplier_id\": row[6],\n",
    "                \"supply_forecast_execution_status_id\": row[7]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_execution: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def update_execution(execution_id, **kwargs):\n",
    "    if not kwargs:\n",
    "        print(\"No hay valores para actualizar\")\n",
    "        return None\n",
    "\n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        set_clause = \", \".join([f\"{key} = %s\" for key in kwargs.keys()])\n",
    "        values = list(kwargs.values())\n",
    "        values.append(execution_id)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            UPDATE public.spl_supply_forecast_execution\n",
    "            SET {set_clause}\n",
    "            WHERE id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(values))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        return get_execution(execution_id)  # Retorna la ejecución actualizada\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en update_execution: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn)\n",
    "\n",
    "def get_excecution_excecute_by_status(status):\n",
    "    if not status:\n",
    "        print(\"No hay estados para filtrar\")\n",
    "        return None\n",
    "    \n",
    "    conn = Open_Conn_Postgres()\n",
    "    if conn is None:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                a.id, \n",
    "                a.description, \n",
    "                a.name, \n",
    "                a.\"timestamp\", \n",
    "                a.supply_forecast_model_id, \n",
    "                a.ext_supplier_code, \n",
    "                a.graphic, \n",
    "                a.monthly_net_margin_in_millions, \n",
    "                a.monthly_purchases_in_millions, \n",
    "                a.monthly_sales_in_millions, \n",
    "                a.sotck_days AS stock_days,  -- Posible corrección\n",
    "                a.sotck_days_colors AS stock_days_colors, -- Posible corrección\n",
    "                a.supplier_id, \n",
    "                a.supply_forecast_execution_status_id,\n",
    "                b.supply_forecast_execution_schedule_id AS forecast_execution_schedule_id, \n",
    "                b.id AS forecast_execution_execute_id\n",
    "            FROM public.spl_supply_forecast_execution a\n",
    "            LEFT JOIN public.spl_supply_forecast_execution_execute b\n",
    "                ON b.supply_forecast_execution_id = a.id\n",
    "            WHERE a.supply_forecast_execution_status_id = {status};\n",
    "        \"\"\"\n",
    "        # Ejecutar la consulta SQL\n",
    "        fexsts = pd.read_sql(query, conn)\n",
    "        return fexsts\n",
    "    except Exception as e:\n",
    "        print(f\"Error en get_excecution_status: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        Close_Connection(conn) \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 0. Rutinas Locales para la generación de gráficos\n",
    "# -----------------------------------------------------------\n",
    "def generar_mini_grafico( folder, name):\n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    fecha_maxima = df_ventas[\"Fecha\"].max()\n",
    "    df_filtrado = df_ventas[df_ventas[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=150))].copy()\n",
    "\n",
    "    # Ventas Semanales\n",
    "    df_filtrado[\"Mes\"] = df_filtrado[\"Fecha\"].dt.to_period(\"M\").astype(str)\n",
    "    df_mes = df_filtrado.groupby(\"Mes\")[\"Unidades\"].sum().reset_index()\n",
    "\n",
    "    # Crear el gráfico compacto\n",
    "    fig, ax = plt.subplots(figsize=(3, 1))  # Tamaño pequeño para una visualización compacta\n",
    "    # ax.bar(df_mes[\"Mes\"], df_mes[\"Unidades\"], color=[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\"], alpha=0.7)\n",
    "    # Usar el índice como secuencia de registros\n",
    "    ax.bar(range(1, len(df_mes) + 1), df_mes[\"Unidades\"], color=[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\"], alpha=0.7)\n",
    "\n",
    "    # Eliminar ejes y etiquetas para que sea más compacto\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    # Mostrar el gráfico\n",
    "    # plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para no solapar con el título\n",
    "\n",
    "    # Guardar gráfico en base64\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_excecution_results(df_forecast_ext, forecast_execution_excecute_id, supplier_id):\n",
    "    print ('Comenzando a grabar el dataframe')\n",
    "    for _, row in df_forecast_ext.iterrows():\n",
    "        create_execution_execute_result(\n",
    "            confidence_level=0.92,  # Valor por defecto ya que no está en df_meged\n",
    "            error_margin=0.07,  # Valor por defecto\n",
    "            expected_demand=row['Forecast'],\n",
    "            average_daily_demand=row['Average'],\n",
    "            lower_bound=row['Q_DIAS_STOCK'],  # Valor por defecto\n",
    "            upper_bound=row['Q_VENTA_DIARIA_NORMAL'],  # Valor por defecto\n",
    "            product_id=row['product_id'],\n",
    "            site_id=row['site_id'],\n",
    "            supply_forecast_execution_execute_id=forecast_execution_excecute_id,\n",
    "            algorithm=row['algoritmo'],\n",
    "            average=row['Average'],\n",
    "            ext_product_code=row['Codigo_Articulo'],\n",
    "            ext_site_code=row['Sucursal'],\n",
    "            ext_supplier_code=row['id_proveedor'],\n",
    "            forcast=row['Q_REPONER_INCLUIDO_SOBRE_STOCK'],\n",
    "            graphic=row['GRAFICO'],\n",
    "            quantity_stock=row['Q_TRANSF_PEND'],  # Valor por defecto\n",
    "            sales_last=row['ventas_last'],\n",
    "            sales_previous=row['ventas_previous'],\n",
    "            sales_same_year=row['ventas_same_year'],\n",
    "            supplier_id=supplier_id,\n",
    "            windows=row['ventana'],\n",
    "            deliveries_pending=1  # Valor por defecto\n",
    "        )\n",
    "    print ('--------------------------------')\n",
    "\n",
    "\n",
    "def get_precios(id_proveedor):\n",
    "    conn = Open_Connection()\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "        A.[C_PROVEEDOR_PRIMARIO],\n",
    "        S.[C_ARTICULO]\n",
    "        ,S.[C_SUCU_EMPR]\n",
    "        ,S.[I_PRECIO_VTA]\n",
    "        ,S.[I_COSTO_ESTADISTICO]\n",
    "        --,S.[M_HABILITADO_SUCU]\n",
    "        --,A.M_BAJA                   \n",
    "        FROM [DIARCOP001].[DiarcoP].[dbo].[T051_ARTICULOS_SUCURSAL] S\n",
    "        LEFT JOIN [DIARCOP001].[DiarcoP].[dbo].[T050_ARTICULOS] A\n",
    "            ON A.[C_ARTICULO] = S.[C_ARTICULO]\n",
    "        \n",
    "        WHERE S.[M_HABILITADO_SUCU] = 'S' -- Permitido Reponer\n",
    "            AND A.M_BAJA = 'N'  -- Activo en Maestro Artículos\n",
    "            AND A.[C_PROVEEDOR_PRIMARIO] = {id_proveedor} -- Solo del Proveedor        \n",
    "        ORDER BY S.[C_ARTICULO],S.[C_SUCU_EMPR];\n",
    "    \"\"\"\n",
    "    # Ejecutar la consulta SQL\n",
    "    precios = pd.read_sql(query, conn)\n",
    "    precios['C_PROVEEDOR_PRIMARIO']= precios['C_PROVEEDOR_PRIMARIO'].astype(int)\n",
    "    precios['C_ARTICULO']= precios['C_ARTICULO'].astype(int)\n",
    "    precios['C_SUCU_EMPR']= precios['C_SUCU_EMPR'].astype(int)\n",
    "    return precios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_site_ids(df_forecast_ext, conn, name):\n",
    "    \"\"\"Reemplaza site_id en df_forecast_ext con datos válidos desde fnd_site\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    WHERE company_id = 'e7498b2e-2669-473f-ab73-e2c8b4dcc585'\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Eliminar site_id anterior si ya existía\n",
    "    df_forecast_ext = df_forecast_ext.drop(columns=['site_id'], errors='ignore')\n",
    "\n",
    "    # Merge con los stores para obtener site_id\n",
    "    df_forecast_ext = df_forecast_ext.merge(\n",
    "        stores[['code', 'id']],\n",
    "        left_on='Sucursal',\n",
    "        right_on='code',\n",
    "        how='left'\n",
    "    ).rename(columns={'id': 'site_id'})\n",
    "\n",
    "    # Validar valores faltantes\n",
    "    missing = df_forecast_ext[df_forecast_ext['site_id'].isna()]\n",
    "    if not missing.empty:\n",
    "        print(f\"⚠️ Faltan site_id en {len(missing)} registros\")\n",
    "        missing.to_csv(f\"{folder}/{name}_Missing_Site_IDs.csv\", index=False)\n",
    "    else:\n",
    "        print(\"✅ Todos los registros tienen site_id válido\")\n",
    "\n",
    "    return df_forecast_ext\n",
    "\n",
    "\n",
    "# -------------------- RUTINA PRINCIPAL --------------------\n",
    "\n",
    "# Leer Dataframe de FORECAST EXECUTION LISTOS PARA IMPORTAR A CONNEXA (DE 40 A 50)\n",
    "fes = get_excecution_excecute_by_status(40)\n",
    "\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 40].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "    supplier_id = row[\"supplier_id\"]\n",
    "    print(f\"Algoritmo: {algoritmo}  - Name: {name} exce_id: {forecast_execution_execute_id} id: Proveedor {id_proveedor}\")\n",
    "    print(f\"supplier-id: {supplier_id} ----------------------------------------------------\")\n",
    "\n",
    "    try:\n",
    "        # Leer forecast extendido\n",
    "        df_forecast_ext = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "        df_forecast_ext['Codigo_Articulo'] = df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "        df_forecast_ext['Sucursal'] = df_forecast_ext['Sucursal'].astype(int)\n",
    "        df_forecast_ext.fillna(0, inplace=True)\n",
    "        print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "        # Agregar site_id desde fnd_site\n",
    "        conn = Open_Conn_Postgres()\n",
    "        df_forecast_ext = actualizar_site_ids(df_forecast_ext, conn, name)\n",
    "        print(f\"-> Se actualizaron los site_ids: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "        # Publicar en tabla de resultados\n",
    "        publish_excecution_results(df_forecast_ext, forecast_execution_execute_id, supplier_id)\n",
    "        print(f\"-> Detalle Forecast Publicado CONNEXA: {id_proveedor}, Label: {name}\")\n",
    "\n",
    "        # Obtener precios y costos\n",
    "        precio = get_precios(id_proveedor)\n",
    "        precio['C_ARTICULO'] = precio['C_ARTICULO'].astype(int)\n",
    "        precio['C_SUCU_EMPR'] = precio['C_SUCU_EMPR'].astype(int)\n",
    "\n",
    "        # Merge con precios\n",
    "        df_forecast_ext = df_forecast_ext.merge(\n",
    "            precio,\n",
    "            left_on=['Codigo_Articulo', 'Sucursal'],\n",
    "            right_on=['C_ARTICULO', 'C_SUCU_EMPR'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Cálculo de métricas\n",
    "        df_forecast_ext['Forecast_VENTA'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_PRECIO_VTA'] / 1000).round(2)\n",
    "        df_forecast_ext['Forecast_COSTO'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_COSTO_ESTADISTICO'] / 1000).round(2)\n",
    "        df_forecast_ext['MARGEN'] = (df_forecast_ext['Forecast_VENTA'] - df_forecast_ext['Forecast_COSTO']).round(2)\n",
    "\n",
    "        # Guardar CSV actualizado\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_forecast_ext.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "\n",
    "        # Totales\n",
    "        total_venta = df_forecast_ext['Forecast_VENTA'].sum()\n",
    "        total_costo = df_forecast_ext['Forecast_COSTO'].sum()\n",
    "        total_margen = df_forecast_ext['MARGEN'].sum()\n",
    "\n",
    "        # Mini gráfico\n",
    "        mini_grafico = generar_mini_grafico(folder, name)\n",
    "\n",
    "        # Marcar como procesado\n",
    "        fes.at[index, \"supply_forecast_execution_status_id\"] = 50\n",
    "\n",
    "        # Actualizar en base de datos\n",
    "        update_execution(\n",
    "            execution_id,\n",
    "            supply_forecast_execution_status_id=50,\n",
    "            monthly_sales_in_millions=total_venta,\n",
    "            monthly_purchases_in_millions=total_costo,\n",
    "            monthly_net_margin_in_millions=total_margen,\n",
    "            graphic=mini_grafico\n",
    "        )\n",
    "        print(f\"✅ Estado actualizado a 50 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"❌ Error procesando {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUTINA PRINCIPAL RECORRE FORECAST EXCEC con STATUS 40 y pasa a 50\n",
    "\n",
    "# Filtrar registros con supply_forecast_execution_status_id = 40 Graficos Generados\n",
    "for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 40].iterrows():\n",
    "    algoritmo = row[\"name\"]\n",
    "    name = algoritmo.split('_ALGO')[0]\n",
    "    execution_id = row[\"id\"]\n",
    "    id_proveedor = row[\"ext_supplier_code\"]\n",
    "    forecast_execution_excecute_id = row['forecast_execution_execute_id']\n",
    "    supplier_id = row['supplier_id']\n",
    "    print(\"Algoritmo: \" + algoritmo + \"  - Name: \" + name + \" exce_id:\" + str(forecast_execution_excecute_id) + \" id: Proveedor \"+id_proveedor)\n",
    "    print(\"supplier-id: \" + str(supplier_id) + \"----------------------------------------------------\")\n",
    "    \n",
    "    try:        \n",
    "        # Recuperar Solicitudes de Compra Extended\n",
    "        df_forecast_ext = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "        df_forecast_ext['Codigo_Articulo']= df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "        df_forecast_ext['Sucursal']= df_forecast_ext['Sucursal'].astype(int)\n",
    "        df_forecast_ext.fillna(0)   # Por si se filtró algún missing value\n",
    "        print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "        \n",
    "        # Agregar site_id desde fnd_site\n",
    "        conn = Open_Conn_Postgres()\n",
    "        df_forecast_ext = actualizar_site_ids(df_forecast_ext, conn, name)\n",
    "        print(f\"-> Se actualizaron los site_ids: {id_proveedor}, Label: {name}\")\n",
    "        \n",
    "        print(\"❗Filas con site_id inválido:\", df_forecast_ext['site_id'].isna().sum())\n",
    "        print(\"❗Filas con product_id inválido:\", df_forecast_ext['product_id'].isna().sum())\n",
    "        \n",
    "        # Publicar Resultados en execut_result\n",
    "        publish_excecution_results(df_forecast_ext, forecast_execution_excecute_id, supplier_id)\n",
    "        print(f\"-> Detalle Forecast Publicado CONNEXA: {id_proveedor}, Label: {name}\")\n",
    "        \n",
    "        # Recuperar PRECIOS y COSTOS\n",
    "        precio = get_precios(id_proveedor)    \n",
    "        precio['C_ARTICULO']= precio['C_ARTICULO'].astype(int)\n",
    "        precio['C_SUCU_EMPR']= precio['C_SUCU_EMPR'].astype(int)\n",
    "        \n",
    "        # UNIR para CALCULAR Datos Adicionales\n",
    "        df_forecast_ext = pd.merge(\n",
    "            df_forecast_ext,  # DataFrame de artículos\n",
    "            precio,    # DataFrame de precios\n",
    "            left_on =['Codigo_Articulo', 'Sucursal'],  # Claves en 'forecast'\n",
    "            right_on=['C_ARTICULO', 'C_SUCU_EMPR'],  # Claves en 'precios'\n",
    "            how='left'  # Solo traer los productos que están en 'forecast'\n",
    "        )\n",
    "        df_forecast_ext['Forecast_VENTA'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_PRECIO_VTA'] / 1000).round(2)\n",
    "        df_forecast_ext['Forecast_COSTO'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_COSTO_ESTADISTICO'] / 1000).round(2)\n",
    "        df_forecast_ext['MARGEN'] = (df_forecast_ext['Forecast_VENTA'] - df_forecast_ext['Forecast_COSTO'] / 1000).round(2)\n",
    "        \n",
    "        # Guardar el archivo CSV\n",
    "        file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "        df_forecast_ext.to_csv(file_path, index=False)\n",
    "        print(f\"Archivo guardado: {file_path}\")\n",
    "        \n",
    "        # Calcular las sumatorias totales de Forecast_VENTA, Forecast_COSTO y MARGEN\n",
    "        total_venta = df_forecast_ext['Forecast_VENTA'].sum()\n",
    "        total_costo = df_forecast_ext['Forecast_COSTO'].sum()\n",
    "        total_margen = df_forecast_ext['MARGEN'].sum()\n",
    "        \n",
    "        mini_grafico = generar_mini_grafico(folder, name)\n",
    "        \n",
    "        # Actualizar el status_id a 40 en el DataFrame original\n",
    "        fes.at[index, \"supply_forecast_execution_status_id\"] = 50\n",
    "        # ✅ Actualizar directamente en la base de datos el estado a 50\n",
    "        \n",
    "        # Llamar a la función update_execution con los valores calculados\n",
    "        update_execution(\n",
    "            execution_id, \n",
    "            supply_forecast_execution_status_id=50, \n",
    "            monthly_sales_in_millions=total_venta, \n",
    "            monthly_purchases_in_millions=total_costo,\n",
    "            monthly_net_margin_in_millions = total_margen,\n",
    "            graphic = mini_grafico\n",
    "        )        \n",
    "        print(f\"Estado actualizado a 50 para {execution_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SANDBOX para Pruebas Manuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"❗Filas con site_id inválido:\", df_forecast_ext['site_id'].isna().sum())\n",
    "print(\"❗Filas con product_id inválido:\", df_forecast_ext['product_id'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Open_Conn_Postgres()\n",
    "query = \"\"\"\n",
    "SELECT code, name, id FROM public.fnd_site\n",
    "WHERE company_id = 'e7498b2e-2669-473f-ab73-e2c8b4dcc585'\n",
    "ORDER BY code \n",
    "\"\"\"\n",
    "\n",
    "stores = pd.read_sql(query, conn)\n",
    "stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "stores['code'] = stores['code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar site_id anterior si ya existía\n",
    "df_forecast_ext = df_forecast_ext.drop(columns=['site_id'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge con los stores para obtener site_id\n",
    "df_forecast_ext = df_forecast_ext.merge(\n",
    "    stores[['code', 'id']],\n",
    "    left_on='Sucursal',\n",
    "    right_on='code',\n",
    "    how='left'\n",
    ").rename(columns={'id': 'site_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_forecast_ext = actualizar_site_ids(df_forecast_ext, conn, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_proveedor = '140'\n",
    "name= '140_UNILEVER'\n",
    "forecast_execution_id ='a85ef5dd-b12a-4a27-a4b5-80b78d10cdf7'\n",
    "forecast_execution_excecute_id ='a97250ed-3d31-4338-b493-659ebcb96691'\n",
    "supplier_id = 'f603814e-c2d8-49b2-8738-24f9cc0a7e89'\n",
    "algoritmo = '140_UNILEVER_ALGO_05'\n",
    "\n",
    "\n",
    "df_forecast_ext = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "df_forecast_ext['Codigo_Articulo']= df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "df_forecast_ext['Sucursal']= df_forecast_ext['Sucursal'].astype(int)\n",
    "df_forecast_ext.fillna(0)   # Por si se filtró algún missing value\n",
    "\n",
    "\n",
    "# Mostrar la tabla con los gráficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Forecast Extendido\", dataframe=df_forecast_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_ext.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Publicar Resultados en execut_result\n",
    "publish_excecution_results(df_forecast_ext, forecast_execution_excecute_id, supplier_id)\n",
    "print(f\"-> Detalle Forecast Publicado CONNEXA: {id_proveedor}, Label: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Open_Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Recuperar PRECIOS y COSTOS\n",
    "precio = get_precios(id_proveedor)    \n",
    "precio['C_ARTICULO']= precio['C_ARTICULO'].astype(int)\n",
    "precio['C_SUCU_EMPR']= precio['C_SUCU_EMPR'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_ext = pd.merge(\n",
    "    df_forecast_ext,  # DataFrame de artículos\n",
    "    precio,    # DataFrame de precios\n",
    "    left_on =['Codigo_Articulo', 'Sucursal'],  # Claves en 'forecast'\n",
    "    right_on=['C_ARTICULO', 'C_SUCU_EMPR'],  # Claves en 'precios'\n",
    "    how='left'  # Solo traer los productos que están en 'forecast'\n",
    ")\n",
    "df_forecast_ext['TOT_VENTA'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_PRECIO_VTA'] / 1000).round(2)\n",
    "df_forecast_ext['TOT_COSTO'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_COSTO_ESTADISTICO'] / 1000).round(2)\n",
    "df_forecast_ext['MARGEN'] = (df_forecast_ext['TOT_VENTA'] - df_forecast_ext['TOT_COSTO'] / 1000).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar Solicitudes de Compra Extended\n",
    "df_suc_faltantes = pd.read_csv(f'{folder}/Sucursales_Faltantes.csv', delimiter=\";\", dtype=str)\n",
    "df_suc_faltantes.columns = df_suc_faltantes.columns.str.strip()  # Elimina espacios adicionales\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el INSERT dinámicamente\n",
    "insert_queries = []\n",
    "for _, row in df_suc_faltantes.iterrows():\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO public.fnd_site(\n",
    "            id, address, latitude, longitude, name, \"timestamp\", type, code, company_id\n",
    "        ) VALUES (\n",
    "            gen_random_uuid(), \n",
    "            '{row['address']}', \n",
    "            {row['latitude'] if row['latitude'] else 'NULL'}, \n",
    "            {row['longitude'] if row['longitude'] else 'NULL'}, \n",
    "            '{row['name']}', \n",
    "            '{row['fecha_hora']}', \n",
    "            '{row['type']}', \n",
    "            {row['code']}, \n",
    "            '{row['company_id']}'\n",
    "        );\n",
    "    \"\"\"\n",
    "    insert_queries.append(query)\n",
    "\n",
    "# Guardar los inserts en un archivo SQL para ejecutarlo luego\n",
    "sql_file_path = f\"{folder}/insert_sucursales.sql\"\n",
    "with open(sql_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(\"\\n\".join(insert_queries))\n",
    "\n",
    "print(f\"Archivo SQL generado en: {sql_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def ejecutar_inserts(sql_file):\n",
    "    conn = None\n",
    "    try:\n",
    "        # Conectar a la base de datos\n",
    "        conn = Open_Conn_Postgres()\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Leer el archivo SQL y ejecutarlo\n",
    "        with open(sql_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_script = f.read()\n",
    "        \n",
    "        cur.execute(sql_script)\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        print(\"Sucursales insertadas correctamente en la base de datos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error ejecutando el insert: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Ejecutar los inserts\n",
    "ejecutar_inserts(sql_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_suc_faltantes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "fecha_maxima = df_ventas[\"Fecha\"].max()\n",
    "df_filtrado = df_ventas[df_ventas[\"Fecha\"] >= (fecha_maxima - pd.Timedelta(days=150))]\n",
    "\n",
    "# Ventas Semanales\n",
    "df_filtrado[\"Mes\"] = df_filtrado[\"Fecha\"].dt.to_period(\"M\").astype(str)\n",
    "df_mes = df_filtrado.groupby(\"Mes\")[\"Unidades\"].sum().reset_index()\n",
    "\n",
    "# Crear el gráfico compacto\n",
    "fig, ax = plt.subplots(figsize=(3, 1))  # Tamaño pequeño para una visualización compacta\n",
    "# ax.bar(df_mes[\"Mes\"], df_mes[\"Unidades\"], color=[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\"], alpha=0.7)\n",
    "# Usar el índice como secuencia de registros\n",
    "ax.bar(range(1, len(df_mes) + 1), df_mes[\"Unidades\"], color=[\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\"], alpha=0.7)\n",
    "\n",
    "# Eliminar ejes y etiquetas para que sea más compacto\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precios = get_precios(id_proveedor)\n",
    "\n",
    "# Mostrar la tabla con los gráficos en base64\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Precios Proveedor\", dataframe=precios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "# Lista de archivos .md a consolidar en un único PDF\n",
    "md_files = [\n",
    "    \"./00_SISTEMA_FORECAST_OVERVIEW.md\",\n",
    "    \"./S10_GENERAR_FORECAST_Planificado.md\",\n",
    "    \"./S20_GENERA_Forecast_Extendido.md\",\n",
    "    \"./S30_GENERA_Grafico_Detalle.md\",\n",
    "    \"./S40_SUBIR_Forecast_Connexa.md\",\n",
    "    \"./funciones_forecast.md\"\n",
    "]\n",
    "\n",
    "# Crear el PDF\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.set_font(\"Arial\", size=11)\n",
    "\n",
    "# Agregar el contenido de cada archivo .md\n",
    "for file_path in md_files:\n",
    "    if os.path.exists(file_path):\n",
    "        pdf.add_page()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip(\"\\n\").replace(\"\\t\", \"    \")\n",
    "                pdf.multi_cell(0, 8, line)\n",
    "\n",
    "# Guardar el documento PDF resultante\n",
    "pdf_output_path = \"./Documentacion_Forecast_Zeetrex.pdf\"\n",
    "pdf.output(pdf_output_path)\n",
    "\n",
    "pdf_output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el contenido de cada archivo .md antes de consolidar en PDF\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "contenido_md = {}\n",
    "for file_path in [\n",
    "    \"./00_SISTEMA_FORECAST_OVERVIEW.md\",\n",
    "    \"./S10_GENERAR_FORECAST_Planificado.md\",\n",
    "    \"./S20_GENERA_Forecast_Extendido.md\",\n",
    "    \"./S30_GENERA_Grafico_Detalle.md\",\n",
    "    \"./S40_SUBIR_Forecast_Connexa.md\",\n",
    "    \"./funciones_forecast.md\"\n",
    "]:\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            contenido_md[file_path] = f.read()\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Contenido de Archivos Markdown\", dataframe=pd.DataFrame.from_dict(contenido_md, orient=\"index\", columns=[\"Contenido\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fes = get_excecution_by_status(40)\n",
    "tools.display_dataframe_to_user(name=\"Contenido de Archivos Markdown\", dataframe=fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def mover_archivos_procesados(algoritmo, folder):    # Movel a procesado los archivos.\n",
    "    destino = os.path.join(folder, \"procesado\")\n",
    "    os.makedirs(destino, exist_ok=True)  # Crea la carpeta si no existe\n",
    "\n",
    "    for archivo in os.listdir(folder):\n",
    "        if archivo.startswith(algoritmo):\n",
    "            origen = os.path.join(folder, archivo)\n",
    "            destino_final = os.path.join(destino, archivo)\n",
    "            shutil.move(origen, destino_final)\n",
    "            print(f\"📁 Archivo movido: {archivo} → {destino_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Archivo movido: 140_UNILEVER_ALGO_05_Pronostico_Extendido.csv → data\\procesado\\140_UNILEVER_ALGO_05_Pronostico_Extendido.csv\n",
      "📁 Archivo movido: 140_UNILEVER_ALGO_05_Solicitudes_Compra.csv → data\\procesado\\140_UNILEVER_ALGO_05_Solicitudes_Compra.csv\n"
     ]
    }
   ],
   "source": [
    "mover_archivos_procesados(algoritmo, folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJECUTABLE PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mover_archivos_procesados' from 'funciones_forecast' (e:\\PY\\DEMANDA\\funciones_forecast.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Solo importar lo necesario desde el módulo de funciones\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunciones_forecast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     Open_Conn_Postgres,\n\u001b[0;32m      4\u001b[0m     Close_Connection,\n\u001b[0;32m      5\u001b[0m     get_excecution_by_status,\n\u001b[0;32m      6\u001b[0m     Open_Postgres_retry,\n\u001b[0;32m      7\u001b[0m     mover_archivos_procesados,\n\u001b[0;32m      8\u001b[0m     actualizar_site_ids,\n\u001b[0;32m      9\u001b[0m     get_precios,\n\u001b[0;32m     10\u001b[0m     get_excecution_excecute_by_status,\n\u001b[0;32m     11\u001b[0m     update_execution,\n\u001b[0;32m     12\u001b[0m     create_execution_execute_result,\n\u001b[0;32m     13\u001b[0m     generar_mini_grafico,\n\u001b[0;32m     14\u001b[0m     generar_grafico_base64\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# uso localmente la lectura de archivos.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools_open\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'mover_archivos_procesados' from 'funciones_forecast' (e:\\PY\\DEMANDA\\funciones_forecast.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Solo importar lo necesario desde el módulo de funciones\n",
    "from funciones_forecast import (\n",
    "    Open_Conn_Postgres,\n",
    "    Close_Connection,\n",
    "    get_excecution_by_status,\n",
    "    Open_Postgres_retry,\n",
    "    mover_archivos_procesados,\n",
    "    actualizar_site_ids,\n",
    "    get_precios,\n",
    "    get_excecution_excecute_by_status,\n",
    "    update_execution,\n",
    "    create_execution_execute_result,\n",
    "    generar_mini_grafico,\n",
    "    generar_grafico_base64\n",
    ")\n",
    "\n",
    "import pandas as pd # uso localmente la lectura de archivos.\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "secrets = dotenv_values(\".env\")\n",
    "folder = secrets[\"FOLDER_DATOS\"]\n",
    "\n",
    "# También podés importar funciones adicionales si tu módulo las necesita\n",
    "\n",
    "def publish_excecution_results(df_forecast_ext, forecast_execution_excecute_id, supplier_id):\n",
    "    print ('Comenzando a grabar el dataframe')\n",
    "    for _, row in df_forecast_ext.iterrows():\n",
    "        create_execution_execute_result(\n",
    "            confidence_level=0.92,  # Valor por defecto ya que no está en df_meged\n",
    "            error_margin=0.07,  # Valor por defecto\n",
    "            expected_demand=row['Forecast'],\n",
    "            average_daily_demand=row['Average'],\n",
    "            lower_bound=row['Q_DIAS_STOCK'],  # Valor por defecto\n",
    "            upper_bound=row['Q_VENTA_DIARIA_NORMAL'],  # Valor por defecto\n",
    "            product_id=row['product_id'],\n",
    "            site_id=row['site_id'],\n",
    "            supply_forecast_execution_execute_id=forecast_execution_excecute_id,\n",
    "            algorithm=row['algoritmo'],\n",
    "            average=row['Average'],\n",
    "            ext_product_code=row['Codigo_Articulo'],\n",
    "            ext_site_code=row['Sucursal'],\n",
    "            ext_supplier_code=row['id_proveedor'],\n",
    "            forcast=row['Q_REPONER_INCLUIDO_SOBRE_STOCK'],\n",
    "            graphic=row['GRAFICO'],\n",
    "            quantity_stock=row['Q_TRANSF_PEND'],  # Valor por defecto\n",
    "            sales_last=row['ventas_last'],\n",
    "            sales_previous=row['ventas_previous'],\n",
    "            sales_same_year=row['ventas_same_year'],\n",
    "            supplier_id=supplier_id,\n",
    "            windows=row['ventana'],\n",
    "            deliveries_pending=1  # Valor por defecto\n",
    "        )\n",
    "    print ('--------------------------------')\n",
    "\n",
    "def actualizar_site_ids(df_forecast_ext, conn, name):\n",
    "    \"\"\"Reemplaza site_id en df_forecast_ext con datos válidos desde fnd_site\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT code, name, id FROM public.fnd_site\n",
    "    WHERE company_id = 'e7498b2e-2669-473f-ab73-e2c8b4dcc585'\n",
    "    ORDER BY code \n",
    "    \"\"\"\n",
    "    stores = pd.read_sql(query, conn)\n",
    "    stores = stores[pd.to_numeric(stores['code'], errors='coerce').notna()].copy()\n",
    "    stores['code'] = stores['code'].astype(int)\n",
    "\n",
    "    # Eliminar site_id anterior si ya existía\n",
    "    df_forecast_ext = df_forecast_ext.drop(columns=['site_id'], errors='ignore')\n",
    "\n",
    "    # Merge con los stores para obtener site_id\n",
    "    df_forecast_ext = df_forecast_ext.merge(\n",
    "        stores[['code', 'id']],\n",
    "        left_on='Sucursal',\n",
    "        right_on='code',\n",
    "        how='left'\n",
    "    ).rename(columns={'id': 'site_id'})\n",
    "\n",
    "    # Validar valores faltantes\n",
    "    missing = df_forecast_ext[df_forecast_ext['site_id'].isna()]\n",
    "    if not missing.empty:\n",
    "        print(f\"⚠️ Faltan site_id en {len(missing)} registros\")\n",
    "        missing.to_csv(f\"{folder}/{name}_Missing_Site_IDs.csv\", index=False)\n",
    "    else:\n",
    "        print(\"✅ Todos los registros tienen site_id válido\")\n",
    "\n",
    "    return df_forecast_ext\n",
    "\n",
    "def insertar_graficos_forecast(algoritmo, name, id_proveedor):\n",
    "        \n",
    "    # Recuperar Historial de Ventas\n",
    "    df_ventas = pd.read_csv(f'{folder}/{name}_Ventas.csv')\n",
    "    df_ventas['Codigo_Articulo']= df_ventas['Codigo_Articulo'].astype(int)\n",
    "    df_ventas['Sucursal']= df_ventas['Sucursal'].astype(int)\n",
    "    df_ventas['Fecha']= pd.to_datetime(df_ventas['Fecha'])\n",
    "\n",
    "    # Recuperando Forecast Calculado\n",
    "    df_forecast = pd.read_csv(f'{folder}/{algoritmo}_Solicitudes_Compra.csv')\n",
    "    df_forecast.fillna(0)   # Por si se filtró algún missing value\n",
    "    print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")\n",
    "    \n",
    "    # Agregar la nueva columna de gráficos en df_forecast Iterando sobre todo el DATAFRAME\n",
    "    df_forecast[\"GRAFICO\"] = df_forecast.apply(\n",
    "        lambda row: generar_grafico_base64(df_ventas, row[\"Codigo_Articulo\"], row[\"Sucursal\"], row[\"Forecast\"], row[\"Average\"], row[\"ventas_last\"], row[\"ventas_previous\"], row[\"ventas_same_year\"]) if not pd.isna(row[\"Codigo_Articulo\"]) and not pd.isna(row[\"Sucursal\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Leer Dataframe de FORECAST EXECUTION LISTOS PARA IMPORTAR A CONNEXA (DE 40 A 50)\n",
    "    fes = get_excecution_excecute_by_status(40)\n",
    "    \n",
    "    for index, row in fes[fes[\"supply_forecast_execution_status_id\"] == 40].iterrows():\n",
    "        algoritmo = row[\"name\"]\n",
    "        name = algoritmo.split('_ALGO')[0]\n",
    "        execution_id = row[\"id\"]\n",
    "        id_proveedor = row[\"ext_supplier_code\"]\n",
    "        forecast_execution_execute_id = row[\"forecast_execution_execute_id\"]\n",
    "        supplier_id = row[\"supplier_id\"]\n",
    "\n",
    "        print(f\"Algoritmo: {algoritmo}  - Name: {name} exce_id: {forecast_execution_execute_id} id: Proveedor {id_proveedor}\")\n",
    "        print(f\"supplier-id: {supplier_id} ----------------------------------------------------\")\n",
    "\n",
    "        try:\n",
    "            # Leer forecast extendido\n",
    "            df_forecast_ext = pd.read_csv(f'{folder}/{algoritmo}_Pronostico_Extendido.csv')\n",
    "            df_forecast_ext['Codigo_Articulo'] = df_forecast_ext['Codigo_Articulo'].astype(int)\n",
    "            df_forecast_ext['Sucursal'] = df_forecast_ext['Sucursal'].astype(int)\n",
    "            df_forecast_ext.fillna(0, inplace=True)\n",
    "            print(f\"-> Datos Recuperados del CACHE: {id_proveedor}, Label: {name}\")            \n",
    "            print(\"❗Filas con site_id inválido:\", df_forecast_ext['site_id'].isna().sum())\n",
    "            print(\"❗Filas con product_id inválido:\", df_forecast_ext['product_id'].isna().sum())\n",
    "\n",
    "            # Agregar site_id desde fnd_site\n",
    "            conn = Open_Conn_Postgres()\n",
    "            df_forecast_ext = actualizar_site_ids(df_forecast_ext, conn, name)\n",
    "            print(f\"-> Se actualizaron los site_ids: {id_proveedor}, Label: {name}\")\n",
    "            \n",
    "            # Hacer merge solo si no existen las columnas de precios y costos\n",
    "            if 'I_PRECIO_VTA' not in df_forecast_ext.columns or 'I_COSTO_ESTADISTICO' not in df_forecast_ext.columns:\n",
    "                print(f\"❌ ERROR: Falta la columna requerida '{col}' procedemos a actualizar {id_proveedor}\")\n",
    "                precio = get_precios(id_proveedor)\n",
    "                precio['C_ARTICULO'] = precio['C_ARTICULO'].astype(int)\n",
    "                precio['C_SUCU_EMPR'] = precio['C_SUCU_EMPR'].astype(int)\n",
    "\n",
    "                df_forecast_ext = df_forecast_ext.merge(\n",
    "                    precio,\n",
    "                    left_on=['Codigo_Articulo', 'Sucursal'],\n",
    "                    right_on=['C_ARTICULO', 'C_SUCU_EMPR'],\n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                print(f\"⚠️ El DataFrame ya contiene precios y costos. Merge evitado para {id_proveedor}\")\n",
    "            \n",
    "            # Verificar columnas necesarias después del merge\n",
    "            columnas_requeridas = ['I_PRECIO_VTA', 'I_COSTO_ESTADISTICO']\n",
    "            for col in columnas_requeridas:\n",
    "                if col not in df_forecast_ext.columns:\n",
    "                    print(f\"❌ ERROR: Falta la columna requerida '{col}' en df_forecast_ext para el proveedor {id_proveedor}\")\n",
    "                    df_forecast_ext.to_csv(f\"{folder}/{algoritmo}_ERROR_MERGE.csv\", index=False)\n",
    "                    raise ValueError(f\"Column '{col}' missing in df_forecast_ext. No se puede continuar.\")\n",
    "\n",
    "            # Cálculo de métricas x Línea en miles\n",
    "            df_forecast_ext['Forecast_VENTA'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_PRECIO_VTA'] / 1000).round(2)\n",
    "            df_forecast_ext['Forecast_COSTO'] = (df_forecast_ext['Forecast'] * df_forecast_ext['I_COSTO_ESTADISTICO'] / 1000).round(2)\n",
    "            df_forecast_ext['MARGEN'] = (df_forecast_ext['Forecast_VENTA'] - df_forecast_ext['Forecast_COSTO'])\n",
    "\n",
    "            # Guardar CSV actualizado\n",
    "            file_path = f\"{folder}/{algoritmo}_Pronostico_Extendido.csv\"\n",
    "            df_forecast_ext.to_csv(file_path, index=False)\n",
    "            print(f\"Archivo guardado: {file_path}\")\n",
    "            \n",
    "            # Asegurar que los valores son del tipo float (nativo de Python)\n",
    "            total_venta = float(round(df_forecast_ext['Forecast_VENTA'].sum() / 1000, 2))\n",
    "            total_costo = float(round(df_forecast_ext['Forecast_COSTO'].sum() / 1000, 2))\n",
    "            total_margen = float(round(df_forecast_ext['MARGEN'].sum() / 1000, 2))\n",
    "\n",
    "            # Mini gráfico\n",
    "            mini_grafico = generar_mini_grafico(folder, name)\n",
    "\n",
    "            # Actualizar en base de datos\n",
    "            update_execution(\n",
    "                execution_id,\n",
    "                supply_forecast_execution_status_id=45,\n",
    "                monthly_sales_in_millions=total_venta,\n",
    "                monthly_purchases_in_millions=total_costo,\n",
    "                monthly_net_margin_in_millions=total_margen,\n",
    "                graphic=mini_grafico\n",
    "            )\n",
    "            \n",
    "            # Publicar en tabla de resultados\n",
    "            publish_excecution_results(df_forecast_ext, forecast_execution_execute_id, supplier_id)\n",
    "            print(f\"-> Detalle Forecast Publicado CONNEXA: {id_proveedor}, Label: {name}\")\n",
    "                        \n",
    "            # ✅ Actualizar Estado intermedio de Procesamiento....\n",
    "            update_execution(execution_id, supply_forecast_execution_status_id=50)\n",
    "            print(f\"✅ Estado actualizado a 50 para {execution_id}\")\n",
    "            \n",
    "            # Mover archivos a Procesado\n",
    "            mover_archivos_procesados(algoritmo, folder)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(f\"❌ Error procesando {name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
